[
  {
    "objectID": "troubleshooting-faq.html",
    "href": "troubleshooting-faq.html",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "This guide addresses common issues you might encounter when using Cyclonetix and provides solutions for troubleshooting them.\n\n\n\n\nProblem: Error when running cargo build\nSolutions:\n\nEnsure you have Rust 1.84+ installed:\nrustc --version\nCheck for missing dependencies:\nsudo apt install build-essential pkg-config libssl-dev\nUpdate Rust and dependencies:\nrustup update\ncargo update\n\n\n\n\nProblem: Docker container exits immediately after starting\nSolutions:\n\nCheck Docker logs:\ndocker logs &lt;container_id&gt;\nEnsure Redis is accessible if using Redis backend:\ndocker exec &lt;container_id&gt; redis-cli -h redis ping\nVerify your configuration is mounted correctly:\ndocker exec &lt;container_id&gt; cat /config/config.yaml\n\n\n\n\n\n\n\nProblem: Cyclonetix can’t connect to Redis\nSolutions:\n\nVerify Redis is running:\nredis-cli ping\nCheck your Redis URL in config.yaml:\nbackend_url: \"redis://localhost:6379\"\nEnsure Redis is accepting connections from your network:\nsudo netstat -pnlt | grep 6379\n\n\n\n\nProblem: Can’t access the Cyclonetix UI at http://localhost:3000\nSolutions:\n\nConfirm Cyclonetix is running:\nps aux | grep cyclonetix\nCheck if another service is using port 3000:\nsudo netstat -pnlt | grep 3000\nVerify firewall settings:\nsudo ufw status\n\n\n\n\n\n\n\nProblem: Tasks remain in pending state and never execute\nSolutions:\n\nCheck for missing dependencies:\n\nVerify that all dependencies exist and are correctly spelled\nCheck task definitions for circular dependencies\n\nEnsure agents are running:\n# Check agent status in UI\n# Or via CLI\n./cyclonetix list-agents\nConfirm agents are subscribed to the right queues:\n\nCheck your agent configuration against task queue assignments\n\n\n\n\n\nProblem: Tasks show as failed but no error message is visible\nSolutions:\n\nEnable verbose logging:\nRUST_LOG=debug ./cyclonetix\nCheck agent logs for command execution errors:\n\nLook for environment variable issues\nCheck for permission problems with executed commands\n\nVerify the task command is valid:\n\nTry running the command manually on the agent machine\n\n\n\n\n\nProblem: Tasks are queued but no agent picks them up\nSolutions:\n\nVerify agent is running and connected:\n\nCheck the Agents page in the UI\nLook for heartbeat updates\n\nEnsure the agent is subscribed to the right queue:\n# In config.yaml\nqueues:\n  - \"default\"\n  - \"high_memory\"\nCheck agent logs for connection issues:\nRUST_LOG=debug ./cyclonetix --agent\n\n\n\n\n\n\n\nProblem: Self-assembling DAG is missing expected tasks\nSolutions:\n\nVerify task dependencies are correctly defined:\n\nCheck each task YAML file for the correct dependency list\n\nCheck for typos in task IDs:\n\nEnsure dependency references match exactly with task IDs\n\nEnable debug logging to see how the DAG is assembled:\nRUST_LOG=debug ./cyclonetix schedule-task &lt;final_task&gt;\n\n\n\n\nProblem: Tasks are executing in an order you didn’t expect\nSolutions:\n\nReview the DAG visualization in the UI:\n\nCheck the actual dependency structure\n\nVerify there are no missing or incorrect dependencies:\n\nTasks with no dependencies (or fewer than expected) will start earlier\n\nRemember that independent parallel paths will execute concurrently:\n\nThe exact order of parallel tasks depends on agent availability\n\n\n\n\n\nProblem: Workflow shows as running even though all tasks are complete\nSolutions:\n\nCheck for tasks in unknown states:\n\nLook for tasks that failed to report completion\n\nTry manually refreshing the DAG status:\n./cyclonetix refresh-dag &lt;dag_id&gt;\nRestart the orchestrator as a last resort:\nsudo systemctl restart cyclonetix\n\n\n\n\n\n\n\nProblem: Workflow state is lost after restarting Cyclonetix\nSolutions:\n\nEnsure you’re using a persistent backend:\n# In config.yaml\nbackend: \"redis\"  # Not \"memory\"\nCheck if Redis persistence is enabled:\nredis-cli config get save\nVerify data is being written to Redis:\nredis-cli keys \"Cyclonetix:*\"\n\n\n\n\nProblem: Some tasks are being executed multiple times\nSolutions:\n\nCheck for multiple agents listening to the same queue:\n\nThis is expected behavior for high concurrency\nEnsure your tasks are idempotent\n\nLook for agent failures and task reassignments:\n\nWhen an agent fails, its tasks are reassigned\n\nVerify that multiple orchestrators aren’t scheduling the same DAG:\n\nCheck for duplicate process instances\n\n\n\n\n\n\n\n\nProblem: DAG visualization in UI is incomplete or incorrect\nSolutions:\n\nTry refreshing the browser:\n\nSome complex DAGs may not render completely on first load\n\nAdjust zoom and layout:\n\nUse mouse wheel to zoom\nDrag nodes to adjust layout\n\nClear browser cache:\n\nOutdated JS assets might cause rendering issues\n\n\n\n\n\nProblem: UI becomes slow or unresponsive with large DAGs\nSolutions:\n\nLimit the DAG size in visualization:\n# In config.yaml\nui:\n  max_dag_nodes: 100\nFilter your view to show only active tasks:\n\nUse the task filter in the UI\n\nUse a more powerful browser:\n\nChrome or Firefox often perform better than Safari for complex visualizations\n\n\n\n\n\n\n\n\n\n\nA task is a single unit of work, while a DAG (Directed Acyclic Graph) is a collection of tasks with dependencies that form a workflow.\n\n\n\nYes, Cyclonetix has an in-memory backend that’s perfect for development and testing. However, for production use or persistence across restarts, Redis is recommended.\n\n\n\nThe primary data to backup is: - Your task definitions in data/tasks/ - Your DAG definitions in data/dags/ - Your context definitions in data/contexts/\nFor runtime state, back up your Redis database.\n\n\n\n\n\n\nCyclonetix is designed to scale to thousands of tasks with a properly configured Redis backend. The limit is mostly determined by your Redis server capacity and network performance.\n\n\n\nA single orchestrator can typically handle multiple agents. A common starting point is: - 1 orchestrator - 3-5 agents - 1 Redis instance\nScale up from there based on your workload.\n\n\n\nFor large workflows: 1. Use multiple agents 2. Increase agent concurrency 3. Consider sharding your Redis backend 4. Use PostgreSQL backend for very large deployments 5. Break extremely large workflows into smaller sub-workflows\n\n\n\n\n\n\nCyclonetix supports: - Basic authentication - OAuth2 for enterprise authentication - API keys for programmatic access\nConfigure these in the security section of your config.yaml.\n\n\n\nBy default, communication is not encrypted. For production deployments, consider: - Using Redis with TLS - Placing all components behind a VPN - Using an HTTPS proxy for the UI\n\n\n\n\n\n\nYes, there are two approaches: 1. Run the agent inside a container and have it execute docker commands 2. Use the command field to invoke docker: yaml    command: \"docker run --rm my-image:latest python /script.py\"\n\n\n\nYes, you can trigger tasks via: - The REST API - Webhook endpoints (coming soon) - Kafka integration (coming soon)\n\n\n\nCyclonetix can be integrated with CI/CD using: 1. API calls from your CI/CD system 2. Git-based execution for code from repositories 3. Shared task definitions between development and production\n\n\n\n\n\nIf you’re still experiencing problems:\n\nCheck the GitHub Issues for similar problems\nEnable debug logging: RUST_LOG=debug ./cyclonetix\nOpen a new issue with:\n\nA detailed description of the problem\nSteps to reproduce\nLog output\nConfiguration (redacted of sensitive information)\n\n\n\n\n\n\nReview the Configuration Reference for detailed settings\nExplore Advanced Features for complex workflows\nCheck the Roadmap for upcoming features",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#installation-issues",
    "href": "troubleshooting-faq.html#installation-issues",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "Problem: Error when running cargo build\nSolutions:\n\nEnsure you have Rust 1.84+ installed:\nrustc --version\nCheck for missing dependencies:\nsudo apt install build-essential pkg-config libssl-dev\nUpdate Rust and dependencies:\nrustup update\ncargo update\n\n\n\n\nProblem: Docker container exits immediately after starting\nSolutions:\n\nCheck Docker logs:\ndocker logs &lt;container_id&gt;\nEnsure Redis is accessible if using Redis backend:\ndocker exec &lt;container_id&gt; redis-cli -h redis ping\nVerify your configuration is mounted correctly:\ndocker exec &lt;container_id&gt; cat /config/config.yaml",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#connection-issues",
    "href": "troubleshooting-faq.html#connection-issues",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "Problem: Cyclonetix can’t connect to Redis\nSolutions:\n\nVerify Redis is running:\nredis-cli ping\nCheck your Redis URL in config.yaml:\nbackend_url: \"redis://localhost:6379\"\nEnsure Redis is accepting connections from your network:\nsudo netstat -pnlt | grep 6379\n\n\n\n\nProblem: Can’t access the Cyclonetix UI at http://localhost:3000\nSolutions:\n\nConfirm Cyclonetix is running:\nps aux | grep cyclonetix\nCheck if another service is using port 3000:\nsudo netstat -pnlt | grep 3000\nVerify firewall settings:\nsudo ufw status",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#task-execution-issues",
    "href": "troubleshooting-faq.html#task-execution-issues",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "Problem: Tasks remain in pending state and never execute\nSolutions:\n\nCheck for missing dependencies:\n\nVerify that all dependencies exist and are correctly spelled\nCheck task definitions for circular dependencies\n\nEnsure agents are running:\n# Check agent status in UI\n# Or via CLI\n./cyclonetix list-agents\nConfirm agents are subscribed to the right queues:\n\nCheck your agent configuration against task queue assignments\n\n\n\n\n\nProblem: Tasks show as failed but no error message is visible\nSolutions:\n\nEnable verbose logging:\nRUST_LOG=debug ./cyclonetix\nCheck agent logs for command execution errors:\n\nLook for environment variable issues\nCheck for permission problems with executed commands\n\nVerify the task command is valid:\n\nTry running the command manually on the agent machine\n\n\n\n\n\nProblem: Tasks are queued but no agent picks them up\nSolutions:\n\nVerify agent is running and connected:\n\nCheck the Agents page in the UI\nLook for heartbeat updates\n\nEnsure the agent is subscribed to the right queue:\n# In config.yaml\nqueues:\n  - \"default\"\n  - \"high_memory\"\nCheck agent logs for connection issues:\nRUST_LOG=debug ./cyclonetix --agent",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#dag-and-workflow-issues",
    "href": "troubleshooting-faq.html#dag-and-workflow-issues",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "Problem: Self-assembling DAG is missing expected tasks\nSolutions:\n\nVerify task dependencies are correctly defined:\n\nCheck each task YAML file for the correct dependency list\n\nCheck for typos in task IDs:\n\nEnsure dependency references match exactly with task IDs\n\nEnable debug logging to see how the DAG is assembled:\nRUST_LOG=debug ./cyclonetix schedule-task &lt;final_task&gt;\n\n\n\n\nProblem: Tasks are executing in an order you didn’t expect\nSolutions:\n\nReview the DAG visualization in the UI:\n\nCheck the actual dependency structure\n\nVerify there are no missing or incorrect dependencies:\n\nTasks with no dependencies (or fewer than expected) will start earlier\n\nRemember that independent parallel paths will execute concurrently:\n\nThe exact order of parallel tasks depends on agent availability\n\n\n\n\n\nProblem: Workflow shows as running even though all tasks are complete\nSolutions:\n\nCheck for tasks in unknown states:\n\nLook for tasks that failed to report completion\n\nTry manually refreshing the DAG status:\n./cyclonetix refresh-dag &lt;dag_id&gt;\nRestart the orchestrator as a last resort:\nsudo systemctl restart cyclonetix",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#state-management-issues",
    "href": "troubleshooting-faq.html#state-management-issues",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "Problem: Workflow state is lost after restarting Cyclonetix\nSolutions:\n\nEnsure you’re using a persistent backend:\n# In config.yaml\nbackend: \"redis\"  # Not \"memory\"\nCheck if Redis persistence is enabled:\nredis-cli config get save\nVerify data is being written to Redis:\nredis-cli keys \"Cyclonetix:*\"\n\n\n\n\nProblem: Some tasks are being executed multiple times\nSolutions:\n\nCheck for multiple agents listening to the same queue:\n\nThis is expected behavior for high concurrency\nEnsure your tasks are idempotent\n\nLook for agent failures and task reassignments:\n\nWhen an agent fails, its tasks are reassigned\n\nVerify that multiple orchestrators aren’t scheduling the same DAG:\n\nCheck for duplicate process instances",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#ui-issues",
    "href": "troubleshooting-faq.html#ui-issues",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "Problem: DAG visualization in UI is incomplete or incorrect\nSolutions:\n\nTry refreshing the browser:\n\nSome complex DAGs may not render completely on first load\n\nAdjust zoom and layout:\n\nUse mouse wheel to zoom\nDrag nodes to adjust layout\n\nClear browser cache:\n\nOutdated JS assets might cause rendering issues\n\n\n\n\n\nProblem: UI becomes slow or unresponsive with large DAGs\nSolutions:\n\nLimit the DAG size in visualization:\n# In config.yaml\nui:\n  max_dag_nodes: 100\nFilter your view to show only active tasks:\n\nUse the task filter in the UI\n\nUse a more powerful browser:\n\nChrome or Firefox often perform better than Safari for complex visualizations",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#frequently-asked-questions",
    "href": "troubleshooting-faq.html#frequently-asked-questions",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "A task is a single unit of work, while a DAG (Directed Acyclic Graph) is a collection of tasks with dependencies that form a workflow.\n\n\n\nYes, Cyclonetix has an in-memory backend that’s perfect for development and testing. However, for production use or persistence across restarts, Redis is recommended.\n\n\n\nThe primary data to backup is: - Your task definitions in data/tasks/ - Your DAG definitions in data/dags/ - Your context definitions in data/contexts/\nFor runtime state, back up your Redis database.\n\n\n\n\n\n\nCyclonetix is designed to scale to thousands of tasks with a properly configured Redis backend. The limit is mostly determined by your Redis server capacity and network performance.\n\n\n\nA single orchestrator can typically handle multiple agents. A common starting point is: - 1 orchestrator - 3-5 agents - 1 Redis instance\nScale up from there based on your workload.\n\n\n\nFor large workflows: 1. Use multiple agents 2. Increase agent concurrency 3. Consider sharding your Redis backend 4. Use PostgreSQL backend for very large deployments 5. Break extremely large workflows into smaller sub-workflows\n\n\n\n\n\n\nCyclonetix supports: - Basic authentication - OAuth2 for enterprise authentication - API keys for programmatic access\nConfigure these in the security section of your config.yaml.\n\n\n\nBy default, communication is not encrypted. For production deployments, consider: - Using Redis with TLS - Placing all components behind a VPN - Using an HTTPS proxy for the UI\n\n\n\n\n\n\nYes, there are two approaches: 1. Run the agent inside a container and have it execute docker commands 2. Use the command field to invoke docker: yaml    command: \"docker run --rm my-image:latest python /script.py\"\n\n\n\nYes, you can trigger tasks via: - The REST API - Webhook endpoints (coming soon) - Kafka integration (coming soon)\n\n\n\nCyclonetix can be integrated with CI/CD using: 1. API calls from your CI/CD system 2. Git-based execution for code from repositories 3. Shared task definitions between development and production",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#still-having-issues",
    "href": "troubleshooting-faq.html#still-having-issues",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "If you’re still experiencing problems:\n\nCheck the GitHub Issues for similar problems\nEnable debug logging: RUST_LOG=debug ./cyclonetix\nOpen a new issue with:\n\nA detailed description of the problem\nSteps to reproduce\nLog output\nConfiguration (redacted of sensitive information)",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "troubleshooting-faq.html#next-steps",
    "href": "troubleshooting-faq.html#next-steps",
    "title": "Troubleshooting & FAQ",
    "section": "",
    "text": "Review the Configuration Reference for detailed settings\nExplore Advanced Features for complex workflows\nCheck the Roadmap for upcoming features",
    "crumbs": [
      "Troubleshooting & FAQ"
    ]
  },
  {
    "objectID": "deployment/configuration.html",
    "href": "deployment/configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "This guide explains how to configure Cyclonetix for different environments and use cases.\n\n\nCyclonetix uses a YAML configuration file to define its behavior. By default, it looks for config.yaml in the current directory, but you can specify a different location with the --config flag:\n./cyclonetix --config /etc/cyclonetix/production.yaml\n\n\n\nHere’s a minimal configuration file:\ntask_directory: \"./data/tasks\"\ncontext_directory: \"./data/contexts\"\nparameter_set_directory: \"./data/parameter_sets\"\ndag_directory: \"./data/dags\"\n\nbackend: \"redis\"\nbackend_url: \"redis://localhost:6379\"\n\n\n\n\n\ntask_directory: \"./data/tasks\"        # Directory containing task definitions\ncontext_directory: \"./data/contexts\"   # Directory containing context definitions\nparameter_set_directory: \"./data/parameter_sets\"  # Directory containing parameter sets\ndag_directory: \"./data/dags\"          # Directory containing DAG definitions\n\n\n\n# In-memory backend (for development)\nbackend: \"memory\"\n\n# Redis backend (for production)\nbackend: \"redis\"\nbackend_url: \"redis://localhost:6379\"\nserialization_format: \"binary\"  # Optional: \"json\" (default) or \"binary\"\n\n# PostgreSQL backend (for large-scale deployments)\nbackend: \"postgresql\"\nbackend_url: \"postgres://user:password@localhost/cyclonetix\"\n\n\n\n# Define available queues\nqueues:\n  - \"default\"          # Required default queue\n  - \"high_memory\"      # Optional additional queues\n  - \"gpu_tasks\"\n  - \"etl\"\n\n\n\nsecurity:\n  enabled: true\n  oauth:\n    provider: \"google\"\n    client_id: \"your-client-id\"\n    client_secret: \"your-client-secret\"\n    redirect_url: \"http://localhost:3000/auth/callback\"\n    allowed_domains: [\"yourdomain.com\"]  # Optional: restrict by domain\n    allowed_emails: [\"user@example.com\"]  # Optional: restrict by email\n  cookie_secret: \"use-a-random-string-here\"\n  session_timeout_minutes: 120\n  public_paths:\n    - \"/static/*\"\n    - \"/login\"\n    - \"/auth/*\"\n    - \"/health\"\n  api_paths:\n    - \"/api/*\"\n\n\n\nui:\n  title: \"Cyclonetix Orchestrator\"\n  logo_path: \"/static/img/custom-logo.png\"  # Optional custom logo\n  theme: \"dark\"                            # \"dark\" or \"light\"\n  refresh_interval_seconds: 10             # Dashboard refresh rate\n  default_view: \"dashboard\"                # Starting page\n\n\n\nagent:\n  concurrency: 4                    # Number of concurrent tasks per agent\n  heartbeat_interval_seconds: 5     # Heartbeat frequency\n  execution_timeout_seconds: 3600   # Default timeout for tasks\n  cleanup_temp_files: true          # Whether to clean up temp files\n\n\n\norchestrator:\n  evaluation_interval_seconds: 5    # How often to evaluate DAGs\n  max_parallel_evaluations: 10      # Max parallel DAG evaluations\n  enable_auto_recovery: true        # Recover from failures automatically\n\n\n\nlogging:\n  level: \"info\"                     # trace, debug, info, warn, error\n  format: \"text\"                    # text or json\n  file: \"/var/log/cyclonetix.log\"   # Optional log file\n  syslog: false                     # Whether to log to syslog\n\n\n\n\nMost configuration options can be overridden with environment variables. The format is CYCLO_SECTION_OPTION. For example:\n\nCYCLO_BACKEND_URL overrides backend_url\nCYCLO_SECURITY_ENABLED overrides security.enabled\nCYCLO_LOGGING_LEVEL overrides logging.level\n\n\n\n\nYou can use different configuration files for different environments:\n# Development\n./cyclonetix --config config.dev.yaml\n\n# Production\n./cyclonetix --config config.prod.yaml\n\n\n\nTo verify your configuration without starting the server:\n./cyclonetix --verify-config --config config.yaml\n\n\n\nFor sensitive information like API keys and secrets, you can:\n\nUse environment variables\nUse a separate config file with restricted permissions\nUse a secret management system like HashiCorp Vault\n\n\n\n\n\n\ntask_directory: \"./data/tasks\"\ncontext_directory: \"./data/contexts\"\nparameter_set_directory: \"./data/parameter_sets\"\ndag_directory: \"./data/dags\"\n\nbackend: \"memory\"\n\nqueues:\n  - \"default\"\n\nsecurity:\n  enabled: false\n\nlogging:\n  level: \"debug\"\n\n\n\ntask_directory: \"/etc/cyclonetix/tasks\"\ncontext_directory: \"/etc/cyclonetix/contexts\"\nparameter_set_directory: \"/etc/cyclonetix/parameters\"\ndag_directory: \"/etc/cyclonetix/dags\"\n\nbackend: \"redis\"\nbackend_url: \"redis://redis.internal:6379\"\nserialization_format: \"binary\"\n\nqueues:\n  - \"default\"\n  - \"high_memory\"\n  - \"gpu_tasks\"\n  - \"etl\"\n\nsecurity:\n  enabled: true\n  oauth:\n    provider: \"google\"\n    client_id: \"${OAUTH_CLIENT_ID}\"\n    client_secret: \"${OAUTH_CLIENT_SECRET}\"\n    redirect_url: \"https://cyclonetix.example.com/auth/callback\"\n    allowed_domains: [\"example.com\"]\n  cookie_secret: \"${COOKIE_SECRET}\"\n  session_timeout_minutes: 480\n\nlogging:\n  level: \"info\"\n  format: \"json\"\n  file: \"/var/log/cyclonetix.log\"\n\n\n\n\n\nReview Security Configuration for securing your deployment\nExplore Scaling Options for large deployments\nCheck the CLI Reference for command-line options",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#configuration-file",
    "href": "deployment/configuration.html#configuration-file",
    "title": "Configuration",
    "section": "",
    "text": "Cyclonetix uses a YAML configuration file to define its behavior. By default, it looks for config.yaml in the current directory, but you can specify a different location with the --config flag:\n./cyclonetix --config /etc/cyclonetix/production.yaml",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#basic-configuration",
    "href": "deployment/configuration.html#basic-configuration",
    "title": "Configuration",
    "section": "",
    "text": "Here’s a minimal configuration file:\ntask_directory: \"./data/tasks\"\ncontext_directory: \"./data/contexts\"\nparameter_set_directory: \"./data/parameter_sets\"\ndag_directory: \"./data/dags\"\n\nbackend: \"redis\"\nbackend_url: \"redis://localhost:6379\"",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#configuration-sections",
    "href": "deployment/configuration.html#configuration-sections",
    "title": "Configuration",
    "section": "",
    "text": "task_directory: \"./data/tasks\"        # Directory containing task definitions\ncontext_directory: \"./data/contexts\"   # Directory containing context definitions\nparameter_set_directory: \"./data/parameter_sets\"  # Directory containing parameter sets\ndag_directory: \"./data/dags\"          # Directory containing DAG definitions\n\n\n\n# In-memory backend (for development)\nbackend: \"memory\"\n\n# Redis backend (for production)\nbackend: \"redis\"\nbackend_url: \"redis://localhost:6379\"\nserialization_format: \"binary\"  # Optional: \"json\" (default) or \"binary\"\n\n# PostgreSQL backend (for large-scale deployments)\nbackend: \"postgresql\"\nbackend_url: \"postgres://user:password@localhost/cyclonetix\"\n\n\n\n# Define available queues\nqueues:\n  - \"default\"          # Required default queue\n  - \"high_memory\"      # Optional additional queues\n  - \"gpu_tasks\"\n  - \"etl\"\n\n\n\nsecurity:\n  enabled: true\n  oauth:\n    provider: \"google\"\n    client_id: \"your-client-id\"\n    client_secret: \"your-client-secret\"\n    redirect_url: \"http://localhost:3000/auth/callback\"\n    allowed_domains: [\"yourdomain.com\"]  # Optional: restrict by domain\n    allowed_emails: [\"user@example.com\"]  # Optional: restrict by email\n  cookie_secret: \"use-a-random-string-here\"\n  session_timeout_minutes: 120\n  public_paths:\n    - \"/static/*\"\n    - \"/login\"\n    - \"/auth/*\"\n    - \"/health\"\n  api_paths:\n    - \"/api/*\"\n\n\n\nui:\n  title: \"Cyclonetix Orchestrator\"\n  logo_path: \"/static/img/custom-logo.png\"  # Optional custom logo\n  theme: \"dark\"                            # \"dark\" or \"light\"\n  refresh_interval_seconds: 10             # Dashboard refresh rate\n  default_view: \"dashboard\"                # Starting page\n\n\n\nagent:\n  concurrency: 4                    # Number of concurrent tasks per agent\n  heartbeat_interval_seconds: 5     # Heartbeat frequency\n  execution_timeout_seconds: 3600   # Default timeout for tasks\n  cleanup_temp_files: true          # Whether to clean up temp files\n\n\n\norchestrator:\n  evaluation_interval_seconds: 5    # How often to evaluate DAGs\n  max_parallel_evaluations: 10      # Max parallel DAG evaluations\n  enable_auto_recovery: true        # Recover from failures automatically\n\n\n\nlogging:\n  level: \"info\"                     # trace, debug, info, warn, error\n  format: \"text\"                    # text or json\n  file: \"/var/log/cyclonetix.log\"   # Optional log file\n  syslog: false                     # Whether to log to syslog",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#environment-variable-overrides",
    "href": "deployment/configuration.html#environment-variable-overrides",
    "title": "Configuration",
    "section": "",
    "text": "Most configuration options can be overridden with environment variables. The format is CYCLO_SECTION_OPTION. For example:\n\nCYCLO_BACKEND_URL overrides backend_url\nCYCLO_SECURITY_ENABLED overrides security.enabled\nCYCLO_LOGGING_LEVEL overrides logging.level",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#configuration-profiles",
    "href": "deployment/configuration.html#configuration-profiles",
    "title": "Configuration",
    "section": "",
    "text": "You can use different configuration files for different environments:\n# Development\n./cyclonetix --config config.dev.yaml\n\n# Production\n./cyclonetix --config config.prod.yaml",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#verifying-configuration",
    "href": "deployment/configuration.html#verifying-configuration",
    "title": "Configuration",
    "section": "",
    "text": "To verify your configuration without starting the server:\n./cyclonetix --verify-config --config config.yaml",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#sensitive-configuration",
    "href": "deployment/configuration.html#sensitive-configuration",
    "title": "Configuration",
    "section": "",
    "text": "For sensitive information like API keys and secrets, you can:\n\nUse environment variables\nUse a separate config file with restricted permissions\nUse a secret management system like HashiCorp Vault",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#configuration-examples",
    "href": "deployment/configuration.html#configuration-examples",
    "title": "Configuration",
    "section": "",
    "text": "task_directory: \"./data/tasks\"\ncontext_directory: \"./data/contexts\"\nparameter_set_directory: \"./data/parameter_sets\"\ndag_directory: \"./data/dags\"\n\nbackend: \"memory\"\n\nqueues:\n  - \"default\"\n\nsecurity:\n  enabled: false\n\nlogging:\n  level: \"debug\"\n\n\n\ntask_directory: \"/etc/cyclonetix/tasks\"\ncontext_directory: \"/etc/cyclonetix/contexts\"\nparameter_set_directory: \"/etc/cyclonetix/parameters\"\ndag_directory: \"/etc/cyclonetix/dags\"\n\nbackend: \"redis\"\nbackend_url: \"redis://redis.internal:6379\"\nserialization_format: \"binary\"\n\nqueues:\n  - \"default\"\n  - \"high_memory\"\n  - \"gpu_tasks\"\n  - \"etl\"\n\nsecurity:\n  enabled: true\n  oauth:\n    provider: \"google\"\n    client_id: \"${OAUTH_CLIENT_ID}\"\n    client_secret: \"${OAUTH_CLIENT_SECRET}\"\n    redirect_url: \"https://cyclonetix.example.com/auth/callback\"\n    allowed_domains: [\"example.com\"]\n  cookie_secret: \"${COOKIE_SECRET}\"\n  session_timeout_minutes: 480\n\nlogging:\n  level: \"info\"\n  format: \"json\"\n  file: \"/var/log/cyclonetix.log\"",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/configuration.html#next-steps",
    "href": "deployment/configuration.html#next-steps",
    "title": "Configuration",
    "section": "",
    "text": "Review Security Configuration for securing your deployment\nExplore Scaling Options for large deployments\nCheck the CLI Reference for command-line options",
    "crumbs": [
      "Deployment",
      "Configuration"
    ]
  },
  {
    "objectID": "deployment/security.html",
    "href": "deployment/security.html",
    "title": "Security",
    "section": "",
    "text": "This guide covers security considerations and best practices for deploying Cyclonetix in production environments.\n\n\n\n\nBy default, security is disabled in development mode. To enable security, update your configuration:\nsecurity:\n  enabled: true\n  # other security settings...\n\n\n\nCyclonetix supports several authentication methods:\n\n\nsecurity:\n  enabled: true\n  oauth:\n    provider: \"google\"  # Currently supported: google, github, azure\n    client_id: \"your-client-id\"\n    client_secret: \"your-client-secret\"\n    redirect_url: \"https://your-cyclonetix-instance.com/auth/callback\"\n    allowed_domains: [\"yourdomain.com\"]  # Optional: restrict by domain\n    allowed_emails: [\"user@example.com\"]  # Optional: restrict by email\nTo set up OAuth:\n\nCreate OAuth credentials with your provider\nConfigure the redirect URL to point to your Cyclonetix instance\nUpdate your Cyclonetix configuration with the client ID and secret\n\n\n\n\nFor simpler deployments, you can use basic authentication:\nsecurity:\n  enabled: true\n  basic_auth:\n    users:\n      - username: \"admin\"\n        password_hash: \"$2b$12$...\"  # BCrypt hash\n      - username: \"viewer\"\n        password_hash: \"$2b$12$...\"\nTo generate a password hash:\n# Install bcrypt tool\npip install bcrypt\n\n# Generate hash\npython -c \"import bcrypt; print(bcrypt.hashpw('your-password'.encode(), bcrypt.gensalt()).decode())\"\n\n\n\nFor programmatic access:\nsecurity:\n  enabled: true\n  api_keys:\n    - key: \"your-secret-api-key\"\n      name: \"ci-pipeline\"\n      roles: [\"scheduler\", \"viewer\"]\n    - key: \"another-secret-key\"\n      name: \"monitoring-system\"\n      roles: [\"viewer\"]\n\n\n\n\nConfigure different access levels with roles:\nsecurity:\n  enabled: true\n  roles:\n    - name: \"admin\"\n      permissions: [\"read\", \"write\", \"execute\", \"admin\"]\n    - name: \"scheduler\"\n      permissions: [\"read\", \"write\", \"execute\"]\n    - name: \"viewer\"\n      permissions: [\"read\"]\n\n  # Assign roles to users\n  user_roles:\n    \"user@example.com\": [\"admin\"]\n    \"team@example.com\": [\"scheduler\"]\n\n\n\n\nConfigure session settings:\nsecurity:\n  cookie_secret: \"use-a-random-string-here\"  # Used to sign cookies\n  session_timeout_minutes: 120                # Session duration\n  secure_cookies: true                        # Require HTTPS\n  same_site: \"lax\"                            # Cookie SameSite policy\nFor the cookie secret, generate a secure random string:\nopenssl rand -base64 32\n\n\n\n\n\nAlways use HTTPS in production. You can:\n\nUse a reverse proxy like Nginx with Let’s Encrypt\nUse a Kubernetes Ingress with TLS\nUse a cloud load balancer with TLS termination\n\n\n\n\nSpecify which paths are accessible without authentication:\nsecurity:\n  public_paths:\n    - \"/static/*\"    # Static assets\n    - \"/login\"       # Login page\n    - \"/auth/*\"      # OAuth callback\n    - \"/health\"      # Health check endpoint\n\n\n\nLimit access by IP address:\nsecurity:\n  allowed_ips:\n    - \"192.168.1.0/24\"  # Internal network\n    - \"203.0.113.42\"    # Specific IP\n\n\n\n\n\n\nCyclonetix itself doesn’t encrypt data at rest, but you can:\n\nUse encrypted filesystems for task/DAG storage\nEnable encryption in Redis or PostgreSQL\nUse cloud provider encryption options\n\n\n\n\nFor sensitive parameters:\n# Task definition with sensitive parameter\nparameters:\n  api_key:\n    value: \"${SECRET_API_KEY}\"\n    sensitive: true  # Will be masked in logs and UI\n\n\n\nFor production, consider using a secret management solution:\nsecurity:\n  secrets_backend: \"vault\"\n  vault:\n    url: \"https://vault.example.com:8200\"\n    token: \"${VAULT_TOKEN}\"\n    path: \"secret/cyclonetix\"\n\n\n\n\n\n\nLimit what commands can be executed:\nsecurity:\n  task_execution:\n    allowed_commands:\n      - \"/usr/bin/python\"\n      - \"/usr/local/bin/custom-script.sh\"\n    allowed_patterns:\n      - \"^/opt/cyclonetix/scripts/.*\\\\.py$\"\n\n\n\nFor better isolation, use:\n\nSeparate user accounts for the agent\nContainer-based execution\nResource limits\n\nagent:\n  execution:\n    user: \"cyclonetix-agent\"\n    cgroup_limits: true\n    memory_limit_mb: 1024\n    cpu_limit_percent: 50\n\n\n\n\nEnable comprehensive audit logging:\nsecurity:\n  audit_logging:\n    enabled: true\n    events:\n      - \"login\"\n      - \"logout\"\n      - \"schedule\"\n      - \"cancel\"\n      - \"admin_action\"\n    log_file: \"/var/log/cyclonetix-audit.log\"\n\n\n\nFor Redis backend security:\n\nEnable Redis authentication:\nbackend_url: \"redis://:password@redis-host:6379\"\nUse Redis TLS:\nbackend_url: \"rediss://:password@redis-host:6379\"\nConfigure Redis ACLs for least privilege\n\n\n\n\nWhen deploying on Kubernetes:\n\nUse namespaces for isolation\nConfigure appropriate SecurityContext\nUse network policies to restrict traffic\nApply Pod Security Standards\n\nExample Kubernetes manifest with security settings:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-agent\nspec:\n  # ...\n  template:\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n      containers:\n      - name: cyclonetix-agent\n        image: cyclonetix:latest\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop: [\"ALL\"]\n          readOnlyRootFilesystem: true\n        # ...\n\n\n\nBefore going to production, verify:\n\nAuthentication is enabled\nStrong, unique passwords or OAuth is configured\nHTTPS is enforced\nRedis is secured with authentication and TLS\nCookie secret is a strong random value\nProper role assignment for users\nAudit logging is enabled\nTask execution is properly isolated\nSensitive parameters are marked as such\nRegular security updates are applied\n\n\n\n\n\nPrinciple of Least Privilege: Give users and components only the permissions they need\nDefense in Depth: Apply multiple layers of security\nKeep Updated: Regularly update Cyclonetix and all dependencies\nSecurity Monitoring: Monitor for unusual activity\nRegular Audits: Periodically review permissions and security settings\n\n\n\n\n\nConfigure Deployment Scaling for your environment\nExplore Configuration Options in detail\nReview the Troubleshooting Guide for common issues",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#authentication-and-authorization",
    "href": "deployment/security.html#authentication-and-authorization",
    "title": "Security",
    "section": "",
    "text": "By default, security is disabled in development mode. To enable security, update your configuration:\nsecurity:\n  enabled: true\n  # other security settings...\n\n\n\nCyclonetix supports several authentication methods:\n\n\nsecurity:\n  enabled: true\n  oauth:\n    provider: \"google\"  # Currently supported: google, github, azure\n    client_id: \"your-client-id\"\n    client_secret: \"your-client-secret\"\n    redirect_url: \"https://your-cyclonetix-instance.com/auth/callback\"\n    allowed_domains: [\"yourdomain.com\"]  # Optional: restrict by domain\n    allowed_emails: [\"user@example.com\"]  # Optional: restrict by email\nTo set up OAuth:\n\nCreate OAuth credentials with your provider\nConfigure the redirect URL to point to your Cyclonetix instance\nUpdate your Cyclonetix configuration with the client ID and secret\n\n\n\n\nFor simpler deployments, you can use basic authentication:\nsecurity:\n  enabled: true\n  basic_auth:\n    users:\n      - username: \"admin\"\n        password_hash: \"$2b$12$...\"  # BCrypt hash\n      - username: \"viewer\"\n        password_hash: \"$2b$12$...\"\nTo generate a password hash:\n# Install bcrypt tool\npip install bcrypt\n\n# Generate hash\npython -c \"import bcrypt; print(bcrypt.hashpw('your-password'.encode(), bcrypt.gensalt()).decode())\"\n\n\n\nFor programmatic access:\nsecurity:\n  enabled: true\n  api_keys:\n    - key: \"your-secret-api-key\"\n      name: \"ci-pipeline\"\n      roles: [\"scheduler\", \"viewer\"]\n    - key: \"another-secret-key\"\n      name: \"monitoring-system\"\n      roles: [\"viewer\"]\n\n\n\n\nConfigure different access levels with roles:\nsecurity:\n  enabled: true\n  roles:\n    - name: \"admin\"\n      permissions: [\"read\", \"write\", \"execute\", \"admin\"]\n    - name: \"scheduler\"\n      permissions: [\"read\", \"write\", \"execute\"]\n    - name: \"viewer\"\n      permissions: [\"read\"]\n\n  # Assign roles to users\n  user_roles:\n    \"user@example.com\": [\"admin\"]\n    \"team@example.com\": [\"scheduler\"]",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#session-management",
    "href": "deployment/security.html#session-management",
    "title": "Security",
    "section": "",
    "text": "Configure session settings:\nsecurity:\n  cookie_secret: \"use-a-random-string-here\"  # Used to sign cookies\n  session_timeout_minutes: 120                # Session duration\n  secure_cookies: true                        # Require HTTPS\n  same_site: \"lax\"                            # Cookie SameSite policy\nFor the cookie secret, generate a secure random string:\nopenssl rand -base64 32",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#network-security",
    "href": "deployment/security.html#network-security",
    "title": "Security",
    "section": "",
    "text": "Always use HTTPS in production. You can:\n\nUse a reverse proxy like Nginx with Let’s Encrypt\nUse a Kubernetes Ingress with TLS\nUse a cloud load balancer with TLS termination\n\n\n\n\nSpecify which paths are accessible without authentication:\nsecurity:\n  public_paths:\n    - \"/static/*\"    # Static assets\n    - \"/login\"       # Login page\n    - \"/auth/*\"      # OAuth callback\n    - \"/health\"      # Health check endpoint\n\n\n\nLimit access by IP address:\nsecurity:\n  allowed_ips:\n    - \"192.168.1.0/24\"  # Internal network\n    - \"203.0.113.42\"    # Specific IP",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#data-security",
    "href": "deployment/security.html#data-security",
    "title": "Security",
    "section": "",
    "text": "Cyclonetix itself doesn’t encrypt data at rest, but you can:\n\nUse encrypted filesystems for task/DAG storage\nEnable encryption in Redis or PostgreSQL\nUse cloud provider encryption options\n\n\n\n\nFor sensitive parameters:\n# Task definition with sensitive parameter\nparameters:\n  api_key:\n    value: \"${SECRET_API_KEY}\"\n    sensitive: true  # Will be masked in logs and UI\n\n\n\nFor production, consider using a secret management solution:\nsecurity:\n  secrets_backend: \"vault\"\n  vault:\n    url: \"https://vault.example.com:8200\"\n    token: \"${VAULT_TOKEN}\"\n    path: \"secret/cyclonetix\"",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#task-execution-security",
    "href": "deployment/security.html#task-execution-security",
    "title": "Security",
    "section": "",
    "text": "Limit what commands can be executed:\nsecurity:\n  task_execution:\n    allowed_commands:\n      - \"/usr/bin/python\"\n      - \"/usr/local/bin/custom-script.sh\"\n    allowed_patterns:\n      - \"^/opt/cyclonetix/scripts/.*\\\\.py$\"\n\n\n\nFor better isolation, use:\n\nSeparate user accounts for the agent\nContainer-based execution\nResource limits\n\nagent:\n  execution:\n    user: \"cyclonetix-agent\"\n    cgroup_limits: true\n    memory_limit_mb: 1024\n    cpu_limit_percent: 50",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#audit-logging",
    "href": "deployment/security.html#audit-logging",
    "title": "Security",
    "section": "",
    "text": "Enable comprehensive audit logging:\nsecurity:\n  audit_logging:\n    enabled: true\n    events:\n      - \"login\"\n      - \"logout\"\n      - \"schedule\"\n      - \"cancel\"\n      - \"admin_action\"\n    log_file: \"/var/log/cyclonetix-audit.log\"",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#securing-redis",
    "href": "deployment/security.html#securing-redis",
    "title": "Security",
    "section": "",
    "text": "For Redis backend security:\n\nEnable Redis authentication:\nbackend_url: \"redis://:password@redis-host:6379\"\nUse Redis TLS:\nbackend_url: \"rediss://:password@redis-host:6379\"\nConfigure Redis ACLs for least privilege",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#kubernetes-security",
    "href": "deployment/security.html#kubernetes-security",
    "title": "Security",
    "section": "",
    "text": "When deploying on Kubernetes:\n\nUse namespaces for isolation\nConfigure appropriate SecurityContext\nUse network policies to restrict traffic\nApply Pod Security Standards\n\nExample Kubernetes manifest with security settings:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-agent\nspec:\n  # ...\n  template:\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n      containers:\n      - name: cyclonetix-agent\n        image: cyclonetix:latest\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop: [\"ALL\"]\n          readOnlyRootFilesystem: true\n        # ...",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#security-checklist",
    "href": "deployment/security.html#security-checklist",
    "title": "Security",
    "section": "",
    "text": "Before going to production, verify:\n\nAuthentication is enabled\nStrong, unique passwords or OAuth is configured\nHTTPS is enforced\nRedis is secured with authentication and TLS\nCookie secret is a strong random value\nProper role assignment for users\nAudit logging is enabled\nTask execution is properly isolated\nSensitive parameters are marked as such\nRegular security updates are applied",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#security-best-practices",
    "href": "deployment/security.html#security-best-practices",
    "title": "Security",
    "section": "",
    "text": "Principle of Least Privilege: Give users and components only the permissions they need\nDefense in Depth: Apply multiple layers of security\nKeep Updated: Regularly update Cyclonetix and all dependencies\nSecurity Monitoring: Monitor for unusual activity\nRegular Audits: Periodically review permissions and security settings",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "deployment/security.html#next-steps",
    "href": "deployment/security.html#next-steps",
    "title": "Security",
    "section": "",
    "text": "Configure Deployment Scaling for your environment\nExplore Configuration Options in detail\nReview the Troubleshooting Guide for common issues",
    "crumbs": [
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html",
    "href": "advanced/evaluation-points.html",
    "title": "Evaluation Points",
    "section": "",
    "text": "Evaluation points are one of Cyclonetix’s most powerful features, allowing for dynamic decision-making within workflows. This guide explains how to use evaluation points to create adaptive, intelligent workflows.\n\n\nAn evaluation point is a special task that can:\n\nMake decisions about what happens next in a workflow\nDynamically modify the execution graph\nConditionally execute downstream tasks\nServe as approval gates for human intervention\nIntegrate with external systems to determine workflow paths\n\n\n\n\nWhen a task is marked as an evaluation point:\n\nThe task executes normally like any other task\nAfter completion, its output is analyzed\nBased on the output, the workflow’s execution graph may be modified\nThe orchestrator reevaluates the graph with these modifications\n\n\n\n\nTo make a task an evaluation point, set the evaluation_point flag to true in the task definition:\nid: \"evaluate_model\"\nname: \"Evaluate Model Performance\"\ncommand: \"python evaluate.py --model ${MODEL_PATH} --threshold ${THRESHOLD}\"\ndependencies:\n  - \"train_model\"\nevaluation_point: true\nparameters:\n  threshold: 0.85\n\n\n\n\n\nThe simplest way to implement an evaluation point is using exit codes:\n#!/bin/bash\n# evaluate_data.sh\n\n# Run validation\n./validate_data.py --input ${INPUT_PATH}\n\n# Check validation result\nif [ $? -eq 0 ]; then\n  # Data is valid, proceed to full processing\n  echo \"CYCLO_NEXT=process_data\" &gt; $CYCLO_EVAL_RESULT\n  exit 0\nelse\n  # Data is invalid, go to error handling\n  echo \"CYCLO_NEXT=handle_invalid_data\" &gt; $CYCLO_EVAL_RESULT\n  exit 1\nfi\n\n\n\nFor more complex scenarios, you can output a JSON result:\n# evaluate_model.py\nimport json\nimport sys\n\n# Perform model evaluation\naccuracy = evaluate_model(model_path)\n\n# Decide next steps based on accuracy\nresult = {\n    \"metrics\": {\n        \"accuracy\": accuracy\n    }\n}\n\nif accuracy &gt;= 0.90:\n    result[\"next_tasks\"] = [\"deploy_model\", \"notify_success\"]\nelif accuracy &gt;= 0.75:\n    result[\"next_tasks\"] = [\"tune_model\", \"retry_training\"]\nelse:\n    result[\"next_tasks\"] = [\"notify_failure\"]\n\n# Write result to the evaluation result file\nwith open(os.environ.get(\"CYCLO_EVAL_RESULT\", \"eval_result.json\"), \"w\") as f:\n    json.dump(result, f)\n\n# Exit with appropriate code\nsys.exit(0 if accuracy &gt;= 0.75 else 1)\n\n\n\n\nThe evaluation result can specify:\n\nNext tasks: Which tasks should be executed next\nParameters: Parameters for those tasks\nContext updates: Changes to the execution context\nMetadata: Additional information about the evaluation\n\nThe evaluation result file (specified by the CYCLO_EVAL_RESULT environment variable) should contain a JSON object with the following structure:\n{\n  \"next_tasks\": [\"task_id_1\", \"task_id_2\"],\n  \"parameters\": {\n    \"task_id_1\": {\n      \"param1\": \"value1\",\n      \"param2\": \"value2\"\n    },\n    \"task_id_2\": {\n      \"param1\": \"value1\"\n    }\n  },\n  \"context_updates\": {\n    \"variable1\": \"new_value\",\n    \"variable2\": \"new_value\"\n  },\n  \"metadata\": {\n    \"reason\": \"Model accuracy below threshold\",\n    \"metrics\": {\n      \"accuracy\": 0.82,\n      \"precision\": 0.79\n    }\n  }\n}\n\n\n\n\n\nOne of the most common uses is to create branches in your workflow:\nid: \"check_data_quality\"\nname: \"Check Data Quality\"\ncommand: \"python scripts/validate_data.py --input ${INPUT_PATH} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"ingest_data\"\nevaluation_point: true\nThe script might output something like:\n{\n  \"next_tasks\": [\"process_clean_data\"],\n  \"metadata\": {\n    \"quality_score\": 98,\n    \"issues_found\": 0\n  }\n}\nOr if problems are found:\n{\n  \"next_tasks\": [\"clean_data\"],\n  \"metadata\": {\n    \"quality_score\": 68,\n    \"issues_found\": 12\n  }\n}\n\n\n\nEvaluation points can serve as approval gates requiring human intervention:\nid: \"approval_gate\"\nname: \"Approve Production Deployment\"\ncommand: \"python scripts/wait_for_approval.py --model ${MODEL_NAME}\"\ndependencies:\n  - \"validate_model\"\nevaluation_point: true\nThe wait_for_approval.py script might wait for a response via API or UI interaction.\n\n\n\nEvaluation points can dynamically determine which tasks to run:\nid: \"analyze_data_types\"\nname: \"Analyze Data Types\"\ncommand: \"python scripts/data_analyzer.py --input ${INPUT_PATH} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"load_data\"\nevaluation_point: true\nThe script might detect different data types and schedule appropriate processing tasks:\n{\n  \"next_tasks\": [\"process_images\", \"process_text\", \"process_numerical\"],\n  \"parameters\": {\n    \"process_images\": {\n      \"count\": 150,\n      \"path\": \"/data/images\"\n    },\n    \"process_text\": {\n      \"count\": 500,\n      \"language\": \"en\"\n    }\n  }\n}\n\n\n\n\nEvaluation points are particularly powerful when combined with AI for intelligent workflow orchestration:\nid: \"ai_analyze_results\"\nname: \"AI Result Analysis\"\ncommand: \"python scripts/ai_analyzer.py --results ${RESULTS_PATH} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"run_experiment\"\nevaluation_point: true\nThe AI analyzer might make sophisticated decisions about next steps based on experiment results.\n\n\n\n\nKeep evaluation logic focused - Evaluation points should make decisions, not perform heavy processing\nPrefer declarative over imperative - Specify what should happen, not how it should happen\nInclude clear metadata - Document why decisions were made for better observability\nHandle failure gracefully - Ensure evaluation points have clear error paths\nTest with different scenarios - Verify all possible branches work as expected\n\n\n\n\nTo debug evaluation points:\n\nEnable verbose logging in your evaluation scripts\nCheck the evaluation result file to ensure it contains valid JSON\nReview orchestrator logs for decision-making information\nUse the UI visualization to see how the graph changes after evaluation\n\n\n\n\n\nLearn about Contexts & Parameters to complement evaluation points\nExplore Git Integration for version-controlled workflows\nSee the Troubleshooting & FAQ for common issues",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#what-are-evaluation-points",
    "href": "advanced/evaluation-points.html#what-are-evaluation-points",
    "title": "Evaluation Points",
    "section": "",
    "text": "An evaluation point is a special task that can:\n\nMake decisions about what happens next in a workflow\nDynamically modify the execution graph\nConditionally execute downstream tasks\nServe as approval gates for human intervention\nIntegrate with external systems to determine workflow paths",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#how-evaluation-points-work",
    "href": "advanced/evaluation-points.html#how-evaluation-points-work",
    "title": "Evaluation Points",
    "section": "",
    "text": "When a task is marked as an evaluation point:\n\nThe task executes normally like any other task\nAfter completion, its output is analyzed\nBased on the output, the workflow’s execution graph may be modified\nThe orchestrator reevaluates the graph with these modifications",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#defining-an-evaluation-point",
    "href": "advanced/evaluation-points.html#defining-an-evaluation-point",
    "title": "Evaluation Points",
    "section": "",
    "text": "To make a task an evaluation point, set the evaluation_point flag to true in the task definition:\nid: \"evaluate_model\"\nname: \"Evaluate Model Performance\"\ncommand: \"python evaluate.py --model ${MODEL_PATH} --threshold ${THRESHOLD}\"\ndependencies:\n  - \"train_model\"\nevaluation_point: true\nparameters:\n  threshold: 0.85",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#implementation-patterns",
    "href": "advanced/evaluation-points.html#implementation-patterns",
    "title": "Evaluation Points",
    "section": "",
    "text": "The simplest way to implement an evaluation point is using exit codes:\n#!/bin/bash\n# evaluate_data.sh\n\n# Run validation\n./validate_data.py --input ${INPUT_PATH}\n\n# Check validation result\nif [ $? -eq 0 ]; then\n  # Data is valid, proceed to full processing\n  echo \"CYCLO_NEXT=process_data\" &gt; $CYCLO_EVAL_RESULT\n  exit 0\nelse\n  # Data is invalid, go to error handling\n  echo \"CYCLO_NEXT=handle_invalid_data\" &gt; $CYCLO_EVAL_RESULT\n  exit 1\nfi\n\n\n\nFor more complex scenarios, you can output a JSON result:\n# evaluate_model.py\nimport json\nimport sys\n\n# Perform model evaluation\naccuracy = evaluate_model(model_path)\n\n# Decide next steps based on accuracy\nresult = {\n    \"metrics\": {\n        \"accuracy\": accuracy\n    }\n}\n\nif accuracy &gt;= 0.90:\n    result[\"next_tasks\"] = [\"deploy_model\", \"notify_success\"]\nelif accuracy &gt;= 0.75:\n    result[\"next_tasks\"] = [\"tune_model\", \"retry_training\"]\nelse:\n    result[\"next_tasks\"] = [\"notify_failure\"]\n\n# Write result to the evaluation result file\nwith open(os.environ.get(\"CYCLO_EVAL_RESULT\", \"eval_result.json\"), \"w\") as f:\n    json.dump(result, f)\n\n# Exit with appropriate code\nsys.exit(0 if accuracy &gt;= 0.75 else 1)",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#evaluation-results-specification",
    "href": "advanced/evaluation-points.html#evaluation-results-specification",
    "title": "Evaluation Points",
    "section": "",
    "text": "The evaluation result can specify:\n\nNext tasks: Which tasks should be executed next\nParameters: Parameters for those tasks\nContext updates: Changes to the execution context\nMetadata: Additional information about the evaluation\n\nThe evaluation result file (specified by the CYCLO_EVAL_RESULT environment variable) should contain a JSON object with the following structure:\n{\n  \"next_tasks\": [\"task_id_1\", \"task_id_2\"],\n  \"parameters\": {\n    \"task_id_1\": {\n      \"param1\": \"value1\",\n      \"param2\": \"value2\"\n    },\n    \"task_id_2\": {\n      \"param1\": \"value1\"\n    }\n  },\n  \"context_updates\": {\n    \"variable1\": \"new_value\",\n    \"variable2\": \"new_value\"\n  },\n  \"metadata\": {\n    \"reason\": \"Model accuracy below threshold\",\n    \"metrics\": {\n      \"accuracy\": 0.82,\n      \"precision\": 0.79\n    }\n  }\n}",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#common-use-cases",
    "href": "advanced/evaluation-points.html#common-use-cases",
    "title": "Evaluation Points",
    "section": "",
    "text": "One of the most common uses is to create branches in your workflow:\nid: \"check_data_quality\"\nname: \"Check Data Quality\"\ncommand: \"python scripts/validate_data.py --input ${INPUT_PATH} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"ingest_data\"\nevaluation_point: true\nThe script might output something like:\n{\n  \"next_tasks\": [\"process_clean_data\"],\n  \"metadata\": {\n    \"quality_score\": 98,\n    \"issues_found\": 0\n  }\n}\nOr if problems are found:\n{\n  \"next_tasks\": [\"clean_data\"],\n  \"metadata\": {\n    \"quality_score\": 68,\n    \"issues_found\": 12\n  }\n}\n\n\n\nEvaluation points can serve as approval gates requiring human intervention:\nid: \"approval_gate\"\nname: \"Approve Production Deployment\"\ncommand: \"python scripts/wait_for_approval.py --model ${MODEL_NAME}\"\ndependencies:\n  - \"validate_model\"\nevaluation_point: true\nThe wait_for_approval.py script might wait for a response via API or UI interaction.\n\n\n\nEvaluation points can dynamically determine which tasks to run:\nid: \"analyze_data_types\"\nname: \"Analyze Data Types\"\ncommand: \"python scripts/data_analyzer.py --input ${INPUT_PATH} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"load_data\"\nevaluation_point: true\nThe script might detect different data types and schedule appropriate processing tasks:\n{\n  \"next_tasks\": [\"process_images\", \"process_text\", \"process_numerical\"],\n  \"parameters\": {\n    \"process_images\": {\n      \"count\": 150,\n      \"path\": \"/data/images\"\n    },\n    \"process_text\": {\n      \"count\": 500,\n      \"language\": \"en\"\n    }\n  }\n}",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#integration-with-ai-decision-making",
    "href": "advanced/evaluation-points.html#integration-with-ai-decision-making",
    "title": "Evaluation Points",
    "section": "",
    "text": "Evaluation points are particularly powerful when combined with AI for intelligent workflow orchestration:\nid: \"ai_analyze_results\"\nname: \"AI Result Analysis\"\ncommand: \"python scripts/ai_analyzer.py --results ${RESULTS_PATH} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"run_experiment\"\nevaluation_point: true\nThe AI analyzer might make sophisticated decisions about next steps based on experiment results.",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#best-practices",
    "href": "advanced/evaluation-points.html#best-practices",
    "title": "Evaluation Points",
    "section": "",
    "text": "Keep evaluation logic focused - Evaluation points should make decisions, not perform heavy processing\nPrefer declarative over imperative - Specify what should happen, not how it should happen\nInclude clear metadata - Document why decisions were made for better observability\nHandle failure gracefully - Ensure evaluation points have clear error paths\nTest with different scenarios - Verify all possible branches work as expected",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#debugging-evaluation-points",
    "href": "advanced/evaluation-points.html#debugging-evaluation-points",
    "title": "Evaluation Points",
    "section": "",
    "text": "To debug evaluation points:\n\nEnable verbose logging in your evaluation scripts\nCheck the evaluation result file to ensure it contains valid JSON\nReview orchestrator logs for decision-making information\nUse the UI visualization to see how the graph changes after evaluation",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/evaluation-points.html#next-steps",
    "href": "advanced/evaluation-points.html#next-steps",
    "title": "Evaluation Points",
    "section": "",
    "text": "Learn about Contexts & Parameters to complement evaluation points\nExplore Git Integration for version-controlled workflows\nSee the Troubleshooting & FAQ for common issues",
    "crumbs": [
      "Advanced Features",
      "Evaluation Points"
    ]
  },
  {
    "objectID": "advanced/git-integration.html",
    "href": "advanced/git-integration.html",
    "title": "Git Integration",
    "section": "",
    "text": "Cyclonetix provides robust Git integration features, allowing you to execute code directly from Git repositories and manage workflow definitions as code. This approach ensures versioning, repeatability, and easier collaboration.\n\n\n\n\nInstead of uploading scripts or defining inline commands, you can reference code in Git repositories:\nid: \"train_model\"\nname: \"Train ML Model\"\ncommand: \"git-exec https://github.com/example/ml-repo.git scripts/train.py --epochs ${EPOCHS}\"\nparameters:\n  EPOCHS: \"100\"\nThe git-exec prefix tells Cyclonetix to:\n\nClone the repository\nCheck out the default branch (or specified branch/tag)\nExecute the specified command within the repository\n\n\n\n\nReference specific versions of your code:\n# Using a branch\ncommand: \"git-exec https://github.com/example/repo.git@develop scripts/process.py\"\n\n# Using a tag\ncommand: \"git-exec https://github.com/example/repo.git@v1.2.3 scripts/process.py\"\n\n# Using a specific commit\ncommand: \"git-exec https://github.com/example/repo.git@a1b2c3d scripts/process.py\"\n\n\n\nFor private repositories, configure authentication:\ngit:\n  auth:\n    # SSH key-based auth\n    ssh_key_path: \"/path/to/private/key\"\n\n    # Or HTTPS with credentials\n    credentials:\n      - domain: \"github.com\"\n        username: \"git-user\"\n        password: \"${GITHUB_TOKEN}\"  # Use personal access token\n\n      - domain: \"gitlab.example.com\"\n        username: \"git-user\"\n        password: \"${GITLAB_TOKEN}\"\n\n\n\nTo avoid repeated cloning, enable repository caching:\ngit:\n  cache:\n    enabled: true\n    path: \"/tmp/cyclonetix-git-cache\"\n    max_size_mb: 1000\n    ttl_minutes: 60\n\n\n\n\n\n\nKeep your task and DAG definitions in a Git repository:\ngit_sync:\n  enabled: true\n  repositories:\n    - url: \"https://github.com/example/workflows.git\"\n      branch: \"main\"\n      paths:\n        - source: \"tasks/\"\n          destination: \"${CYCLO_TASK_DIRECTORY}\"\n        - source: \"dags/\"\n          destination: \"${CYCLO_DAG_DIRECTORY}\"\n      poll_interval_seconds: 60\n      credentials:\n        username: \"git-user\"\n        password: \"${GIT_TOKEN}\"\nThis configuration: 1. Periodically pulls from the repository 2. Syncs task and DAG definitions to their respective directories 3. Automatically updates workflows when changes are pushed\n\n\n\nUse Git references to run specific versions of workflows:\n# Run a DAG from a specific branch\n./cyclonetix schedule-dag my_workflow --git-ref develop\n\n# Run a DAG from a specific tag\n./cyclonetix schedule-dag my_workflow --git-ref v1.2.3\n\n\n\n\nImplement a GitOps approach to workflow management:\n\nStore all workflow definitions in Git\nUse pull requests for workflow changes\nAutomatically deploy changes when merged\nTrack workflow history through Git history\n\nExample GitOps setup:\ngit_sync:\n  enabled: true\n  repositories:\n    - url: \"https://github.com/example/workflows.git\"\n      branch: \"main\"\n      paths:\n        - source: \"/\"\n          destination: \"${CYCLO_CONFIG_DIRECTORY}\"\n  webhook:\n    enabled: true\n    endpoint: \"/api/git-sync/webhook\"\n    secret: \"${WEBHOOK_SECRET}\"\nConfigure your Git provider to send webhooks when changes are pushed, triggering immediate sync.\n\n\n\n\n\nReference code from multiple repositories in a single workflow:\n# Task 1 from repo A\nid: \"data_prep\"\ncommand: \"git-exec https://github.com/example/data-tools.git scripts/prepare.py\"\n\n# Task 2 from repo B\nid: \"model_training\"\ncommand: \"git-exec https://github.com/example/ml-models.git scripts/train.py\"\ndependencies:\n  - \"data_prep\"\n\n\n\nWork with repositories that use Git submodules:\ncommand: \"git-exec https://github.com/example/main-repo.git --recursive scripts/run.sh\"\nThe --recursive flag tells Cyclonetix to initialize and update submodules.\n\n\n\nSet up specific environments for Git-based execution:\ngit:\n  environments:\n    - name: \"python-data-science\"\n      setup_commands:\n        - \"python -m venv .venv\"\n        - \"source .venv/bin/activate\"\n        - \"pip install -r requirements.txt\"\n      pre_execute: \"source .venv/bin/activate\"\n\n    - name: \"nodejs\"\n      setup_commands:\n        - \"npm install\"\n      pre_execute: \"\"\nReference the environment in your task:\nid: \"run_analysis\"\ncommand: \"git-exec https://github.com/example/data-analysis.git --env python-data-science scripts/analyze.py\"\n\n\n\nFor large repositories, use sparse checkout to only download necessary files:\ncommand: \"git-exec https://github.com/example/large-repo.git --sparse=scripts,configs scripts/run.py\"\nThis only checks out the scripts and configs directories, saving time and space.\n\n\n\n\n\n\nLimit which repositories can be used:\ngit:\n  security:\n    allowed_domains:\n      - \"github.com\"\n      - \"gitlab.example.com\"\n    allowed_repos:\n      - \"github.com/trusted-org/*\"\n      - \"gitlab.example.com/internal/*\"\n    disallowed_repos:\n      - \"github.com/untrusted-org/risky-repo\"\n\n\n\nIntegrate with scanning tools to check code before execution:\ngit:\n  security:\n    scan:\n      enabled: true\n      command: \"trivy fs --security-checks vuln,config,secret {repo_path}\"\n      fail_on_findings: true\n\n\n\nSecurely manage Git credentials:\n\nUse environment variables for tokens\nUse SSH agent forwarding when possible\nUse short-lived credentials\nConsider integration with vault systems\n\n\n\n\n\n\nUse Pinned References: Always pin to specific commits or tags in production\nKeep Repositories Clean: Structure repositories for clarity and minimal dependencies\nUse Feature Branches: Develop new workflows in feature branches\nTest Before Merging: Validate workflows in a test environment\nMonitor Git Performance: Large repositories can slow down task startup\n\n\n\n\nHere’s a complete example of a Git-integrated workflow:\n# Task definitions\n- id: \"fetch_data\"\n  name: \"Fetch Latest Data\"\n  command: \"git-exec https://github.com/example/data-tools.git@v2.1.0 --env python-data scripts/fetch.py --source ${DATA_SOURCE}\"\n  parameters:\n    DATA_SOURCE: \"production_api\"\n\n- id: \"process_data\"\n  name: \"Process Raw Data\"\n  command: \"git-exec https://github.com/example/data-tools.git@v2.1.0 --env python-data scripts/process.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\n  dependencies:\n    - \"fetch_data\"\n  parameters:\n    INPUT_PATH: \"/data/raw\"\n    OUTPUT_PATH: \"/data/processed\"\n\n- id: \"train_model\"\n  name: \"Train ML Model\"\n  command: \"git-exec https://github.com/example/ml-models.git@stable --env ml-training scripts/train.py --data ${DATA_PATH} --epochs ${EPOCHS} --model-type ${MODEL_TYPE}\"\n  dependencies:\n    - \"process_data\"\n  parameters:\n    DATA_PATH: \"/data/processed\"\n    EPOCHS: \"50\"\n    MODEL_TYPE: \"xgboost\"\n  queue: \"gpu_tasks\"\n\n- id: \"evaluate_model\"\n  name: \"Evaluate Model Performance\"\n  command: \"git-exec https://github.com/example/ml-models.git@stable --env ml-training scripts/evaluate.py --model ${MODEL_PATH} --test-data ${TEST_DATA}\"\n  dependencies:\n    - \"train_model\"\n  parameters:\n    MODEL_PATH: \"/models/latest\"\n    TEST_DATA: \"/data/test\"\n  evaluation_point: true\n\n\n\n\nExplore Contexts & Parameters for managing environment variables\nLearn about Evaluation Points for dynamic workflows\nCheck the Developer Guide for extending Git integration",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "advanced/git-integration.html#git-based-task-execution",
    "href": "advanced/git-integration.html#git-based-task-execution",
    "title": "Git Integration",
    "section": "",
    "text": "Instead of uploading scripts or defining inline commands, you can reference code in Git repositories:\nid: \"train_model\"\nname: \"Train ML Model\"\ncommand: \"git-exec https://github.com/example/ml-repo.git scripts/train.py --epochs ${EPOCHS}\"\nparameters:\n  EPOCHS: \"100\"\nThe git-exec prefix tells Cyclonetix to:\n\nClone the repository\nCheck out the default branch (or specified branch/tag)\nExecute the specified command within the repository\n\n\n\n\nReference specific versions of your code:\n# Using a branch\ncommand: \"git-exec https://github.com/example/repo.git@develop scripts/process.py\"\n\n# Using a tag\ncommand: \"git-exec https://github.com/example/repo.git@v1.2.3 scripts/process.py\"\n\n# Using a specific commit\ncommand: \"git-exec https://github.com/example/repo.git@a1b2c3d scripts/process.py\"\n\n\n\nFor private repositories, configure authentication:\ngit:\n  auth:\n    # SSH key-based auth\n    ssh_key_path: \"/path/to/private/key\"\n\n    # Or HTTPS with credentials\n    credentials:\n      - domain: \"github.com\"\n        username: \"git-user\"\n        password: \"${GITHUB_TOKEN}\"  # Use personal access token\n\n      - domain: \"gitlab.example.com\"\n        username: \"git-user\"\n        password: \"${GITLAB_TOKEN}\"\n\n\n\nTo avoid repeated cloning, enable repository caching:\ngit:\n  cache:\n    enabled: true\n    path: \"/tmp/cyclonetix-git-cache\"\n    max_size_mb: 1000\n    ttl_minutes: 60",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "advanced/git-integration.html#git-based-workflow-definitions",
    "href": "advanced/git-integration.html#git-based-workflow-definitions",
    "title": "Git Integration",
    "section": "",
    "text": "Keep your task and DAG definitions in a Git repository:\ngit_sync:\n  enabled: true\n  repositories:\n    - url: \"https://github.com/example/workflows.git\"\n      branch: \"main\"\n      paths:\n        - source: \"tasks/\"\n          destination: \"${CYCLO_TASK_DIRECTORY}\"\n        - source: \"dags/\"\n          destination: \"${CYCLO_DAG_DIRECTORY}\"\n      poll_interval_seconds: 60\n      credentials:\n        username: \"git-user\"\n        password: \"${GIT_TOKEN}\"\nThis configuration: 1. Periodically pulls from the repository 2. Syncs task and DAG definitions to their respective directories 3. Automatically updates workflows when changes are pushed\n\n\n\nUse Git references to run specific versions of workflows:\n# Run a DAG from a specific branch\n./cyclonetix schedule-dag my_workflow --git-ref develop\n\n# Run a DAG from a specific tag\n./cyclonetix schedule-dag my_workflow --git-ref v1.2.3",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "advanced/git-integration.html#gitops-workflow",
    "href": "advanced/git-integration.html#gitops-workflow",
    "title": "Git Integration",
    "section": "",
    "text": "Implement a GitOps approach to workflow management:\n\nStore all workflow definitions in Git\nUse pull requests for workflow changes\nAutomatically deploy changes when merged\nTrack workflow history through Git history\n\nExample GitOps setup:\ngit_sync:\n  enabled: true\n  repositories:\n    - url: \"https://github.com/example/workflows.git\"\n      branch: \"main\"\n      paths:\n        - source: \"/\"\n          destination: \"${CYCLO_CONFIG_DIRECTORY}\"\n  webhook:\n    enabled: true\n    endpoint: \"/api/git-sync/webhook\"\n    secret: \"${WEBHOOK_SECRET}\"\nConfigure your Git provider to send webhooks when changes are pushed, triggering immediate sync.",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "advanced/git-integration.html#advanced-git-features",
    "href": "advanced/git-integration.html#advanced-git-features",
    "title": "Git Integration",
    "section": "",
    "text": "Reference code from multiple repositories in a single workflow:\n# Task 1 from repo A\nid: \"data_prep\"\ncommand: \"git-exec https://github.com/example/data-tools.git scripts/prepare.py\"\n\n# Task 2 from repo B\nid: \"model_training\"\ncommand: \"git-exec https://github.com/example/ml-models.git scripts/train.py\"\ndependencies:\n  - \"data_prep\"\n\n\n\nWork with repositories that use Git submodules:\ncommand: \"git-exec https://github.com/example/main-repo.git --recursive scripts/run.sh\"\nThe --recursive flag tells Cyclonetix to initialize and update submodules.\n\n\n\nSet up specific environments for Git-based execution:\ngit:\n  environments:\n    - name: \"python-data-science\"\n      setup_commands:\n        - \"python -m venv .venv\"\n        - \"source .venv/bin/activate\"\n        - \"pip install -r requirements.txt\"\n      pre_execute: \"source .venv/bin/activate\"\n\n    - name: \"nodejs\"\n      setup_commands:\n        - \"npm install\"\n      pre_execute: \"\"\nReference the environment in your task:\nid: \"run_analysis\"\ncommand: \"git-exec https://github.com/example/data-analysis.git --env python-data-science scripts/analyze.py\"\n\n\n\nFor large repositories, use sparse checkout to only download necessary files:\ncommand: \"git-exec https://github.com/example/large-repo.git --sparse=scripts,configs scripts/run.py\"\nThis only checks out the scripts and configs directories, saving time and space.",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "advanced/git-integration.html#security-considerations",
    "href": "advanced/git-integration.html#security-considerations",
    "title": "Git Integration",
    "section": "",
    "text": "Limit which repositories can be used:\ngit:\n  security:\n    allowed_domains:\n      - \"github.com\"\n      - \"gitlab.example.com\"\n    allowed_repos:\n      - \"github.com/trusted-org/*\"\n      - \"gitlab.example.com/internal/*\"\n    disallowed_repos:\n      - \"github.com/untrusted-org/risky-repo\"\n\n\n\nIntegrate with scanning tools to check code before execution:\ngit:\n  security:\n    scan:\n      enabled: true\n      command: \"trivy fs --security-checks vuln,config,secret {repo_path}\"\n      fail_on_findings: true\n\n\n\nSecurely manage Git credentials:\n\nUse environment variables for tokens\nUse SSH agent forwarding when possible\nUse short-lived credentials\nConsider integration with vault systems",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "advanced/git-integration.html#best-practices",
    "href": "advanced/git-integration.html#best-practices",
    "title": "Git Integration",
    "section": "",
    "text": "Use Pinned References: Always pin to specific commits or tags in production\nKeep Repositories Clean: Structure repositories for clarity and minimal dependencies\nUse Feature Branches: Develop new workflows in feature branches\nTest Before Merging: Validate workflows in a test environment\nMonitor Git Performance: Large repositories can slow down task startup",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "advanced/git-integration.html#example-workflow",
    "href": "advanced/git-integration.html#example-workflow",
    "title": "Git Integration",
    "section": "",
    "text": "Here’s a complete example of a Git-integrated workflow:\n# Task definitions\n- id: \"fetch_data\"\n  name: \"Fetch Latest Data\"\n  command: \"git-exec https://github.com/example/data-tools.git@v2.1.0 --env python-data scripts/fetch.py --source ${DATA_SOURCE}\"\n  parameters:\n    DATA_SOURCE: \"production_api\"\n\n- id: \"process_data\"\n  name: \"Process Raw Data\"\n  command: \"git-exec https://github.com/example/data-tools.git@v2.1.0 --env python-data scripts/process.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\n  dependencies:\n    - \"fetch_data\"\n  parameters:\n    INPUT_PATH: \"/data/raw\"\n    OUTPUT_PATH: \"/data/processed\"\n\n- id: \"train_model\"\n  name: \"Train ML Model\"\n  command: \"git-exec https://github.com/example/ml-models.git@stable --env ml-training scripts/train.py --data ${DATA_PATH} --epochs ${EPOCHS} --model-type ${MODEL_TYPE}\"\n  dependencies:\n    - \"process_data\"\n  parameters:\n    DATA_PATH: \"/data/processed\"\n    EPOCHS: \"50\"\n    MODEL_TYPE: \"xgboost\"\n  queue: \"gpu_tasks\"\n\n- id: \"evaluate_model\"\n  name: \"Evaluate Model Performance\"\n  command: \"git-exec https://github.com/example/ml-models.git@stable --env ml-training scripts/evaluate.py --model ${MODEL_PATH} --test-data ${TEST_DATA}\"\n  dependencies:\n    - \"train_model\"\n  parameters:\n    MODEL_PATH: \"/models/latest\"\n    TEST_DATA: \"/data/test\"\n  evaluation_point: true",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "advanced/git-integration.html#next-steps",
    "href": "advanced/git-integration.html#next-steps",
    "title": "Git Integration",
    "section": "",
    "text": "Explore Contexts & Parameters for managing environment variables\nLearn about Evaluation Points for dynamic workflows\nCheck the Developer Guide for extending Git integration",
    "crumbs": [
      "Advanced Features",
      "Git Integration"
    ]
  },
  {
    "objectID": "reference/configuration.html",
    "href": "reference/configuration.html",
    "title": "Configuration Reference",
    "section": "",
    "text": "This page provides a comprehensive reference for all configuration options available in Cyclonetix. The configuration is defined in a YAML file, by default named config.yaml.\n\n\nBy default, Cyclonetix looks for config.yaml in the current working directory. You can specify a different location using the --config command-line flag:\n./cyclonetix --config /path/to/config.yaml\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\ntask_directory\nstring\n\"./data/tasks\"\nDirectory containing task definitions\n\n\ncontext_directory\nstring\n\"./data/contexts\"\nDirectory containing context definitions\n\n\nparameter_set_directory\nstring\n\"./data/parameter_sets\"\nDirectory containing parameter sets\n\n\ndag_directory\nstring\n\"./data/dags\"\nDirectory containing DAG definitions\n\n\n\nExample:\ntask_directory: \"/etc/cyclonetix/tasks\"\ncontext_directory: \"/etc/cyclonetix/contexts\"\nparameter_set_directory: \"/etc/cyclonetix/parameter_sets\"\ndag_directory: \"/etc/cyclonetix/dags\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nbackend\nstring\n\"memory\"\nBackend type (\"memory\", \"redis\", or \"postgresql\")\n\n\nbackend_url\nstring\n\"\"\nConnection URL for the backend\n\n\nserialization_format\nstring\n\"json\"\nFormat for serializing data (\"json\" or \"binary\")\n\n\ncluster_id\nstring\n\"default_cluster\"\nUnique identifier for this Cyclonetix cluster\n\n\n\nExample:\nbackend: \"redis\"\nbackend_url: \"redis://localhost:6379\"\nserialization_format: \"binary\"\ncluster_id: \"production_cluster\"\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nredis.cluster_mode\nboolean\nfalse\nWhether to use Redis cluster mode\n\n\nredis.read_from_replicas\nboolean\nfalse\nWhether to read from Redis replicas\n\n\nredis.connection_pool_size\ninteger\n10\nSize of the Redis connection pool\n\n\nredis.connection_timeout_ms\ninteger\n1000\nConnection timeout in milliseconds\n\n\nredis.retry_interval_ms\ninteger\n100\nRetry interval in milliseconds\n\n\nredis.max_retries\ninteger\n3\nMaximum number of connection retries\n\n\n\nExample:\nredis:\n  cluster_mode: true\n  read_from_replicas: true\n  connection_pool_size: 20\n  connection_timeout_ms: 2000\n  retry_interval_ms: 200\n  max_retries: 5\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\npostgresql.max_connections\ninteger\n10\nMaximum number of database connections\n\n\npostgresql.statement_timeout_seconds\ninteger\n30\nSQL statement timeout in seconds\n\n\npostgresql.use_prepared_statements\nboolean\ntrue\nWhether to use prepared statements\n\n\npostgresql.connection_lifetime_seconds\ninteger\n3600\nMaximum connection lifetime in seconds\n\n\n\nExample:\npostgresql:\n  max_connections: 20\n  statement_timeout_seconds: 60\n  use_prepared_statements: true\n  connection_lifetime_seconds: 1800\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nqueues\nstring[]\n[\"default\"]\nList of queue names to use\n\n\ndefault_queue\nstring\n\"default\"\nDefault queue for tasks without a specified queue\n\n\n\nExample:\nqueues:\n  - \"default\"\n  - \"high_memory\"\n  - \"gpu_tasks\"\n  - \"etl\"\ndefault_queue: \"default\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nagent.concurrency\ninteger\n4\nNumber of concurrent tasks per agent\n\n\nagent.heartbeat_interval_seconds\ninteger\n5\nInterval between agent heartbeats\n\n\nagent.execution_timeout_seconds\ninteger\n3600\nDefault timeout for task execution\n\n\nagent.cleanup_temp_files\nboolean\ntrue\nWhether to clean up temporary files\n\n\nagent.queues\nstring[]\n[]\nQueues this agent should process (defaults to all queues)\n\n\nagent.tags\nstring[]\n[]\nTags for agent classification\n\n\n\nExample:\nagent:\n  concurrency: 8\n  heartbeat_interval_seconds: 10\n  execution_timeout_seconds: 7200\n  cleanup_temp_files: true\n  queues:\n    - \"default\"\n    - \"high_memory\"\n  tags:\n    - \"region:us-east\"\n    - \"type:general\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\norchestrator.id\nstring\n\"auto\"\nOrchestrator identifier (or “auto” to generate)\n\n\norchestrator.cluster_mode\nboolean\nfalse\nEnable orchestrator clustering\n\n\norchestrator.distribution_algorithm\nstring\n\"consistent_hash\"\nAlgorithm for distributing work\n\n\norchestrator.evaluation_interval_seconds\ninteger\n5\nInterval for evaluating DAGs\n\n\norchestrator.max_parallel_evaluations\ninteger\n10\nMaximum parallel DAG evaluations\n\n\norchestrator.enable_auto_recovery\nboolean\ntrue\nAutomatically recover from failures\n\n\n\nExample:\norchestrator:\n  id: \"orchestrator-1\"\n  cluster_mode: true\n  distribution_algorithm: \"consistent_hash\"\n  evaluation_interval_seconds: 10\n  max_parallel_evaluations: 20\n  enable_auto_recovery: true\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nui.title\nstring\n\"Cyclonetix\"\nUI title\n\n\nui.logo_path\nstring\n\"\"\nPath to custom logo\n\n\nui.theme\nstring\n\"dark\"\nDefault theme (\"dark\" or \"light\")\n\n\nui.refresh_interval_seconds\ninteger\n10\nDashboard refresh rate\n\n\nui.default_view\nstring\n\"dashboard\"\nStarting page\n\n\nui.max_dag_nodes\ninteger\n500\nMaximum DAG nodes to render\n\n\n\nExample:\nui:\n  title: \"Company Workflow Orchestrator\"\n  logo_path: \"/static/img/company-logo.png\"\n  theme: \"dark\"\n  refresh_interval_seconds: 5\n  default_view: \"dashboard\"\n  max_dag_nodes: 300\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nsecurity.enabled\nboolean\nfalse\nWhether security is enabled\n\n\nsecurity.cookie_secret\nstring\n\"change-me\"\nSecret for signing cookies\n\n\nsecurity.session_timeout_minutes\ninteger\n120\nSession timeout\n\n\nsecurity.secure_cookies\nboolean\nfalse\nWhether cookies require HTTPS\n\n\nsecurity.same_site\nstring\n\"lax\"\nCookie SameSite policy\n\n\nsecurity.public_paths\nstring[]\n[\"/static/*\", \"/login\", \"/auth/*\", \"/health\"]\nPaths accessible without auth\n\n\n\nExample:\nsecurity:\n  enabled: true\n  cookie_secret: \"use-a-random-string-here\"\n  session_timeout_minutes: 240\n  secure_cookies: true\n  same_site: \"strict\"\n  public_paths:\n    - \"/static/*\"\n    - \"/login\"\n    - \"/auth/*\"\n    - \"/health\"\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nsecurity.oauth.provider\nstring\n\"google\"\nOAuth provider (\"google\", \"github\", \"azure\")\n\n\nsecurity.oauth.client_id\nstring\n\"\"\nOAuth client ID\n\n\nsecurity.oauth.client_secret\nstring\n\"\"\nOAuth client secret\n\n\nsecurity.oauth.redirect_url\nstring\n\"\"\nOAuth redirect URL\n\n\nsecurity.oauth.allowed_domains\nstring[]\n[]\nAllowed email domains (empty = all)\n\n\nsecurity.oauth.allowed_emails\nstring[]\n[]\nAllowed email addresses (empty = all)\n\n\n\nExample:\nsecurity:\n  oauth:\n    provider: \"google\"\n    client_id: \"your-client-id\"\n    client_secret: \"your-client-secret\"\n    redirect_url: \"https://cyclonetix.example.com/auth/callback\"\n    allowed_domains:\n      - \"example.com\"\n    allowed_emails:\n      - \"admin@othercompany.com\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nsecurity.basic_auth.users\nobject[]\n[]\nList of users with usernames and password hashes\n\n\n\nExample:\nsecurity:\n  basic_auth:\n    users:\n      - username: \"admin\"\n        password_hash: \"$2b$12$...\"  # BCrypt hash\n      - username: \"viewer\"\n        password_hash: \"$2b$12$...\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nsecurity.api_keys\nobject[]\n[]\nList of API keys with names and roles\n\n\n\nExample:\nsecurity:\n  api_keys:\n    - key: \"your-secret-api-key\"\n      name: \"ci-pipeline\"\n      roles: [\"scheduler\", \"viewer\"]\n    - key: \"another-secret-key\"\n      name: \"monitoring-system\"\n      roles: [\"viewer\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nlogging.level\nstring\n\"info\"\nLog level (\"trace\", \"debug\", \"info\", \"warn\", \"error\")\n\n\nlogging.format\nstring\n\"text\"\nLog format (\"text\" or \"json\")\n\n\nlogging.file\nstring\n\"\"\nOptional log file\n\n\nlogging.syslog\nboolean\nfalse\nWhether to log to syslog\n\n\n\nExample:\nlogging:\n  level: \"info\"\n  format: \"json\"\n  file: \"/var/log/cyclonetix.log\"\n  syslog: true\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\ngit.cache.enabled\nboolean\ntrue\nWhether to cache Git repositories\n\n\ngit.cache.path\nstring\n\"/tmp/cyclonetix-git-cache\"\nPath for Git cache\n\n\ngit.cache.max_size_mb\ninteger\n1000\nMaximum cache size in MB\n\n\ngit.cache.ttl_minutes\ninteger\n60\nCache TTL in minutes\n\n\n\nExample:\ngit:\n  cache:\n    enabled: true\n    path: \"/var/cache/cyclonetix/git\"\n    max_size_mb: 5000\n    ttl_minutes: 120\n  auth:\n    ssh_key_path: \"/etc/cyclonetix/ssh/id_rsa\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nmonitoring.prometheus.enabled\nboolean\nfalse\nWhether to expose Prometheus metrics\n\n\nmonitoring.prometheus.endpoint\nstring\n\"/metrics\"\nMetrics endpoint\n\n\nmonitoring.metrics.include_task_metrics\nboolean\ntrue\nInclude task metrics\n\n\nmonitoring.metrics.include_queue_metrics\nboolean\ntrue\nInclude queue metrics\n\n\nmonitoring.metrics.include_agent_metrics\nboolean\ntrue\nInclude agent metrics\n\n\n\nExample:\nmonitoring:\n  prometheus:\n    enabled: true\n    endpoint: \"/metrics\"\n  metrics:\n    include_task_metrics: true\n    include_queue_metrics: true\n    include_agent_metrics: true\n\n\n\n\nHere’s a comprehensive configuration example:\n# Core paths\ntask_directory: \"/etc/cyclonetix/tasks\"\ncontext_directory: \"/etc/cyclonetix/contexts\"\nparameter_set_directory: \"/etc/cyclonetix/parameter_sets\"\ndag_directory: \"/etc/cyclonetix/dags\"\n\n# Backend configuration\nbackend: \"redis\"\nbackend_url: \"redis://redis.internal:6379\"\nserialization_format: \"binary\"\ncluster_id: \"production_cluster\"\n\n# Redis-specific options\nredis:\n  cluster_mode: true\n  read_from_replicas: true\n  connection_pool_size: 20\n  connection_timeout_ms: 2000\n  retry_interval_ms: 200\n  max_retries: 5\n\n# Queue configuration\nqueues:\n  - \"default\"\n  - \"high_memory\"\n  - \"gpu_tasks\"\n  - \"etl\"\ndefault_queue: \"default\"\n\n# Agent configuration\nagent:\n  concurrency: 8\n  heartbeat_interval_seconds: 10\n  execution_timeout_seconds: 7200\n  cleanup_temp_files: true\n  queues:\n    - \"default\"\n    - \"high_memory\"\n  tags:\n    - \"region:us-east\"\n    - \"type:general\"\n\n# Orchestrator configuration\norchestrator:\n  id: \"auto\"\n  cluster_mode: true\n  distribution_algorithm: \"consistent_hash\"\n  evaluation_interval_seconds: 10\n  max_parallel_evaluations: 20\n  enable_auto_recovery: true\n\n# UI configuration\nui:\n  title: \"Company Workflow Orchestrator\"\n  logo_path: \"/static/img/company-logo.png\"\n  theme: \"dark\"\n  refresh_interval_seconds: 5\n  default_view: \"dashboard\"\n  max_dag_nodes: 300\n\n# Security configuration\nsecurity:\n  enabled: true\n  cookie_secret: \"use-a-random-string-here\"\n  session_timeout_minutes: 240\n  secure_cookies: true\n  same_site: \"strict\"\n  public_paths:\n    - \"/static/*\"\n    - \"/login\"\n    - \"/auth/*\"\n    - \"/health\"\n  oauth:\n    provider: \"google\"\n    client_id: \"${OAUTH_CLIENT_ID}\"\n    client_secret: \"${OAUTH_CLIENT_SECRET}\"\n    redirect_url: \"https://cyclonetix.example.com/auth/callback\"\n    allowed_domains:\n      - \"example.com\"\n  api_keys:\n    - key: \"${API_KEY_CI}\"\n      name: \"ci-pipeline\"\n      roles: [\"scheduler\", \"viewer\"]\n\n# Logging configuration\nlogging:\n  level: \"info\"\n  format: \"json\"\n  file: \"/var/log/cyclonetix.log\"\n  syslog: false\n\n# Git configuration\ngit:\n  cache:\n    enabled: true\n    path: \"/var/cache/cyclonetix/git\"\n    max_size_mb: 5000\n    ttl_minutes: 120\n  auth:\n    ssh_key_path: \"/etc/cyclonetix/ssh/id_rsa\"\n\n# Monitoring configuration\nmonitoring:\n  prometheus:\n    enabled: true\n    endpoint: \"/metrics\"\n  metrics:\n    include_task_metrics: true\n    include_queue_metrics: true\n    include_agent_metrics: true\n\n\n\nMost configuration options can be overridden with environment variables. The format is CYCLO_SECTION_OPTION. For example:\n\nCYCLO_BACKEND_URL overrides backend_url\nCYCLO_SECURITY_ENABLED overrides security.enabled\nCYCLO_LOGGING_LEVEL overrides logging.level\n\nNested configurations use underscores: - CYCLO_REDIS_CLUSTER_MODE overrides redis.cluster_mode - CYCLO_SECURITY_OAUTH_CLIENT_ID overrides security.oauth.client_id\n\n\n\n\nCheck the CLI Reference for command-line options\nReview the API Reference for API documentation\nExplore the YAML Schema for task and DAG definitions",
    "crumbs": [
      "Reference",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "reference/configuration.html#configuration-file-location",
    "href": "reference/configuration.html#configuration-file-location",
    "title": "Configuration Reference",
    "section": "",
    "text": "By default, Cyclonetix looks for config.yaml in the current working directory. You can specify a different location using the --config command-line flag:\n./cyclonetix --config /path/to/config.yaml",
    "crumbs": [
      "Reference",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "reference/configuration.html#configuration-sections",
    "href": "reference/configuration.html#configuration-sections",
    "title": "Configuration Reference",
    "section": "",
    "text": "Option\nType\nDefault\nDescription\n\n\n\n\ntask_directory\nstring\n\"./data/tasks\"\nDirectory containing task definitions\n\n\ncontext_directory\nstring\n\"./data/contexts\"\nDirectory containing context definitions\n\n\nparameter_set_directory\nstring\n\"./data/parameter_sets\"\nDirectory containing parameter sets\n\n\ndag_directory\nstring\n\"./data/dags\"\nDirectory containing DAG definitions\n\n\n\nExample:\ntask_directory: \"/etc/cyclonetix/tasks\"\ncontext_directory: \"/etc/cyclonetix/contexts\"\nparameter_set_directory: \"/etc/cyclonetix/parameter_sets\"\ndag_directory: \"/etc/cyclonetix/dags\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nbackend\nstring\n\"memory\"\nBackend type (\"memory\", \"redis\", or \"postgresql\")\n\n\nbackend_url\nstring\n\"\"\nConnection URL for the backend\n\n\nserialization_format\nstring\n\"json\"\nFormat for serializing data (\"json\" or \"binary\")\n\n\ncluster_id\nstring\n\"default_cluster\"\nUnique identifier for this Cyclonetix cluster\n\n\n\nExample:\nbackend: \"redis\"\nbackend_url: \"redis://localhost:6379\"\nserialization_format: \"binary\"\ncluster_id: \"production_cluster\"\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nredis.cluster_mode\nboolean\nfalse\nWhether to use Redis cluster mode\n\n\nredis.read_from_replicas\nboolean\nfalse\nWhether to read from Redis replicas\n\n\nredis.connection_pool_size\ninteger\n10\nSize of the Redis connection pool\n\n\nredis.connection_timeout_ms\ninteger\n1000\nConnection timeout in milliseconds\n\n\nredis.retry_interval_ms\ninteger\n100\nRetry interval in milliseconds\n\n\nredis.max_retries\ninteger\n3\nMaximum number of connection retries\n\n\n\nExample:\nredis:\n  cluster_mode: true\n  read_from_replicas: true\n  connection_pool_size: 20\n  connection_timeout_ms: 2000\n  retry_interval_ms: 200\n  max_retries: 5\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\npostgresql.max_connections\ninteger\n10\nMaximum number of database connections\n\n\npostgresql.statement_timeout_seconds\ninteger\n30\nSQL statement timeout in seconds\n\n\npostgresql.use_prepared_statements\nboolean\ntrue\nWhether to use prepared statements\n\n\npostgresql.connection_lifetime_seconds\ninteger\n3600\nMaximum connection lifetime in seconds\n\n\n\nExample:\npostgresql:\n  max_connections: 20\n  statement_timeout_seconds: 60\n  use_prepared_statements: true\n  connection_lifetime_seconds: 1800\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nqueues\nstring[]\n[\"default\"]\nList of queue names to use\n\n\ndefault_queue\nstring\n\"default\"\nDefault queue for tasks without a specified queue\n\n\n\nExample:\nqueues:\n  - \"default\"\n  - \"high_memory\"\n  - \"gpu_tasks\"\n  - \"etl\"\ndefault_queue: \"default\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nagent.concurrency\ninteger\n4\nNumber of concurrent tasks per agent\n\n\nagent.heartbeat_interval_seconds\ninteger\n5\nInterval between agent heartbeats\n\n\nagent.execution_timeout_seconds\ninteger\n3600\nDefault timeout for task execution\n\n\nagent.cleanup_temp_files\nboolean\ntrue\nWhether to clean up temporary files\n\n\nagent.queues\nstring[]\n[]\nQueues this agent should process (defaults to all queues)\n\n\nagent.tags\nstring[]\n[]\nTags for agent classification\n\n\n\nExample:\nagent:\n  concurrency: 8\n  heartbeat_interval_seconds: 10\n  execution_timeout_seconds: 7200\n  cleanup_temp_files: true\n  queues:\n    - \"default\"\n    - \"high_memory\"\n  tags:\n    - \"region:us-east\"\n    - \"type:general\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\norchestrator.id\nstring\n\"auto\"\nOrchestrator identifier (or “auto” to generate)\n\n\norchestrator.cluster_mode\nboolean\nfalse\nEnable orchestrator clustering\n\n\norchestrator.distribution_algorithm\nstring\n\"consistent_hash\"\nAlgorithm for distributing work\n\n\norchestrator.evaluation_interval_seconds\ninteger\n5\nInterval for evaluating DAGs\n\n\norchestrator.max_parallel_evaluations\ninteger\n10\nMaximum parallel DAG evaluations\n\n\norchestrator.enable_auto_recovery\nboolean\ntrue\nAutomatically recover from failures\n\n\n\nExample:\norchestrator:\n  id: \"orchestrator-1\"\n  cluster_mode: true\n  distribution_algorithm: \"consistent_hash\"\n  evaluation_interval_seconds: 10\n  max_parallel_evaluations: 20\n  enable_auto_recovery: true\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nui.title\nstring\n\"Cyclonetix\"\nUI title\n\n\nui.logo_path\nstring\n\"\"\nPath to custom logo\n\n\nui.theme\nstring\n\"dark\"\nDefault theme (\"dark\" or \"light\")\n\n\nui.refresh_interval_seconds\ninteger\n10\nDashboard refresh rate\n\n\nui.default_view\nstring\n\"dashboard\"\nStarting page\n\n\nui.max_dag_nodes\ninteger\n500\nMaximum DAG nodes to render\n\n\n\nExample:\nui:\n  title: \"Company Workflow Orchestrator\"\n  logo_path: \"/static/img/company-logo.png\"\n  theme: \"dark\"\n  refresh_interval_seconds: 5\n  default_view: \"dashboard\"\n  max_dag_nodes: 300\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nsecurity.enabled\nboolean\nfalse\nWhether security is enabled\n\n\nsecurity.cookie_secret\nstring\n\"change-me\"\nSecret for signing cookies\n\n\nsecurity.session_timeout_minutes\ninteger\n120\nSession timeout\n\n\nsecurity.secure_cookies\nboolean\nfalse\nWhether cookies require HTTPS\n\n\nsecurity.same_site\nstring\n\"lax\"\nCookie SameSite policy\n\n\nsecurity.public_paths\nstring[]\n[\"/static/*\", \"/login\", \"/auth/*\", \"/health\"]\nPaths accessible without auth\n\n\n\nExample:\nsecurity:\n  enabled: true\n  cookie_secret: \"use-a-random-string-here\"\n  session_timeout_minutes: 240\n  secure_cookies: true\n  same_site: \"strict\"\n  public_paths:\n    - \"/static/*\"\n    - \"/login\"\n    - \"/auth/*\"\n    - \"/health\"\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nsecurity.oauth.provider\nstring\n\"google\"\nOAuth provider (\"google\", \"github\", \"azure\")\n\n\nsecurity.oauth.client_id\nstring\n\"\"\nOAuth client ID\n\n\nsecurity.oauth.client_secret\nstring\n\"\"\nOAuth client secret\n\n\nsecurity.oauth.redirect_url\nstring\n\"\"\nOAuth redirect URL\n\n\nsecurity.oauth.allowed_domains\nstring[]\n[]\nAllowed email domains (empty = all)\n\n\nsecurity.oauth.allowed_emails\nstring[]\n[]\nAllowed email addresses (empty = all)\n\n\n\nExample:\nsecurity:\n  oauth:\n    provider: \"google\"\n    client_id: \"your-client-id\"\n    client_secret: \"your-client-secret\"\n    redirect_url: \"https://cyclonetix.example.com/auth/callback\"\n    allowed_domains:\n      - \"example.com\"\n    allowed_emails:\n      - \"admin@othercompany.com\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nsecurity.basic_auth.users\nobject[]\n[]\nList of users with usernames and password hashes\n\n\n\nExample:\nsecurity:\n  basic_auth:\n    users:\n      - username: \"admin\"\n        password_hash: \"$2b$12$...\"  # BCrypt hash\n      - username: \"viewer\"\n        password_hash: \"$2b$12$...\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nsecurity.api_keys\nobject[]\n[]\nList of API keys with names and roles\n\n\n\nExample:\nsecurity:\n  api_keys:\n    - key: \"your-secret-api-key\"\n      name: \"ci-pipeline\"\n      roles: [\"scheduler\", \"viewer\"]\n    - key: \"another-secret-key\"\n      name: \"monitoring-system\"\n      roles: [\"viewer\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nlogging.level\nstring\n\"info\"\nLog level (\"trace\", \"debug\", \"info\", \"warn\", \"error\")\n\n\nlogging.format\nstring\n\"text\"\nLog format (\"text\" or \"json\")\n\n\nlogging.file\nstring\n\"\"\nOptional log file\n\n\nlogging.syslog\nboolean\nfalse\nWhether to log to syslog\n\n\n\nExample:\nlogging:\n  level: \"info\"\n  format: \"json\"\n  file: \"/var/log/cyclonetix.log\"\n  syslog: true\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\ngit.cache.enabled\nboolean\ntrue\nWhether to cache Git repositories\n\n\ngit.cache.path\nstring\n\"/tmp/cyclonetix-git-cache\"\nPath for Git cache\n\n\ngit.cache.max_size_mb\ninteger\n1000\nMaximum cache size in MB\n\n\ngit.cache.ttl_minutes\ninteger\n60\nCache TTL in minutes\n\n\n\nExample:\ngit:\n  cache:\n    enabled: true\n    path: \"/var/cache/cyclonetix/git\"\n    max_size_mb: 5000\n    ttl_minutes: 120\n  auth:\n    ssh_key_path: \"/etc/cyclonetix/ssh/id_rsa\"\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nmonitoring.prometheus.enabled\nboolean\nfalse\nWhether to expose Prometheus metrics\n\n\nmonitoring.prometheus.endpoint\nstring\n\"/metrics\"\nMetrics endpoint\n\n\nmonitoring.metrics.include_task_metrics\nboolean\ntrue\nInclude task metrics\n\n\nmonitoring.metrics.include_queue_metrics\nboolean\ntrue\nInclude queue metrics\n\n\nmonitoring.metrics.include_agent_metrics\nboolean\ntrue\nInclude agent metrics\n\n\n\nExample:\nmonitoring:\n  prometheus:\n    enabled: true\n    endpoint: \"/metrics\"\n  metrics:\n    include_task_metrics: true\n    include_queue_metrics: true\n    include_agent_metrics: true",
    "crumbs": [
      "Reference",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "reference/configuration.html#complete-configuration-example",
    "href": "reference/configuration.html#complete-configuration-example",
    "title": "Configuration Reference",
    "section": "",
    "text": "Here’s a comprehensive configuration example:\n# Core paths\ntask_directory: \"/etc/cyclonetix/tasks\"\ncontext_directory: \"/etc/cyclonetix/contexts\"\nparameter_set_directory: \"/etc/cyclonetix/parameter_sets\"\ndag_directory: \"/etc/cyclonetix/dags\"\n\n# Backend configuration\nbackend: \"redis\"\nbackend_url: \"redis://redis.internal:6379\"\nserialization_format: \"binary\"\ncluster_id: \"production_cluster\"\n\n# Redis-specific options\nredis:\n  cluster_mode: true\n  read_from_replicas: true\n  connection_pool_size: 20\n  connection_timeout_ms: 2000\n  retry_interval_ms: 200\n  max_retries: 5\n\n# Queue configuration\nqueues:\n  - \"default\"\n  - \"high_memory\"\n  - \"gpu_tasks\"\n  - \"etl\"\ndefault_queue: \"default\"\n\n# Agent configuration\nagent:\n  concurrency: 8\n  heartbeat_interval_seconds: 10\n  execution_timeout_seconds: 7200\n  cleanup_temp_files: true\n  queues:\n    - \"default\"\n    - \"high_memory\"\n  tags:\n    - \"region:us-east\"\n    - \"type:general\"\n\n# Orchestrator configuration\norchestrator:\n  id: \"auto\"\n  cluster_mode: true\n  distribution_algorithm: \"consistent_hash\"\n  evaluation_interval_seconds: 10\n  max_parallel_evaluations: 20\n  enable_auto_recovery: true\n\n# UI configuration\nui:\n  title: \"Company Workflow Orchestrator\"\n  logo_path: \"/static/img/company-logo.png\"\n  theme: \"dark\"\n  refresh_interval_seconds: 5\n  default_view: \"dashboard\"\n  max_dag_nodes: 300\n\n# Security configuration\nsecurity:\n  enabled: true\n  cookie_secret: \"use-a-random-string-here\"\n  session_timeout_minutes: 240\n  secure_cookies: true\n  same_site: \"strict\"\n  public_paths:\n    - \"/static/*\"\n    - \"/login\"\n    - \"/auth/*\"\n    - \"/health\"\n  oauth:\n    provider: \"google\"\n    client_id: \"${OAUTH_CLIENT_ID}\"\n    client_secret: \"${OAUTH_CLIENT_SECRET}\"\n    redirect_url: \"https://cyclonetix.example.com/auth/callback\"\n    allowed_domains:\n      - \"example.com\"\n  api_keys:\n    - key: \"${API_KEY_CI}\"\n      name: \"ci-pipeline\"\n      roles: [\"scheduler\", \"viewer\"]\n\n# Logging configuration\nlogging:\n  level: \"info\"\n  format: \"json\"\n  file: \"/var/log/cyclonetix.log\"\n  syslog: false\n\n# Git configuration\ngit:\n  cache:\n    enabled: true\n    path: \"/var/cache/cyclonetix/git\"\n    max_size_mb: 5000\n    ttl_minutes: 120\n  auth:\n    ssh_key_path: \"/etc/cyclonetix/ssh/id_rsa\"\n\n# Monitoring configuration\nmonitoring:\n  prometheus:\n    enabled: true\n    endpoint: \"/metrics\"\n  metrics:\n    include_task_metrics: true\n    include_queue_metrics: true\n    include_agent_metrics: true",
    "crumbs": [
      "Reference",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "reference/configuration.html#environment-variable-overrides",
    "href": "reference/configuration.html#environment-variable-overrides",
    "title": "Configuration Reference",
    "section": "",
    "text": "Most configuration options can be overridden with environment variables. The format is CYCLO_SECTION_OPTION. For example:\n\nCYCLO_BACKEND_URL overrides backend_url\nCYCLO_SECURITY_ENABLED overrides security.enabled\nCYCLO_LOGGING_LEVEL overrides logging.level\n\nNested configurations use underscores: - CYCLO_REDIS_CLUSTER_MODE overrides redis.cluster_mode - CYCLO_SECURITY_OAUTH_CLIENT_ID overrides security.oauth.client_id",
    "crumbs": [
      "Reference",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "reference/configuration.html#next-steps",
    "href": "reference/configuration.html#next-steps",
    "title": "Configuration Reference",
    "section": "",
    "text": "Check the CLI Reference for command-line options\nReview the API Reference for API documentation\nExplore the YAML Schema for task and DAG definitions",
    "crumbs": [
      "Reference",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html",
    "href": "reference/yaml-schema.html",
    "title": "YAML Schema",
    "section": "",
    "text": "This page describes the schema for YAML files used in Cyclonetix, including tasks, DAGs, contexts, and parameter sets.\n\n\nTasks are defined in YAML files with the following schema:\n# Unique identifier for the task (required)\nid: \"task_id\"\n\n# Human-readable name (required)\nname: \"Task Name\"\n\n# Optional description\ndescription: \"Detailed description of the task's purpose\"\n\n# Command to execute (required)\ncommand: \"python script.py --input ${INPUT_FILE} --output ${OUTPUT_FILE}\"\n\n# Dependencies on other tasks (optional)\ndependencies:\n  - \"dependency_task_1\"\n  - \"dependency_task_2:parameter_set_name\"  # With specific parameter set\n\n# Default parameters (optional)\nparameters:\n  PARAM1: \"default_value\"\n  PARAM2: \"another_value\"\n  # Complex parameter with validation\n  COMPLEX_PARAM:\n    default: \"value1\"\n    description: \"Parameter description\"\n    required: true\n    options: [\"value1\", \"value2\", \"value3\"]\n    validation: \"^[a-z0-9_]+$\"\n\n# Queue assignment (optional, defaults to \"default\")\nqueue: \"gpu_tasks\"\n\n# Whether this is an evaluation point (optional, defaults to false)\nevaluation_point: false\n\n# Timeout in seconds (optional)\ntimeout_seconds: 3600\n\n# Retry configuration (optional)\nretries:\n  count: 3\n  delay_seconds: 300\n  exponential_backoff: true\n  max_delay_seconds: 3600\n\n# Resource requirements (optional)\nresources:\n  cpu: 2\n  memory_mb: 4096\n  gpu: 1\n  disk_mb: 10240\n\n# Task tags for organization (optional)\ntags:\n  - \"etl\"\n  - \"data_processing\"\n\n# Git configuration for code execution (optional)\ngit:\n  repository: \"https://github.com/example/repo.git\"\n  ref: \"main\"  # Branch, tag, or commit\n  path: \"scripts/process.py\"\n  environment: \"python-data-science\"\n\n\nid: \"train_model\"\nname: \"Train Machine Learning Model\"\ndescription: \"Trains a machine learning model using the prepared dataset\"\ncommand: |\n  python train.py \\\n    --data-path ${DATA_PATH} \\\n    --model-type ${MODEL_TYPE} \\\n    --epochs ${EPOCHS} \\\n    --batch-size ${BATCH_SIZE} \\\n    --output-path ${MODEL_OUTPUT}\ndependencies:\n  - \"prepare_data\"\n  - \"feature_engineering\"\nparameters:\n  DATA_PATH: \"/data/processed\"\n  MODEL_TYPE: \"random_forest\"\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"32\"\n  MODEL_OUTPUT: \"/models/latest\"\nqueue: \"gpu_tasks\"\ntimeout_seconds: 7200\nresources:\n  cpu: 4\n  memory_mb: 16384\n  gpu: 1\ntags:\n  - \"ml\"\n  - \"training\"\n\n\n\n\nDAGs are defined in YAML files with the following schema:\n# Unique identifier for the DAG (required)\nid: \"dag_id\"\n\n# Human-readable name (required)\nname: \"DAG Name\"\n\n# Optional description\ndescription: \"Detailed description of the DAG's purpose\"\n\n# List of tasks to include (required)\ntasks:\n  # Simple task reference\n  - id: \"task_1\"\n\n  # Task with parameter overrides\n  - id: \"task_2\"\n    parameters:\n      PARAM1: \"override_value\"\n      PARAM2: \"another_override\"\n\n  # Task with different queue\n  - id: \"task_3\"\n    queue: \"high_priority\"\n\n# Default context for all tasks (optional)\ncontext: \"production\"\n\n# Schedule for periodic execution (optional)\nschedule:\n  cron: \"0 5 * * *\"        # Cron expression (run at 5:00 AM daily)\n  timezone: \"UTC\"          # Timezone for the cron expression\n  start_date: \"2023-01-01\" # Optional start date\n  end_date: \"2023-12-31\"   # Optional end date\n  catchup: false           # Whether to run for missed intervals\n  depends_on_past: false   # Whether each run depends on the previous run\n\n# Maximum concurrency (optional)\nmax_active_runs: 1\n\n# DAG-level timeout (optional)\ntimeout_seconds: 86400  # 24 hours\n\n# DAG tags for organization (optional)\ntags:\n  - \"production\"\n  - \"daily\"\n\n\nid: \"etl_pipeline\"\nname: \"ETL Pipeline\"\ndescription: \"Extract, transform, and load data pipeline\"\ntasks:\n  - id: \"data_extraction\"\n    parameters:\n      SOURCE: \"production_db\"\n      LIMIT: \"10000\"\n\n  - id: \"data_cleaning\"\n    parameters:\n      CLEAN_MODE: \"strict\"\n\n  - id: \"data_transformation\"\n    parameters:\n      TRANSFORM_TYPE: \"normalize\"\n\n  - id: \"data_validation\"\n    evaluation_point: true\n\n  - id: \"data_loading\"\n    parameters:\n      DESTINATION: \"data_warehouse\"\n      MODE: \"append\"\n\ncontext: \"production\"\nschedule:\n  cron: \"0 2 * * *\"  # Daily at 2:00 AM\n  timezone: \"UTC\"\n  catchup: false\nmax_active_runs: 1\ntags:\n  - \"etl\"\n  - \"production\"\n  - \"daily\"\n\n\n\n\nContexts are defined in YAML files with the following schema:\n# Unique identifier for the context (required)\nid: \"context_id\"\n\n# Optional parent context to extend\nextends: \"parent_context_id\"\n\n# Variables defined in this context (required)\nvariables:\n  ENV: \"production\"\n  LOG_LEVEL: \"info\"\n  DATA_PATH: \"/data/production\"\n  API_URL: \"https://api.example.com\"\n  DB_CONNECTION: \"postgresql://user:pass@host:5432/db\"\n  # Dynamic values\n  RUN_DATE: \"${DATE}\"\n  RUN_ID: \"${RUN_ID}\"\n  RANDOM_ID: \"${RANDOM:16}\"\n\n# Optional description\ndescription: \"Production environment context\"\n\n\nid: \"production\"\ndescription: \"Production environment context\"\nvariables:\n  # Environment information\n  ENV: \"production\"\n  LOG_LEVEL: \"info\"\n\n  # Paths and locations\n  BASE_PATH: \"/data/production\"\n  LOG_PATH: \"/var/log/cyclonetix\"\n\n  # Service connections\n  API_URL: \"https://api.example.com\"\n  DB_HOST: \"db.example.com\"\n  DB_PORT: \"5432\"\n  DB_NAME: \"production_db\"\n  DB_USER: \"app_user\"\n\n  # Runtime configuration\n  MAX_THREADS: \"16\"\n  TIMEOUT_SECONDS: \"3600\"\n  BATCH_SIZE: \"1000\"\n\n\n\n\nParameter sets are defined in YAML files with the following schema:\n# Unique identifier for the parameter set (required)\nid: \"parameter_set_id\"\n\n# Parameters defined in this set (required)\nparameters:\n  PARAM1: \"value1\"\n  PARAM2: \"value2\"\n  NESTED_PARAM:\n    SUBPARAM1: \"subvalue1\"\n    SUBPARAM2: \"subvalue2\"\n\n# Optional description\ndescription: \"Parameter set description\"\n\n# Optional tags for organization\ntags:\n  - \"tag1\"\n  - \"tag2\"\n\n\nid: \"large_training\"\ndescription: \"Parameters for large-scale model training\"\nparameters:\n  # Model configuration\n  MODEL_TYPE: \"deep_learning\"\n  ARCHITECTURE: \"transformer\"\n\n  # Training parameters\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"128\"\n  LEARNING_RATE: \"0.001\"\n  OPTIMIZER: \"adam\"\n\n  # Resource allocation\n  NUM_GPUS: \"4\"\n  DISTRIBUTED: \"true\"\n\n  # Logging and checkpoints\n  LOG_FREQUENCY: \"10\"\n  CHECKPOINT_FREQUENCY: \"20\"\ntags:\n  - \"ml\"\n  - \"deep_learning\"\n  - \"large_scale\"\n\n\n\n\nEvaluation tasks output results in the following JSON format:\n{\n  \"next_tasks\": [\"task_id_1\", \"task_id_2\"],\n  \"parameters\": {\n    \"task_id_1\": {\n      \"PARAM1\": \"value1\",\n      \"PARAM2\": \"value2\"\n    },\n    \"task_id_2\": {\n      \"PARAM1\": \"value1\"\n    }\n  },\n  \"context_updates\": {\n    \"VARIABLE1\": \"new_value\",\n    \"VARIABLE2\": \"new_value\"\n  },\n  \"metadata\": {\n    \"reason\": \"Model accuracy below threshold\",\n    \"metrics\": {\n      \"accuracy\": 0.82,\n      \"precision\": 0.79\n    }\n  }\n}\n\n\n\nCyclonetix validates all YAML files against these schemas at load time. If a file doesn’t conform to the schema, an error will be reported.\nYou can validate your YAML files without loading them into Cyclonetix using the command:\ncyclonetix validate-yaml &lt;path-to-yaml-file&gt;\n\n\n\nWhile Cyclonetix doesn’t enforce specific file naming conventions, the following are recommended:\n\nTasks: &lt;task_id&gt;.yaml or grouped in subdirectories\nDAGs: &lt;dag_id&gt;.yaml\nContexts: &lt;context_id&gt;.yaml\nParameter Sets: &lt;parameter_set_id&gt;.yaml\n\n\n\n\n\nReview the API Reference for API documentation\nCheck the CLI Reference for command-line options\nExplore the Configuration Reference for system configuration",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html#task-definition-schema",
    "href": "reference/yaml-schema.html#task-definition-schema",
    "title": "YAML Schema",
    "section": "",
    "text": "Tasks are defined in YAML files with the following schema:\n# Unique identifier for the task (required)\nid: \"task_id\"\n\n# Human-readable name (required)\nname: \"Task Name\"\n\n# Optional description\ndescription: \"Detailed description of the task's purpose\"\n\n# Command to execute (required)\ncommand: \"python script.py --input ${INPUT_FILE} --output ${OUTPUT_FILE}\"\n\n# Dependencies on other tasks (optional)\ndependencies:\n  - \"dependency_task_1\"\n  - \"dependency_task_2:parameter_set_name\"  # With specific parameter set\n\n# Default parameters (optional)\nparameters:\n  PARAM1: \"default_value\"\n  PARAM2: \"another_value\"\n  # Complex parameter with validation\n  COMPLEX_PARAM:\n    default: \"value1\"\n    description: \"Parameter description\"\n    required: true\n    options: [\"value1\", \"value2\", \"value3\"]\n    validation: \"^[a-z0-9_]+$\"\n\n# Queue assignment (optional, defaults to \"default\")\nqueue: \"gpu_tasks\"\n\n# Whether this is an evaluation point (optional, defaults to false)\nevaluation_point: false\n\n# Timeout in seconds (optional)\ntimeout_seconds: 3600\n\n# Retry configuration (optional)\nretries:\n  count: 3\n  delay_seconds: 300\n  exponential_backoff: true\n  max_delay_seconds: 3600\n\n# Resource requirements (optional)\nresources:\n  cpu: 2\n  memory_mb: 4096\n  gpu: 1\n  disk_mb: 10240\n\n# Task tags for organization (optional)\ntags:\n  - \"etl\"\n  - \"data_processing\"\n\n# Git configuration for code execution (optional)\ngit:\n  repository: \"https://github.com/example/repo.git\"\n  ref: \"main\"  # Branch, tag, or commit\n  path: \"scripts/process.py\"\n  environment: \"python-data-science\"\n\n\nid: \"train_model\"\nname: \"Train Machine Learning Model\"\ndescription: \"Trains a machine learning model using the prepared dataset\"\ncommand: |\n  python train.py \\\n    --data-path ${DATA_PATH} \\\n    --model-type ${MODEL_TYPE} \\\n    --epochs ${EPOCHS} \\\n    --batch-size ${BATCH_SIZE} \\\n    --output-path ${MODEL_OUTPUT}\ndependencies:\n  - \"prepare_data\"\n  - \"feature_engineering\"\nparameters:\n  DATA_PATH: \"/data/processed\"\n  MODEL_TYPE: \"random_forest\"\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"32\"\n  MODEL_OUTPUT: \"/models/latest\"\nqueue: \"gpu_tasks\"\ntimeout_seconds: 7200\nresources:\n  cpu: 4\n  memory_mb: 16384\n  gpu: 1\ntags:\n  - \"ml\"\n  - \"training\"",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html#dag-definition-schema",
    "href": "reference/yaml-schema.html#dag-definition-schema",
    "title": "YAML Schema",
    "section": "",
    "text": "DAGs are defined in YAML files with the following schema:\n# Unique identifier for the DAG (required)\nid: \"dag_id\"\n\n# Human-readable name (required)\nname: \"DAG Name\"\n\n# Optional description\ndescription: \"Detailed description of the DAG's purpose\"\n\n# List of tasks to include (required)\ntasks:\n  # Simple task reference\n  - id: \"task_1\"\n\n  # Task with parameter overrides\n  - id: \"task_2\"\n    parameters:\n      PARAM1: \"override_value\"\n      PARAM2: \"another_override\"\n\n  # Task with different queue\n  - id: \"task_3\"\n    queue: \"high_priority\"\n\n# Default context for all tasks (optional)\ncontext: \"production\"\n\n# Schedule for periodic execution (optional)\nschedule:\n  cron: \"0 5 * * *\"        # Cron expression (run at 5:00 AM daily)\n  timezone: \"UTC\"          # Timezone for the cron expression\n  start_date: \"2023-01-01\" # Optional start date\n  end_date: \"2023-12-31\"   # Optional end date\n  catchup: false           # Whether to run for missed intervals\n  depends_on_past: false   # Whether each run depends on the previous run\n\n# Maximum concurrency (optional)\nmax_active_runs: 1\n\n# DAG-level timeout (optional)\ntimeout_seconds: 86400  # 24 hours\n\n# DAG tags for organization (optional)\ntags:\n  - \"production\"\n  - \"daily\"\n\n\nid: \"etl_pipeline\"\nname: \"ETL Pipeline\"\ndescription: \"Extract, transform, and load data pipeline\"\ntasks:\n  - id: \"data_extraction\"\n    parameters:\n      SOURCE: \"production_db\"\n      LIMIT: \"10000\"\n\n  - id: \"data_cleaning\"\n    parameters:\n      CLEAN_MODE: \"strict\"\n\n  - id: \"data_transformation\"\n    parameters:\n      TRANSFORM_TYPE: \"normalize\"\n\n  - id: \"data_validation\"\n    evaluation_point: true\n\n  - id: \"data_loading\"\n    parameters:\n      DESTINATION: \"data_warehouse\"\n      MODE: \"append\"\n\ncontext: \"production\"\nschedule:\n  cron: \"0 2 * * *\"  # Daily at 2:00 AM\n  timezone: \"UTC\"\n  catchup: false\nmax_active_runs: 1\ntags:\n  - \"etl\"\n  - \"production\"\n  - \"daily\"",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html#context-definition-schema",
    "href": "reference/yaml-schema.html#context-definition-schema",
    "title": "YAML Schema",
    "section": "",
    "text": "Contexts are defined in YAML files with the following schema:\n# Unique identifier for the context (required)\nid: \"context_id\"\n\n# Optional parent context to extend\nextends: \"parent_context_id\"\n\n# Variables defined in this context (required)\nvariables:\n  ENV: \"production\"\n  LOG_LEVEL: \"info\"\n  DATA_PATH: \"/data/production\"\n  API_URL: \"https://api.example.com\"\n  DB_CONNECTION: \"postgresql://user:pass@host:5432/db\"\n  # Dynamic values\n  RUN_DATE: \"${DATE}\"\n  RUN_ID: \"${RUN_ID}\"\n  RANDOM_ID: \"${RANDOM:16}\"\n\n# Optional description\ndescription: \"Production environment context\"\n\n\nid: \"production\"\ndescription: \"Production environment context\"\nvariables:\n  # Environment information\n  ENV: \"production\"\n  LOG_LEVEL: \"info\"\n\n  # Paths and locations\n  BASE_PATH: \"/data/production\"\n  LOG_PATH: \"/var/log/cyclonetix\"\n\n  # Service connections\n  API_URL: \"https://api.example.com\"\n  DB_HOST: \"db.example.com\"\n  DB_PORT: \"5432\"\n  DB_NAME: \"production_db\"\n  DB_USER: \"app_user\"\n\n  # Runtime configuration\n  MAX_THREADS: \"16\"\n  TIMEOUT_SECONDS: \"3600\"\n  BATCH_SIZE: \"1000\"",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html#parameter-set-definition-schema",
    "href": "reference/yaml-schema.html#parameter-set-definition-schema",
    "title": "YAML Schema",
    "section": "",
    "text": "Parameter sets are defined in YAML files with the following schema:\n# Unique identifier for the parameter set (required)\nid: \"parameter_set_id\"\n\n# Parameters defined in this set (required)\nparameters:\n  PARAM1: \"value1\"\n  PARAM2: \"value2\"\n  NESTED_PARAM:\n    SUBPARAM1: \"subvalue1\"\n    SUBPARAM2: \"subvalue2\"\n\n# Optional description\ndescription: \"Parameter set description\"\n\n# Optional tags for organization\ntags:\n  - \"tag1\"\n  - \"tag2\"\n\n\nid: \"large_training\"\ndescription: \"Parameters for large-scale model training\"\nparameters:\n  # Model configuration\n  MODEL_TYPE: \"deep_learning\"\n  ARCHITECTURE: \"transformer\"\n\n  # Training parameters\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"128\"\n  LEARNING_RATE: \"0.001\"\n  OPTIMIZER: \"adam\"\n\n  # Resource allocation\n  NUM_GPUS: \"4\"\n  DISTRIBUTED: \"true\"\n\n  # Logging and checkpoints\n  LOG_FREQUENCY: \"10\"\n  CHECKPOINT_FREQUENCY: \"20\"\ntags:\n  - \"ml\"\n  - \"deep_learning\"\n  - \"large_scale\"",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html#evaluation-result-schema",
    "href": "reference/yaml-schema.html#evaluation-result-schema",
    "title": "YAML Schema",
    "section": "",
    "text": "Evaluation tasks output results in the following JSON format:\n{\n  \"next_tasks\": [\"task_id_1\", \"task_id_2\"],\n  \"parameters\": {\n    \"task_id_1\": {\n      \"PARAM1\": \"value1\",\n      \"PARAM2\": \"value2\"\n    },\n    \"task_id_2\": {\n      \"PARAM1\": \"value1\"\n    }\n  },\n  \"context_updates\": {\n    \"VARIABLE1\": \"new_value\",\n    \"VARIABLE2\": \"new_value\"\n  },\n  \"metadata\": {\n    \"reason\": \"Model accuracy below threshold\",\n    \"metrics\": {\n      \"accuracy\": 0.82,\n      \"precision\": 0.79\n    }\n  }\n}",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html#schema-validation",
    "href": "reference/yaml-schema.html#schema-validation",
    "title": "YAML Schema",
    "section": "",
    "text": "Cyclonetix validates all YAML files against these schemas at load time. If a file doesn’t conform to the schema, an error will be reported.\nYou can validate your YAML files without loading them into Cyclonetix using the command:\ncyclonetix validate-yaml &lt;path-to-yaml-file&gt;",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html#file-naming-conventions",
    "href": "reference/yaml-schema.html#file-naming-conventions",
    "title": "YAML Schema",
    "section": "",
    "text": "While Cyclonetix doesn’t enforce specific file naming conventions, the following are recommended:\n\nTasks: &lt;task_id&gt;.yaml or grouped in subdirectories\nDAGs: &lt;dag_id&gt;.yaml\nContexts: &lt;context_id&gt;.yaml\nParameter Sets: &lt;parameter_set_id&gt;.yaml",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "reference/yaml-schema.html#next-steps",
    "href": "reference/yaml-schema.html#next-steps",
    "title": "YAML Schema",
    "section": "",
    "text": "Review the API Reference for API documentation\nCheck the CLI Reference for command-line options\nExplore the Configuration Reference for system configuration",
    "crumbs": [
      "Reference",
      "YAML Schema"
    ]
  },
  {
    "objectID": "core-concepts/architecture.html",
    "href": "core-concepts/architecture.html",
    "title": "Architecture Overview",
    "section": "",
    "text": "Cyclonetix is designed with a clean, modular architecture that allows for flexibility, scalability, and ease of maintenance.\n\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\nflowchart TB\n    Orchestrator[\"Orchestrator - Builds execution graphs - Evaluates task readiness - Schedules work\"]\n    Agents[\"Agents - Pick up tasks from queues - Execute commands - Report results\"]\n    UI[\"UI Server - Web interface - Visualization - Monitoring\"]\n    StateManager[\"State Manager - Persists workflow state - Enables communication - Abstracts storage backend\"]\n\n    Redis[\"Redis (Production)\"]\n    PostgreSQL[\"PostgreSQL (Large-scale)\"]\n    InMemory[\"In-Memory (Development)\"]\n\n    Tasks[\"Task Definitions\"] --&gt; Orchestrator\n    DAGs[\"DAG Definitions\"] --&gt; Orchestrator\n    External[\"External Systems (Triggers/APIs)\"] &lt;--&gt; UI\n    Users[\"Users (Web Interface)\"] &lt;--&gt; UI\n    Scripts[\"Target Scripts and Commands\"] &lt;--&gt; Agents\n\n    Orchestrator &lt;--&gt; StateManager\n    Agents &lt;--&gt; StateManager\n    UI &lt;--&gt; StateManager\n    StateManager --- Redis\n    StateManager --- PostgreSQL\n    StateManager --- InMemory\n\n\n\n\n\n\nCyclonetix consists of the following key components:\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nOrchestrator\nBuilds execution graphs, evaluates task readiness, and schedules work.\n\n\nAgent\nPicks up tasks from queues and executes them.\n\n\nExecution Graph\nSelf-assembled DAG built from tasks, outcomes, and dependencies.\n\n\nState Manager\nStores task states, execution metadata, and scheduled outcomes.\n\n\nUI\nProvides real-time execution tracking and DAG visualization.\n\n\nAuthentication\nEnsures secure access to Cyclonetix resources.\n\n\n\n\n\n\n\nOrchestrator - The orchestrator is responsible for:\n\nBuilding and updating execution graphs\nDetermining which tasks are ready to execute\nScheduling tasks onto appropriate queues\nMonitoring the overall execution state\n\nAgent - The agent is responsible for:\n\nMonitoring one or more queues for available work\nExecuting tasks with their required environment\nReporting task success or failure\nMaintaining heartbeats for health monitoring\n\nState Manager - Provides a storage backend that:\n\nPersists task and DAG definitions\nMaintains execution state\nEnables communication between components\nSupports different backends (Redis, PostgreSQL, in-memory)\n\nUI Server - Provides a web interface for:\n\nMonitoring execution progress\nVisualizing DAGs\nScheduling new tasks or DAGs\nViewing logs and results\n\n\n\n\n\nCyclonetix supports multiple backend storage systems:\n\nIn-memory: For local development and testing\nRedis: For production deployments with moderate scale\nPostgreSQL: (Planned) For high-scale production deployments\n\nThe StateManager trait abstracts these implementations, allowing you to swap backends without changing your application code.\n\n\n\nWhen a task is executed, it flows through several components:\n\nScheduler (part of the Orchestrator):\n\nDetermines task readiness based on dependencies\nAssigns tasks to appropriate queues\n\nQueues:\n\nStore tasks ready for execution\nAllow prioritization and categorization\n\nAgents:\n\nPull tasks from queues\nExecute tasks in isolated environments\nReport results back to the state manager\n\nEvent System:\n\nNotifies the orchestrator of task completion\nTriggers evaluation of dependent tasks\n\n\n\n\n\nCyclonetix is designed to handle failures at multiple levels:\n\nTask Failures: Failed tasks are marked and can be retried\nAgent Failures: Heartbeat monitoring detects failed agents, and their tasks are reassigned\nOrchestrator Failures: Multiple orchestrators can run in parallel with work distribution\nState Recovery: On startup, the system recovers in-progress executions from the state backend\n\n\n\n\n\nLearn about Tasks and Dependencies\nUnderstand the Execution Flow\nExplore the different Scheduling Models",
    "crumbs": [
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "core-concepts/architecture.html#core-components",
    "href": "core-concepts/architecture.html#core-components",
    "title": "Architecture Overview",
    "section": "",
    "text": "%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\nflowchart TB\n    Orchestrator[\"Orchestrator - Builds execution graphs - Evaluates task readiness - Schedules work\"]\n    Agents[\"Agents - Pick up tasks from queues - Execute commands - Report results\"]\n    UI[\"UI Server - Web interface - Visualization - Monitoring\"]\n    StateManager[\"State Manager - Persists workflow state - Enables communication - Abstracts storage backend\"]\n\n    Redis[\"Redis (Production)\"]\n    PostgreSQL[\"PostgreSQL (Large-scale)\"]\n    InMemory[\"In-Memory (Development)\"]\n\n    Tasks[\"Task Definitions\"] --&gt; Orchestrator\n    DAGs[\"DAG Definitions\"] --&gt; Orchestrator\n    External[\"External Systems (Triggers/APIs)\"] &lt;--&gt; UI\n    Users[\"Users (Web Interface)\"] &lt;--&gt; UI\n    Scripts[\"Target Scripts and Commands\"] &lt;--&gt; Agents\n\n    Orchestrator &lt;--&gt; StateManager\n    Agents &lt;--&gt; StateManager\n    UI &lt;--&gt; StateManager\n    StateManager --- Redis\n    StateManager --- PostgreSQL\n    StateManager --- InMemory\n\n\n\n\n\n\nCyclonetix consists of the following key components:\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nOrchestrator\nBuilds execution graphs, evaluates task readiness, and schedules work.\n\n\nAgent\nPicks up tasks from queues and executes them.\n\n\nExecution Graph\nSelf-assembled DAG built from tasks, outcomes, and dependencies.\n\n\nState Manager\nStores task states, execution metadata, and scheduled outcomes.\n\n\nUI\nProvides real-time execution tracking and DAG visualization.\n\n\nAuthentication\nEnsures secure access to Cyclonetix resources.",
    "crumbs": [
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "core-concepts/architecture.html#component-interactions",
    "href": "core-concepts/architecture.html#component-interactions",
    "title": "Architecture Overview",
    "section": "",
    "text": "Orchestrator - The orchestrator is responsible for:\n\nBuilding and updating execution graphs\nDetermining which tasks are ready to execute\nScheduling tasks onto appropriate queues\nMonitoring the overall execution state\n\nAgent - The agent is responsible for:\n\nMonitoring one or more queues for available work\nExecuting tasks with their required environment\nReporting task success or failure\nMaintaining heartbeats for health monitoring\n\nState Manager - Provides a storage backend that:\n\nPersists task and DAG definitions\nMaintains execution state\nEnables communication between components\nSupports different backends (Redis, PostgreSQL, in-memory)\n\nUI Server - Provides a web interface for:\n\nMonitoring execution progress\nVisualizing DAGs\nScheduling new tasks or DAGs\nViewing logs and results",
    "crumbs": [
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "core-concepts/architecture.html#modular-backend-design",
    "href": "core-concepts/architecture.html#modular-backend-design",
    "title": "Architecture Overview",
    "section": "",
    "text": "Cyclonetix supports multiple backend storage systems:\n\nIn-memory: For local development and testing\nRedis: For production deployments with moderate scale\nPostgreSQL: (Planned) For high-scale production deployments\n\nThe StateManager trait abstracts these implementations, allowing you to swap backends without changing your application code.",
    "crumbs": [
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "core-concepts/architecture.html#task-execution-flow",
    "href": "core-concepts/architecture.html#task-execution-flow",
    "title": "Architecture Overview",
    "section": "",
    "text": "When a task is executed, it flows through several components:\n\nScheduler (part of the Orchestrator):\n\nDetermines task readiness based on dependencies\nAssigns tasks to appropriate queues\n\nQueues:\n\nStore tasks ready for execution\nAllow prioritization and categorization\n\nAgents:\n\nPull tasks from queues\nExecute tasks in isolated environments\nReport results back to the state manager\n\nEvent System:\n\nNotifies the orchestrator of task completion\nTriggers evaluation of dependent tasks",
    "crumbs": [
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "core-concepts/architecture.html#resilience-and-recovery",
    "href": "core-concepts/architecture.html#resilience-and-recovery",
    "title": "Architecture Overview",
    "section": "",
    "text": "Cyclonetix is designed to handle failures at multiple levels:\n\nTask Failures: Failed tasks are marked and can be retried\nAgent Failures: Heartbeat monitoring detects failed agents, and their tasks are reassigned\nOrchestrator Failures: Multiple orchestrators can run in parallel with work distribution\nState Recovery: On startup, the system recovers in-progress executions from the state backend",
    "crumbs": [
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "core-concepts/architecture.html#next-steps",
    "href": "core-concepts/architecture.html#next-steps",
    "title": "Architecture Overview",
    "section": "",
    "text": "Learn about Tasks and Dependencies\nUnderstand the Execution Flow\nExplore the different Scheduling Models",
    "crumbs": [
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "core-concepts/execution-flow.html",
    "href": "core-concepts/execution-flow.html",
    "title": "Execution Flow",
    "section": "",
    "text": "Understanding how Cyclonetix executes tasks and manages workflow progression is key to effectively utilizing the system and troubleshooting when needed.\n\n\nLet’s walk through the complete lifecycle of a workflow execution:\n\n\nThe execution begins when either:\n\nA user schedules a DAG explicitly via the UI or CLI\nA user schedules an outcome (target task) and Cyclonetix resolves dependencies\nAn external trigger initiates execution via the API\n\nDuring scheduling, Cyclonetix:\n\nCreates a unique run ID for the execution\nBuilds an execution graph (DAG) of all required tasks\nPersists the execution state in the state manager\nTransitions the DAG status to “Pending”\n\n\n\n\nOnce scheduled, the orchestrator evaluates the execution graph to determine which tasks are ready to execute:\n\nTasks with no dependencies are immediately ready\nTasks with dependencies wait until all dependencies are satisfied\nThe graph is continuously evaluated as tasks complete\n\n\n\n\nWhen a task is determined to be ready:\n\nIts status changes to “Queued”\nIt’s placed in the appropriate execution queue\nRequired environment variables from contexts are prepared\n\n\n\n\nAgents continuously monitor queues for available work:\n\nWhen a task is found, the agent claims it\nThe agent executes the task’s command in the appropriate environment\nTask status is updated to “Running”\nAgent sends heartbeats to indicate it’s still processing the task\n\n\n\n\nWhen task execution finishes:\n\nThe agent updates the task status to “Completed” or “Failed”\nThe agent releases the task assignment\nThe orchestrator is notified of the state change\n\n\n\n\nAfter each task completion:\n\nThe orchestrator re-evaluates the execution graph\nNewly eligible tasks are queued\nThe process continues until all tasks are completed or failed\n\n\n\n\nWhen all tasks in the DAG are processed:\n\nThe DAG status is updated to “Completed” if all tasks succeeded\nThe DAG status is updated to “Failed” if any critical tasks failed\nFinal metrics and statistics are calculated\n\n\n\n\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; }'}}%%\nsequenceDiagram\n    participant Client as Client\n    participant Orch as Orchestrator\n    participant State as State Manager\n    participant Agent as Agent\n\n    Client-&gt;&gt;Orch: Schedule Task\n    Orch-&gt;&gt;State: Create & save execution plan\n\n    loop Task Execution\n        Orch-&gt;&gt;State: Find ready tasks\n        Orch--&gt;&gt;Agent: Assign tasks\n\n        Agent-&gt;&gt;Agent: Execute task\n\n        Agent-&gt;&gt;State: Update task result\n        State--&gt;&gt;Orch: Notify state change\n    end\n\n    Orch-&gt;&gt;State: Update final status\n    State--&gt;&gt;Client: Execution complete\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; }'}}%%\nstateDiagram-v2\n    [*] --&gt; Pending: Task Created\n    Pending --&gt; Queued: Task Ready\n    Queued --&gt; Running: Agent Picks Up\n    Running --&gt; Completed: Execution Success\n    Running --&gt; Failed: Execution Error\n    Failed --&gt; Queued: Retry\n    Completed --&gt; [*]\n    Failed --&gt; [*]\n\n\n\n\n\n\nEach state transition is recorded in the state manager, allowing for tracking and visualization of progress.\n\n\n\nCyclonetix has several mechanisms for handling failures:\n\n\nTasks can be configured with retry parameters:\n\nMaximum retry count\nRetry delay\nExponential backoff\n\n\n\n\nIf an agent stops sending heartbeats:\n\nTasks assigned to the agent are identified\nThese tasks are reset to “Queued” status\nThey are picked up by other available agents\n\n\n\n\nIf an orchestrator fails and restarts:\n\nIt loads incomplete DAGs from the state manager\nIt reconstructs the execution state\nIt continues processing from where it left off\n\n\n\n\n\nThe execution flow can be monitored through:\n\nThe Cyclonetix UI, showing real-time execution status\nLogs containing detailed execution information\nMetrics tracking execution times, success rates, etc.\n\n\n\n\n\nLearn about Scheduling Models\nUnderstand how to use Contexts & Parameters\nExplore Evaluation Points for dynamic workflows",
    "crumbs": [
      "Core Concepts",
      "Execution Flow"
    ]
  },
  {
    "objectID": "core-concepts/execution-flow.html#execution-lifecycle",
    "href": "core-concepts/execution-flow.html#execution-lifecycle",
    "title": "Execution Flow",
    "section": "",
    "text": "Let’s walk through the complete lifecycle of a workflow execution:\n\n\nThe execution begins when either:\n\nA user schedules a DAG explicitly via the UI or CLI\nA user schedules an outcome (target task) and Cyclonetix resolves dependencies\nAn external trigger initiates execution via the API\n\nDuring scheduling, Cyclonetix:\n\nCreates a unique run ID for the execution\nBuilds an execution graph (DAG) of all required tasks\nPersists the execution state in the state manager\nTransitions the DAG status to “Pending”\n\n\n\n\nOnce scheduled, the orchestrator evaluates the execution graph to determine which tasks are ready to execute:\n\nTasks with no dependencies are immediately ready\nTasks with dependencies wait until all dependencies are satisfied\nThe graph is continuously evaluated as tasks complete\n\n\n\n\nWhen a task is determined to be ready:\n\nIts status changes to “Queued”\nIt’s placed in the appropriate execution queue\nRequired environment variables from contexts are prepared\n\n\n\n\nAgents continuously monitor queues for available work:\n\nWhen a task is found, the agent claims it\nThe agent executes the task’s command in the appropriate environment\nTask status is updated to “Running”\nAgent sends heartbeats to indicate it’s still processing the task\n\n\n\n\nWhen task execution finishes:\n\nThe agent updates the task status to “Completed” or “Failed”\nThe agent releases the task assignment\nThe orchestrator is notified of the state change\n\n\n\n\nAfter each task completion:\n\nThe orchestrator re-evaluates the execution graph\nNewly eligible tasks are queued\nThe process continues until all tasks are completed or failed\n\n\n\n\nWhen all tasks in the DAG are processed:\n\nThe DAG status is updated to “Completed” if all tasks succeeded\nThe DAG status is updated to “Failed” if any critical tasks failed\nFinal metrics and statistics are calculated",
    "crumbs": [
      "Core Concepts",
      "Execution Flow"
    ]
  },
  {
    "objectID": "core-concepts/execution-flow.html#component-interactions",
    "href": "core-concepts/execution-flow.html#component-interactions",
    "title": "Execution Flow",
    "section": "",
    "text": "%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; }'}}%%\nsequenceDiagram\n    participant Client as Client\n    participant Orch as Orchestrator\n    participant State as State Manager\n    participant Agent as Agent\n\n    Client-&gt;&gt;Orch: Schedule Task\n    Orch-&gt;&gt;State: Create & save execution plan\n\n    loop Task Execution\n        Orch-&gt;&gt;State: Find ready tasks\n        Orch--&gt;&gt;Agent: Assign tasks\n\n        Agent-&gt;&gt;Agent: Execute task\n\n        Agent-&gt;&gt;State: Update task result\n        State--&gt;&gt;Orch: Notify state change\n    end\n\n    Orch-&gt;&gt;State: Update final status\n    State--&gt;&gt;Client: Execution complete",
    "crumbs": [
      "Core Concepts",
      "Execution Flow"
    ]
  },
  {
    "objectID": "core-concepts/execution-flow.html#state-transitions",
    "href": "core-concepts/execution-flow.html#state-transitions",
    "title": "Execution Flow",
    "section": "",
    "text": "%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; }'}}%%\nstateDiagram-v2\n    [*] --&gt; Pending: Task Created\n    Pending --&gt; Queued: Task Ready\n    Queued --&gt; Running: Agent Picks Up\n    Running --&gt; Completed: Execution Success\n    Running --&gt; Failed: Execution Error\n    Failed --&gt; Queued: Retry\n    Completed --&gt; [*]\n    Failed --&gt; [*]\n\n\n\n\n\n\nEach state transition is recorded in the state manager, allowing for tracking and visualization of progress.",
    "crumbs": [
      "Core Concepts",
      "Execution Flow"
    ]
  },
  {
    "objectID": "core-concepts/execution-flow.html#handling-failures",
    "href": "core-concepts/execution-flow.html#handling-failures",
    "title": "Execution Flow",
    "section": "",
    "text": "Cyclonetix has several mechanisms for handling failures:\n\n\nTasks can be configured with retry parameters:\n\nMaximum retry count\nRetry delay\nExponential backoff\n\n\n\n\nIf an agent stops sending heartbeats:\n\nTasks assigned to the agent are identified\nThese tasks are reset to “Queued” status\nThey are picked up by other available agents\n\n\n\n\nIf an orchestrator fails and restarts:\n\nIt loads incomplete DAGs from the state manager\nIt reconstructs the execution state\nIt continues processing from where it left off",
    "crumbs": [
      "Core Concepts",
      "Execution Flow"
    ]
  },
  {
    "objectID": "core-concepts/execution-flow.html#monitoring-and-observability",
    "href": "core-concepts/execution-flow.html#monitoring-and-observability",
    "title": "Execution Flow",
    "section": "",
    "text": "The execution flow can be monitored through:\n\nThe Cyclonetix UI, showing real-time execution status\nLogs containing detailed execution information\nMetrics tracking execution times, success rates, etc.",
    "crumbs": [
      "Core Concepts",
      "Execution Flow"
    ]
  },
  {
    "objectID": "core-concepts/execution-flow.html#next-steps",
    "href": "core-concepts/execution-flow.html#next-steps",
    "title": "Execution Flow",
    "section": "",
    "text": "Learn about Scheduling Models\nUnderstand how to use Contexts & Parameters\nExplore Evaluation Points for dynamic workflows",
    "crumbs": [
      "Core Concepts",
      "Execution Flow"
    ]
  },
  {
    "objectID": "build-notes/api_design.html",
    "href": "build-notes/api_design.html",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "This document outlines the REST API endpoints needed to support the Cyclonetix orchestration framework’s UI. The API is organized by functional areas that correspond to the core pages and features defined in the UI wireframes.\n\n\n\nAll API endpoints are relative to: https://api.cyclonetix.io/v1\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nPOST\n/auth/login\nAuthenticate user and return a JWT token\n\n\nPOST\n/auth/logout\nLog out user and invalidate token\n\n\nPOST\n/auth/refresh\nRefresh an expiring JWT token\n\n\nPOST\n/auth/forgot-password\nInitiate password reset workflow\n\n\nPOST\n/auth/reset-password\nComplete password reset with token\n\n\n\n\n\n\n\n\nPOST /auth/login\nRequest:\n{\n  \"username\": \"user@example.com\",\n  \"password\": \"secure_password\"\n}\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n    \"expires_at\": \"2025-03-11T14:30:00Z\",\n    \"user\": {\n      \"id\": \"usr_123456\",\n      \"username\": \"user@example.com\",\n      \"name\": \"John Doe\",\n      \"role\": \"admin\"\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/dashboard/summary\nGet summary metrics for dashboard\n\n\nGET\n/dashboard/queues\nGet queue activity metrics with sparkline data\n\n\nGET\n/dashboard/dags/running\nGet list of currently running DAGs\n\n\nGET\n/dashboard/dags/failed\nGet list of recently failed DAGs\n\n\nGET\n/dashboard/dags/completed\nGet list of recently completed DAGs\n\n\n\n\n\n\n\n\nGET /dashboard/summary\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"total_dags\": 256,\n    \"running_dags\": 12,\n    \"completed_dags_24h\": 183,\n    \"failed_dags_24h\": 7,\n    \"active_agents\": 8,\n    \"pending_tasks\": 34,\n    \"system_load\": 68.2\n  }\n}\n\n\n\nGET /dashboard/queues\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"queues\": [\n      {\n        \"id\": \"q_default\",\n        \"name\": \"Default Queue\",\n        \"current_depth\": 5,\n        \"tasks_processed_24h\": 1245,\n        \"avg_wait_time_ms\": 340,\n        \"sparkline_data\": [4, 7, 12, 8, 5, 3, 5, 9, 11, 5]\n      },\n      {\n        \"id\": \"q_high_priority\",\n        \"name\": \"High Priority\",\n        \"current_depth\": 1,\n        \"tasks_processed_24h\": 567,\n        \"avg_wait_time_ms\": 120,\n        \"sparkline_data\": [2, 3, 1, 0, 1, 2, 3, 2, 1, 1]\n      }\n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/dags\nGet paginated list of all DAGs\n\n\nGET\n/dags/{dag_id}\nGet details of a specific DAG\n\n\nPOST\n/dags\nCreate a new DAG\n\n\nPUT\n/dags/{dag_id}\nUpdate an existing DAG\n\n\nDELETE\n/dags/{dag_id}\nDelete a DAG\n\n\nPOST\n/dags/{dag_id}/trigger\nTrigger execution of a DAG\n\n\nGET\n/dags/search\nSearch for DAGs by name, owner, or description\n\n\n\n\n\n\n\n\nGET /dags?page=1&limit=20&status=active\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"dags\": [\n      {\n        \"id\": \"dag_123456\",\n        \"name\": \"Daily Analytics Processing\",\n        \"description\": \"Processes daily analytics data\",\n        \"owner\": \"data_team\",\n        \"is_active\": true,\n        \"schedule\": \"0 0 * * *\",\n        \"last_run\": \"2025-03-03T00:00:00Z\",\n        \"next_run\": \"2025-03-04T00:00:00Z\",\n        \"last_status\": \"success\",\n        \"task_count\": 8\n      },\n      ...\n    ],\n    \"pagination\": {\n      \"page\": 1,\n      \"limit\": 20,\n      \"total\": 256,\n      \"pages\": 13\n    }\n  }\n}\n\n\n\nGET /dags/dag_123456\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"dag_123456\",\n    \"name\": \"Daily Analytics Processing\",\n    \"description\": \"Processes daily analytics data\",\n    \"owner\": \"data_team\",\n    \"is_active\": true,\n    \"schedule\": \"0 0 * * *\",\n    \"created_at\": \"2024-12-01T09:15:00Z\",\n    \"updated_at\": \"2025-02-15T14:22:00Z\",\n    \"last_run\": \"2025-03-03T00:00:00Z\",\n    \"next_run\": \"2025-03-04T00:00:00Z\",\n    \"last_status\": \"success\",\n    \"tasks\": [\n      {\n        \"id\": \"task_a1b2c3\",\n        \"name\": \"Extract Data\",\n        \"type\": \"python\",\n        \"upstream\": [],\n        \"downstream\": [\"task_d4e5f6\"]\n      },\n      {\n        \"id\": \"task_d4e5f6\",\n        \"name\": \"Transform Data\",\n        \"type\": \"python\",\n        \"upstream\": [\"task_a1b2c3\"],\n        \"downstream\": [\"task_g7h8i9\"]\n      },\n      {\n        \"id\": \"task_g7h8i9\",\n        \"name\": \"Load Data\",\n        \"type\": \"python\",\n        \"upstream\": [\"task_d4e5f6\"],\n        \"downstream\": []\n      }\n    ],\n    \"edges\": [\n      {\n        \"source\": \"task_a1b2c3\",\n        \"target\": \"task_d4e5f6\"\n      },\n      {\n        \"source\": \"task_d4e5f6\",\n        \"target\": \"task_g7h8i9\"\n      }\n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/executions\nGet paginated list of DAG executions\n\n\nGET\n/executions/{execution_id}\nGet details of a specific execution\n\n\nPOST\n/executions/{execution_id}/cancel\nCancel a running execution\n\n\nGET\n/executions/{execution_id}/tasks\nGet tasks for a specific execution\n\n\nGET\n/executions/{execution_id}/logs\nGet execution logs\n\n\nGET\n/executions/{execution_id}/graph\nGet execution graph with state information\n\n\n\n\n\n\n\n\nGET /executions/exec_789012\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"exec_789012\",\n    \"dag_id\": \"dag_123456\",\n    \"dag_name\": \"Daily Analytics Processing\",\n    \"status\": \"running\",\n    \"start_time\": \"2025-03-04T00:00:00Z\",\n    \"end_time\": null,\n    \"duration_s\": 345,\n    \"triggered_by\": \"scheduler\",\n    \"execution_params\": {\n      \"date\": \"2025-03-03\"\n    },\n    \"task_summary\": {\n      \"total\": 8,\n      \"succeeded\": 3,\n      \"failed\": 0,\n      \"running\": 2,\n      \"pending\": 3\n    }\n  }\n}\n\n\n\nGET /executions/exec_789012/graph\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"nodes\": [\n      {\n        \"id\": \"task_a1b2c3\",\n        \"label\": \"Extract Data\",\n        \"status\": \"success\",\n        \"start_time\": \"2025-03-04T00:00:00Z\",\n        \"end_time\": \"2025-03-04T00:01:23Z\",\n        \"duration_s\": 83\n      },\n      {\n        \"id\": \"task_d4e5f6\",\n        \"label\": \"Transform Data\",\n        \"status\": \"running\",\n        \"start_time\": \"2025-03-04T00:01:24Z\",\n        \"end_time\": null,\n        \"duration_s\": 261\n      }\n    ],\n    \"edges\": [\n      {\n        \"source\": \"task_a1b2c3\",\n        \"target\": \"task_d4e5f6\",\n        \"data_transferred\": \"1.2GB\"\n      }\n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/tasks/{task_id}\nGet task details\n\n\nPUT\n/tasks/{task_id}\nUpdate task properties\n\n\nGET\n/tasks/{task_id}/code\nGet task script code\n\n\nPUT\n/tasks/{task_id}/code\nUpdate task script code\n\n\nGET\n/tasks/{task_id}/logs\nGet task execution logs\n\n\nPOST\n/tasks/{task_id}/retry\nRetry a failed task\n\n\nPOST\n/tasks/{task_id}/test\nTest execute a task with sample data\n\n\n\n\n\n\n\n\nGET /tasks/task_a1b2c3/code\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"code\": \"import pandas as pd\\n\\ndef extract_data():\\n    df = pd.read_csv('s3://data-bucket/raw/daily.csv')\\n    return df\\n\\nresult = extract_data()\",\n    \"language\": \"python\",\n    \"version\": \"3.10\",\n    \"last_updated\": \"2025-02-15T14:22:00Z\",\n    \"last_updated_by\": \"user@example.com\"\n  }\n}\n\n\n\nPUT /tasks/task_a1b2c3/code\nRequest:\n{\n  \"code\": \"import pandas as pd\\n\\ndef extract_data():\\n    # Updated to use the new data source\\n    df = pd.read_csv('s3://new-data-bucket/raw/daily.csv')\\n    return df\\n\\nresult = extract_data()\",\n  \"language\": \"python\",\n  \"version\": \"3.10\",\n  \"commit_message\": \"Updated data source\"\n}\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"task_a1b2c3\",\n    \"version\": 4,\n    \"updated_at\": \"2025-03-04T12:34:56Z\",\n    \"validation_status\": \"passed\"\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/designer/task-templates\nGet available task templates\n\n\nPOST\n/designer/validate-dag\nValidate DAG structure without saving\n\n\nPOST\n/designer/validate-task\nValidate task properties without saving\n\n\nGET\n/designer/inputs-outputs/{task_type}\nGet I/O schema for a task type\n\n\n\n\n\n\n\n\nGET /designer/task-templates\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"templates\": [\n      {\n        \"id\": \"python_script\",\n        \"name\": \"Python Script\",\n        \"icon\": \"mdi-language-python\",\n        \"description\": \"Run a Python script\",\n        \"properties\": {\n          \"version\": [\"3.8\", \"3.9\", \"3.10\", \"3.11\"],\n          \"timeout\": 3600,\n          \"memory\": 2048\n        },\n        \"code_template\": \"def main():\\n    # Your code here\\n    pass\\n\\nresult = main()\"\n      },\n      {\n        \"id\": \"http_request\",\n        \"name\": \"HTTP Request\",\n        \"icon\": \"mdi-api\",\n        \"description\": \"Make an HTTP request\",\n        \"properties\": {\n          \"method\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n          \"timeout\": 60,\n          \"retry\": 3\n        }\n      }\n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/secrets\nGet list of all accessible secrets (names only)\n\n\nPOST\n/secrets\nCreate a new secret\n\n\nGET\n/secrets/{secret_id}\nGet secret metadata (not the value)\n\n\nPUT\n/secrets/{secret_id}\nUpdate a secret\n\n\nDELETE\n/secrets/{secret_id}\nDelete a secret\n\n\nGET\n/secrets/{secret_id}/permissions\nGet permissions for a secret\n\n\nPUT\n/secrets/{secret_id}/permissions\nUpdate permissions for a secret\n\n\n\n\n\n\n\n\nGET /secrets\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"secrets\": [\n      {\n        \"id\": \"sec_abc123\",\n        \"name\": \"AWS_CREDENTIALS\",\n        \"description\": \"AWS credentials for data team\",\n        \"created_at\": \"2025-01-15T09:10:00Z\",\n        \"updated_at\": \"2025-02-20T14:30:00Z\",\n        \"type\": \"key_value\",\n        \"owner\": \"data_team\"\n      },\n      {\n        \"id\": \"sec_def456\",\n        \"name\": \"DATABASE_CREDENTIALS\",\n        \"description\": \"Production database access\",\n        \"created_at\": \"2024-12-10T11:25:00Z\",\n        \"updated_at\": \"2024-12-10T11:25:00Z\",\n        \"type\": \"key_value\",\n        \"owner\": \"infrastructure_team\"\n      }\n    ]\n  }\n}\n\n\n\nPOST /secrets\nRequest:\n{\n  \"name\": \"API_TOKEN\",\n  \"description\": \"Token for external API access\",\n  \"type\": \"key_value\",\n  \"values\": {\n    \"api_key\": \"sk_live_1234567890abcdef\",\n    \"api_secret\": \"xyz_123456_abcdef\"\n  },\n  \"permissions\": [\n    {\n      \"team_id\": \"team_integration\",\n      \"access_level\": \"read\"\n    }\n  ]\n}\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"sec_ghi789\",\n    \"name\": \"API_TOKEN\",\n    \"description\": \"Token for external API access\",\n    \"created_at\": \"2025-03-04T12:34:56Z\",\n    \"updated_at\": \"2025-03-04T12:34:56Z\",\n    \"type\": \"key_value\"\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/agents\nGet list of all execution agents\n\n\nGET\n/agents/{agent_id}\nGet details of a specific agent\n\n\nPOST\n/agents\nRegister a new agent\n\n\nPUT\n/agents/{agent_id}\nUpdate agent configuration\n\n\nDELETE\n/agents/{agent_id}\nDeregister an agent\n\n\nPOST\n/agents/{agent_id}/restart\nRestart an agent\n\n\nPOST\n/agents/{agent_id}/drain\nDrain tasks from an agent\n\n\nGET\n/agents/{agent_id}/metrics\nGet performance metrics for an agent\n\n\n\n\n\n\n\n\nGET /agents\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"agents\": [\n      {\n        \"id\": \"agent_123\",\n        \"name\": \"worker-01\",\n        \"status\": \"running\",\n        \"ip\": \"10.0.1.5\",\n        \"registered_at\": \"2025-01-01T00:00:00Z\",\n        \"last_heartbeat\": \"2025-03-04T12:30:12Z\",\n        \"version\": \"1.2.3\",\n        \"queues\": [\"default\", \"high_priority\"],\n        \"tags\": [\"python\", \"data_processing\"],\n        \"current_tasks\": 2,\n        \"capacity\": 5,\n        \"system_load\": 42.5\n      },\n      {\n        \"id\": \"agent_456\",\n        \"name\": \"worker-02\",\n        \"status\": \"running\",\n        \"ip\": \"10.0.1.6\",\n        \"registered_at\": \"2025-01-01T00:00:00Z\",\n        \"last_heartbeat\": \"2025-03-04T12:30:08Z\",\n        \"version\": \"1.2.3\",\n        \"queues\": [\"default\"],\n        \"tags\": [\"python\", \"ml\"],\n        \"current_tasks\": 1,\n        \"capacity\": 3,\n        \"system_load\": 68.3\n      }\n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/queues\nGet list of all task queues\n\n\nGET\n/queues/{queue_id}\nGet details of a specific queue\n\n\nPOST\n/queues\nCreate a new queue\n\n\nPUT\n/queues/{queue_id}\nUpdate queue properties\n\n\nDELETE\n/queues/{queue_id}\nDelete a queue\n\n\nGET\n/queues/{queue_id}/tasks\nGet tasks in a queue\n\n\nPOST\n/queues/{queue_id}/purge\nPurge all tasks from a queue\n\n\nPOST\n/queues/{queue_id}/pause\nPause a queue\n\n\nPOST\n/queues/{queue_id}/resume\nResume a paused queue\n\n\n\n\n\n\n\n\nGET /queues/q_default\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"q_default\",\n    \"name\": \"Default Queue\",\n    \"description\": \"General purpose task queue\",\n    \"status\": \"active\",\n    \"priority\": 1,\n    \"created_at\": \"2024-10-01T00:00:00Z\",\n    \"current_depth\": 5,\n    \"tasks_processed_24h\": 1245,\n    \"avg_wait_time_ms\": 340,\n    \"max_concurrent\": 100,\n    \"assigned_agents\": 5,\n    \"metrics\": {\n      \"hourly_throughput\": [56, 48, 52, 45, 67, 78, 89, 92],\n      \"avg_processing_time_ms\": 512\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/config\nGet all configuration categories\n\n\nGET\n/config/{category}\nGet configuration for a specific category\n\n\nPUT\n/config/{category}\nUpdate configuration for a category\n\n\nPOST\n/config/validate\nValidate configuration changes without applying\n\n\nGET\n/config/history\nGet configuration change history\n\n\n\n\n\n\n\n\nGET /config/scheduler\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"category\": \"scheduler\",\n    \"description\": \"Task scheduler settings\",\n    \"updated_at\": \"2025-02-10T15:30:00Z\",\n    \"updated_by\": \"admin@example.com\",\n    \"settings\": [\n      {\n        \"key\": \"scheduler.interval_seconds\",\n        \"value\": 30,\n        \"default_value\": 60,\n        \"description\": \"How often scheduler checks for new tasks\",\n        \"type\": \"integer\",\n        \"range\": [10, 300],\n        \"requires_restart\": true\n      },\n      {\n        \"key\": \"scheduler.max_concurrent_tasks\",\n        \"value\": 1000,\n        \"default_value\": 500,\n        \"description\": \"Maximum number of concurrent tasks\",\n        \"type\": \"integer\",\n        \"range\": [100, 10000],\n        \"requires_restart\": false\n      }\n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/metrics/system\nGet system-wide performance metrics\n\n\nGET\n/metrics/scheduler\nGet scheduler performance metrics\n\n\nGET\n/metrics/workers\nGet worker performance metrics\n\n\nGET\n/metrics/database\nGet database performance metrics\n\n\nGET\n/metrics/redis\nGet Redis/queue performance metrics\n\n\nGET\n/metrics/dags\nGet DAG execution metrics\n\n\n\n\n\n\n\n\nGET /metrics/system\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"timestamp\": \"2025-03-04T12:34:56Z\",\n    \"uptime_days\": 45.2,\n    \"cpu_usage_percent\": 68.5,\n    \"memory_usage_percent\": 72.3,\n    \"disk_usage_percent\": 45.8,\n    \"network_in_mbps\": 125.6,\n    \"network_out_mbps\": 87.2,\n    \"active_connections\": 35,\n    \"total_dags\": 256,\n    \"total_tasks\": 1856,\n    \"historical\": {\n      \"interval\": \"hourly\",\n      \"periods\": 24,\n      \"cpu_usage\": [65.2, 67.8, 70.1, 68.5, 64.2, 62.8, 63.5, 67.8],\n      \"memory_usage\": [70.1, 71.2, 72.8, 72.3, 71.5, 71.2, 72.0, 72.3]\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/users\nGet paginated list of all users\n\n\nGET\n/users/{user_id}\nGet details of a specific user\n\n\nPOST\n/users\nCreate a new user\n\n\nPUT\n/users/{user_id}\nUpdate user information\n\n\nDELETE\n/users/{user_id}\nDelete a user\n\n\nGET\n/users/{user_id}/permissions\nGet user permissions\n\n\nPUT\n/users/{user_id}/permissions\nUpdate user permissions\n\n\nGET\n/users/{user_id}/activity\nGet user activity log\n\n\n\n\n\n\n\n\nGET /users/usr_123456\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"usr_123456\",\n    \"username\": \"john.doe@example.com\",\n    \"name\": \"John Doe\",\n    \"role\": \"data_engineer\",\n    \"created_at\": \"2024-11-01T09:00:00Z\",\n    \"updated_at\": \"2025-02-15T14:30:00Z\",\n    \"last_login\": \"2025-03-04T09:15:23Z\",\n    \"status\": \"active\",\n    \"teams\": [\"data_team\", \"infrastructure\"],\n    \"permissions\": {\n      \"dags\": \"read_write\",\n      \"executions\": \"read_write\",\n      \"agents\": \"read\",\n      \"queues\": \"read\",\n      \"config\": \"none\",\n      \"secrets\": \"read\"\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEndpoint\nDescription\n\n\n\n\nGET\n/teams\nGet list of all teams\n\n\nGET\n/teams/{team_id}\nGet details of a specific team\n\n\nPOST\n/teams\nCreate a new team\n\n\nPUT\n/teams/{team_id}\nUpdate team information\n\n\nDELETE\n/teams/{team_id}\nDelete a team\n\n\nGET\n/teams/{team_id}/members\nGet team members\n\n\nPOST\n/teams/{team_id}/members\nAdd user to team\n\n\nDELETE\n/teams/{team_id}/members/{user_id}\nRemove user from team\n\n\nGET\n/teams/{team_id}/permissions\nGet team permissions\n\n\nPUT\n/teams/{team_id}/permissions\nUpdate team permissions\n\n\n\n\n\n\n\n\nGET /teams/team_data\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"team_data\",\n    \"name\": \"Data Team\",\n    \"description\": \"Data engineering and analytics team\",\n    \"created_at\": \"2024-10-01T00:00:00Z\",\n    \"updated_at\": \"2025-01-15T10:30:00Z\",\n    \"member_count\": 8,\n    \"owner_id\": \"usr_123456\",\n    \"permissions\": {\n      \"dags\": \"read_write\",\n      \"executions\": \"read_write\",\n      \"agents\": \"read\",\n      \"queues\": \"read\",\n      \"config\": \"none\",\n      \"secrets\": \"read_write\"\n    }\n  }\n}\n\n\n\n\n\nAll API endpoints follow a consistent error format:\n{\n  \"status\": \"error\",\n  \"error\": {\n    \"code\": \"invalid_request\",\n    \"message\": \"The request was invalid for the following reasons\",\n    \"details\": [\n      \"Field 'username' is required\",\n      \"Password must be at least 8 characters long\"\n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\nError Code\nHTTP Status\nDescription\n\n\n\n\nunauthorized\n401\nAuthentication required or invalid token\n\n\nforbidden\n403\nInsufficient permissions for the requested operation\n\n\nnot_found\n404\nThe requested resource was not found\n\n\ninvalid_request\n400\nInvalid request parameters or body\n\n\nvalidation_error\n400\nValidation failed for the provided data\n\n\nconflict\n409\nResource conflict (e.g., duplicate name)\n\n\ninternal_error\n500\nServer encountered an unexpected error\n\n\nservice_unavailable\n503\nService is temporarily unavailable\n\n\n\n\n\n\n\nAll list endpoints support pagination with the following query parameters:\n\npage: Page number (default: 1)\nlimit: Items per page (default: 20, max: 100)\nsort: Field to sort by\norder: Sort order (asc or desc)\n\nPagination metadata is included in the response:\n{\n  \"pagination\": {\n    \"page\": 2,\n    \"limit\": 20,\n    \"total\": 256,\n    \"pages\": 13\n  }\n}\n\n\n\nAPI requests are subject to rate limiting. The current limits are included in the HTTP headers of each response:\nX-RateLimit-Limit: 5000\nX-RateLimit-Remaining: 4995\nX-RateLimit-Reset: 1583247600\nWhen the rate limit is exceeded, a 429 Too Many Requests status is returned with a Retry-After header."
  },
  {
    "objectID": "build-notes/api_design.html#overview",
    "href": "build-notes/api_design.html#overview",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "This document outlines the REST API endpoints needed to support the Cyclonetix orchestration framework’s UI. The API is organized by functional areas that correspond to the core pages and features defined in the UI wireframes."
  },
  {
    "objectID": "build-notes/api_design.html#base-url",
    "href": "build-notes/api_design.html#base-url",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "All API endpoints are relative to: https://api.cyclonetix.io/v1"
  },
  {
    "objectID": "build-notes/api_design.html#authentication",
    "href": "build-notes/api_design.html#authentication",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nPOST\n/auth/login\nAuthenticate user and return a JWT token\n\n\nPOST\n/auth/logout\nLog out user and invalidate token\n\n\nPOST\n/auth/refresh\nRefresh an expiring JWT token\n\n\nPOST\n/auth/forgot-password\nInitiate password reset workflow\n\n\nPOST\n/auth/reset-password\nComplete password reset with token\n\n\n\n\n\n\n\n\nPOST /auth/login\nRequest:\n{\n  \"username\": \"user@example.com\",\n  \"password\": \"secure_password\"\n}\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n    \"expires_at\": \"2025-03-11T14:30:00Z\",\n    \"user\": {\n      \"id\": \"usr_123456\",\n      \"username\": \"user@example.com\",\n      \"name\": \"John Doe\",\n      \"role\": \"admin\"\n    }\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#dashboard",
    "href": "build-notes/api_design.html#dashboard",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/dashboard/summary\nGet summary metrics for dashboard\n\n\nGET\n/dashboard/queues\nGet queue activity metrics with sparkline data\n\n\nGET\n/dashboard/dags/running\nGet list of currently running DAGs\n\n\nGET\n/dashboard/dags/failed\nGet list of recently failed DAGs\n\n\nGET\n/dashboard/dags/completed\nGet list of recently completed DAGs\n\n\n\n\n\n\n\n\nGET /dashboard/summary\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"total_dags\": 256,\n    \"running_dags\": 12,\n    \"completed_dags_24h\": 183,\n    \"failed_dags_24h\": 7,\n    \"active_agents\": 8,\n    \"pending_tasks\": 34,\n    \"system_load\": 68.2\n  }\n}\n\n\n\nGET /dashboard/queues\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"queues\": [\n      {\n        \"id\": \"q_default\",\n        \"name\": \"Default Queue\",\n        \"current_depth\": 5,\n        \"tasks_processed_24h\": 1245,\n        \"avg_wait_time_ms\": 340,\n        \"sparkline_data\": [4, 7, 12, 8, 5, 3, 5, 9, 11, 5]\n      },\n      {\n        \"id\": \"q_high_priority\",\n        \"name\": \"High Priority\",\n        \"current_depth\": 1,\n        \"tasks_processed_24h\": 567,\n        \"avg_wait_time_ms\": 120,\n        \"sparkline_data\": [2, 3, 1, 0, 1, 2, 3, 2, 1, 1]\n      }\n    ]\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#dag-management",
    "href": "build-notes/api_design.html#dag-management",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/dags\nGet paginated list of all DAGs\n\n\nGET\n/dags/{dag_id}\nGet details of a specific DAG\n\n\nPOST\n/dags\nCreate a new DAG\n\n\nPUT\n/dags/{dag_id}\nUpdate an existing DAG\n\n\nDELETE\n/dags/{dag_id}\nDelete a DAG\n\n\nPOST\n/dags/{dag_id}/trigger\nTrigger execution of a DAG\n\n\nGET\n/dags/search\nSearch for DAGs by name, owner, or description\n\n\n\n\n\n\n\n\nGET /dags?page=1&limit=20&status=active\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"dags\": [\n      {\n        \"id\": \"dag_123456\",\n        \"name\": \"Daily Analytics Processing\",\n        \"description\": \"Processes daily analytics data\",\n        \"owner\": \"data_team\",\n        \"is_active\": true,\n        \"schedule\": \"0 0 * * *\",\n        \"last_run\": \"2025-03-03T00:00:00Z\",\n        \"next_run\": \"2025-03-04T00:00:00Z\",\n        \"last_status\": \"success\",\n        \"task_count\": 8\n      },\n      ...\n    ],\n    \"pagination\": {\n      \"page\": 1,\n      \"limit\": 20,\n      \"total\": 256,\n      \"pages\": 13\n    }\n  }\n}\n\n\n\nGET /dags/dag_123456\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"dag_123456\",\n    \"name\": \"Daily Analytics Processing\",\n    \"description\": \"Processes daily analytics data\",\n    \"owner\": \"data_team\",\n    \"is_active\": true,\n    \"schedule\": \"0 0 * * *\",\n    \"created_at\": \"2024-12-01T09:15:00Z\",\n    \"updated_at\": \"2025-02-15T14:22:00Z\",\n    \"last_run\": \"2025-03-03T00:00:00Z\",\n    \"next_run\": \"2025-03-04T00:00:00Z\",\n    \"last_status\": \"success\",\n    \"tasks\": [\n      {\n        \"id\": \"task_a1b2c3\",\n        \"name\": \"Extract Data\",\n        \"type\": \"python\",\n        \"upstream\": [],\n        \"downstream\": [\"task_d4e5f6\"]\n      },\n      {\n        \"id\": \"task_d4e5f6\",\n        \"name\": \"Transform Data\",\n        \"type\": \"python\",\n        \"upstream\": [\"task_a1b2c3\"],\n        \"downstream\": [\"task_g7h8i9\"]\n      },\n      {\n        \"id\": \"task_g7h8i9\",\n        \"name\": \"Load Data\",\n        \"type\": \"python\",\n        \"upstream\": [\"task_d4e5f6\"],\n        \"downstream\": []\n      }\n    ],\n    \"edges\": [\n      {\n        \"source\": \"task_a1b2c3\",\n        \"target\": \"task_d4e5f6\"\n      },\n      {\n        \"source\": \"task_d4e5f6\",\n        \"target\": \"task_g7h8i9\"\n      }\n    ]\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#dag-execution",
    "href": "build-notes/api_design.html#dag-execution",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/executions\nGet paginated list of DAG executions\n\n\nGET\n/executions/{execution_id}\nGet details of a specific execution\n\n\nPOST\n/executions/{execution_id}/cancel\nCancel a running execution\n\n\nGET\n/executions/{execution_id}/tasks\nGet tasks for a specific execution\n\n\nGET\n/executions/{execution_id}/logs\nGet execution logs\n\n\nGET\n/executions/{execution_id}/graph\nGet execution graph with state information\n\n\n\n\n\n\n\n\nGET /executions/exec_789012\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"exec_789012\",\n    \"dag_id\": \"dag_123456\",\n    \"dag_name\": \"Daily Analytics Processing\",\n    \"status\": \"running\",\n    \"start_time\": \"2025-03-04T00:00:00Z\",\n    \"end_time\": null,\n    \"duration_s\": 345,\n    \"triggered_by\": \"scheduler\",\n    \"execution_params\": {\n      \"date\": \"2025-03-03\"\n    },\n    \"task_summary\": {\n      \"total\": 8,\n      \"succeeded\": 3,\n      \"failed\": 0,\n      \"running\": 2,\n      \"pending\": 3\n    }\n  }\n}\n\n\n\nGET /executions/exec_789012/graph\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"nodes\": [\n      {\n        \"id\": \"task_a1b2c3\",\n        \"label\": \"Extract Data\",\n        \"status\": \"success\",\n        \"start_time\": \"2025-03-04T00:00:00Z\",\n        \"end_time\": \"2025-03-04T00:01:23Z\",\n        \"duration_s\": 83\n      },\n      {\n        \"id\": \"task_d4e5f6\",\n        \"label\": \"Transform Data\",\n        \"status\": \"running\",\n        \"start_time\": \"2025-03-04T00:01:24Z\",\n        \"end_time\": null,\n        \"duration_s\": 261\n      }\n    ],\n    \"edges\": [\n      {\n        \"source\": \"task_a1b2c3\",\n        \"target\": \"task_d4e5f6\",\n        \"data_transferred\": \"1.2GB\"\n      }\n    ]\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#task-management",
    "href": "build-notes/api_design.html#task-management",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/tasks/{task_id}\nGet task details\n\n\nPUT\n/tasks/{task_id}\nUpdate task properties\n\n\nGET\n/tasks/{task_id}/code\nGet task script code\n\n\nPUT\n/tasks/{task_id}/code\nUpdate task script code\n\n\nGET\n/tasks/{task_id}/logs\nGet task execution logs\n\n\nPOST\n/tasks/{task_id}/retry\nRetry a failed task\n\n\nPOST\n/tasks/{task_id}/test\nTest execute a task with sample data\n\n\n\n\n\n\n\n\nGET /tasks/task_a1b2c3/code\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"code\": \"import pandas as pd\\n\\ndef extract_data():\\n    df = pd.read_csv('s3://data-bucket/raw/daily.csv')\\n    return df\\n\\nresult = extract_data()\",\n    \"language\": \"python\",\n    \"version\": \"3.10\",\n    \"last_updated\": \"2025-02-15T14:22:00Z\",\n    \"last_updated_by\": \"user@example.com\"\n  }\n}\n\n\n\nPUT /tasks/task_a1b2c3/code\nRequest:\n{\n  \"code\": \"import pandas as pd\\n\\ndef extract_data():\\n    # Updated to use the new data source\\n    df = pd.read_csv('s3://new-data-bucket/raw/daily.csv')\\n    return df\\n\\nresult = extract_data()\",\n  \"language\": \"python\",\n  \"version\": \"3.10\",\n  \"commit_message\": \"Updated data source\"\n}\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"task_a1b2c3\",\n    \"version\": 4,\n    \"updated_at\": \"2025-03-04T12:34:56Z\",\n    \"validation_status\": \"passed\"\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#dag-designer",
    "href": "build-notes/api_design.html#dag-designer",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/designer/task-templates\nGet available task templates\n\n\nPOST\n/designer/validate-dag\nValidate DAG structure without saving\n\n\nPOST\n/designer/validate-task\nValidate task properties without saving\n\n\nGET\n/designer/inputs-outputs/{task_type}\nGet I/O schema for a task type\n\n\n\n\n\n\n\n\nGET /designer/task-templates\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"templates\": [\n      {\n        \"id\": \"python_script\",\n        \"name\": \"Python Script\",\n        \"icon\": \"mdi-language-python\",\n        \"description\": \"Run a Python script\",\n        \"properties\": {\n          \"version\": [\"3.8\", \"3.9\", \"3.10\", \"3.11\"],\n          \"timeout\": 3600,\n          \"memory\": 2048\n        },\n        \"code_template\": \"def main():\\n    # Your code here\\n    pass\\n\\nresult = main()\"\n      },\n      {\n        \"id\": \"http_request\",\n        \"name\": \"HTTP Request\",\n        \"icon\": \"mdi-api\",\n        \"description\": \"Make an HTTP request\",\n        \"properties\": {\n          \"method\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n          \"timeout\": 60,\n          \"retry\": 3\n        }\n      }\n    ]\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#secrets-management",
    "href": "build-notes/api_design.html#secrets-management",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/secrets\nGet list of all accessible secrets (names only)\n\n\nPOST\n/secrets\nCreate a new secret\n\n\nGET\n/secrets/{secret_id}\nGet secret metadata (not the value)\n\n\nPUT\n/secrets/{secret_id}\nUpdate a secret\n\n\nDELETE\n/secrets/{secret_id}\nDelete a secret\n\n\nGET\n/secrets/{secret_id}/permissions\nGet permissions for a secret\n\n\nPUT\n/secrets/{secret_id}/permissions\nUpdate permissions for a secret\n\n\n\n\n\n\n\n\nGET /secrets\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"secrets\": [\n      {\n        \"id\": \"sec_abc123\",\n        \"name\": \"AWS_CREDENTIALS\",\n        \"description\": \"AWS credentials for data team\",\n        \"created_at\": \"2025-01-15T09:10:00Z\",\n        \"updated_at\": \"2025-02-20T14:30:00Z\",\n        \"type\": \"key_value\",\n        \"owner\": \"data_team\"\n      },\n      {\n        \"id\": \"sec_def456\",\n        \"name\": \"DATABASE_CREDENTIALS\",\n        \"description\": \"Production database access\",\n        \"created_at\": \"2024-12-10T11:25:00Z\",\n        \"updated_at\": \"2024-12-10T11:25:00Z\",\n        \"type\": \"key_value\",\n        \"owner\": \"infrastructure_team\"\n      }\n    ]\n  }\n}\n\n\n\nPOST /secrets\nRequest:\n{\n  \"name\": \"API_TOKEN\",\n  \"description\": \"Token for external API access\",\n  \"type\": \"key_value\",\n  \"values\": {\n    \"api_key\": \"sk_live_1234567890abcdef\",\n    \"api_secret\": \"xyz_123456_abcdef\"\n  },\n  \"permissions\": [\n    {\n      \"team_id\": \"team_integration\",\n      \"access_level\": \"read\"\n    }\n  ]\n}\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"sec_ghi789\",\n    \"name\": \"API_TOKEN\",\n    \"description\": \"Token for external API access\",\n    \"created_at\": \"2025-03-04T12:34:56Z\",\n    \"updated_at\": \"2025-03-04T12:34:56Z\",\n    \"type\": \"key_value\"\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#agent-management",
    "href": "build-notes/api_design.html#agent-management",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/agents\nGet list of all execution agents\n\n\nGET\n/agents/{agent_id}\nGet details of a specific agent\n\n\nPOST\n/agents\nRegister a new agent\n\n\nPUT\n/agents/{agent_id}\nUpdate agent configuration\n\n\nDELETE\n/agents/{agent_id}\nDeregister an agent\n\n\nPOST\n/agents/{agent_id}/restart\nRestart an agent\n\n\nPOST\n/agents/{agent_id}/drain\nDrain tasks from an agent\n\n\nGET\n/agents/{agent_id}/metrics\nGet performance metrics for an agent\n\n\n\n\n\n\n\n\nGET /agents\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"agents\": [\n      {\n        \"id\": \"agent_123\",\n        \"name\": \"worker-01\",\n        \"status\": \"running\",\n        \"ip\": \"10.0.1.5\",\n        \"registered_at\": \"2025-01-01T00:00:00Z\",\n        \"last_heartbeat\": \"2025-03-04T12:30:12Z\",\n        \"version\": \"1.2.3\",\n        \"queues\": [\"default\", \"high_priority\"],\n        \"tags\": [\"python\", \"data_processing\"],\n        \"current_tasks\": 2,\n        \"capacity\": 5,\n        \"system_load\": 42.5\n      },\n      {\n        \"id\": \"agent_456\",\n        \"name\": \"worker-02\",\n        \"status\": \"running\",\n        \"ip\": \"10.0.1.6\",\n        \"registered_at\": \"2025-01-01T00:00:00Z\",\n        \"last_heartbeat\": \"2025-03-04T12:30:08Z\",\n        \"version\": \"1.2.3\",\n        \"queues\": [\"default\"],\n        \"tags\": [\"python\", \"ml\"],\n        \"current_tasks\": 1,\n        \"capacity\": 3,\n        \"system_load\": 68.3\n      }\n    ]\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#queue-management",
    "href": "build-notes/api_design.html#queue-management",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/queues\nGet list of all task queues\n\n\nGET\n/queues/{queue_id}\nGet details of a specific queue\n\n\nPOST\n/queues\nCreate a new queue\n\n\nPUT\n/queues/{queue_id}\nUpdate queue properties\n\n\nDELETE\n/queues/{queue_id}\nDelete a queue\n\n\nGET\n/queues/{queue_id}/tasks\nGet tasks in a queue\n\n\nPOST\n/queues/{queue_id}/purge\nPurge all tasks from a queue\n\n\nPOST\n/queues/{queue_id}/pause\nPause a queue\n\n\nPOST\n/queues/{queue_id}/resume\nResume a paused queue\n\n\n\n\n\n\n\n\nGET /queues/q_default\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"q_default\",\n    \"name\": \"Default Queue\",\n    \"description\": \"General purpose task queue\",\n    \"status\": \"active\",\n    \"priority\": 1,\n    \"created_at\": \"2024-10-01T00:00:00Z\",\n    \"current_depth\": 5,\n    \"tasks_processed_24h\": 1245,\n    \"avg_wait_time_ms\": 340,\n    \"max_concurrent\": 100,\n    \"assigned_agents\": 5,\n    \"metrics\": {\n      \"hourly_throughput\": [56, 48, 52, 45, 67, 78, 89, 92],\n      \"avg_processing_time_ms\": 512\n    }\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#configuration-management",
    "href": "build-notes/api_design.html#configuration-management",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/config\nGet all configuration categories\n\n\nGET\n/config/{category}\nGet configuration for a specific category\n\n\nPUT\n/config/{category}\nUpdate configuration for a category\n\n\nPOST\n/config/validate\nValidate configuration changes without applying\n\n\nGET\n/config/history\nGet configuration change history\n\n\n\n\n\n\n\n\nGET /config/scheduler\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"category\": \"scheduler\",\n    \"description\": \"Task scheduler settings\",\n    \"updated_at\": \"2025-02-10T15:30:00Z\",\n    \"updated_by\": \"admin@example.com\",\n    \"settings\": [\n      {\n        \"key\": \"scheduler.interval_seconds\",\n        \"value\": 30,\n        \"default_value\": 60,\n        \"description\": \"How often scheduler checks for new tasks\",\n        \"type\": \"integer\",\n        \"range\": [10, 300],\n        \"requires_restart\": true\n      },\n      {\n        \"key\": \"scheduler.max_concurrent_tasks\",\n        \"value\": 1000,\n        \"default_value\": 500,\n        \"description\": \"Maximum number of concurrent tasks\",\n        \"type\": \"integer\",\n        \"range\": [100, 10000],\n        \"requires_restart\": false\n      }\n    ]\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#system-metrics",
    "href": "build-notes/api_design.html#system-metrics",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/metrics/system\nGet system-wide performance metrics\n\n\nGET\n/metrics/scheduler\nGet scheduler performance metrics\n\n\nGET\n/metrics/workers\nGet worker performance metrics\n\n\nGET\n/metrics/database\nGet database performance metrics\n\n\nGET\n/metrics/redis\nGet Redis/queue performance metrics\n\n\nGET\n/metrics/dags\nGet DAG execution metrics\n\n\n\n\n\n\n\n\nGET /metrics/system\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"timestamp\": \"2025-03-04T12:34:56Z\",\n    \"uptime_days\": 45.2,\n    \"cpu_usage_percent\": 68.5,\n    \"memory_usage_percent\": 72.3,\n    \"disk_usage_percent\": 45.8,\n    \"network_in_mbps\": 125.6,\n    \"network_out_mbps\": 87.2,\n    \"active_connections\": 35,\n    \"total_dags\": 256,\n    \"total_tasks\": 1856,\n    \"historical\": {\n      \"interval\": \"hourly\",\n      \"periods\": 24,\n      \"cpu_usage\": [65.2, 67.8, 70.1, 68.5, 64.2, 62.8, 63.5, 67.8],\n      \"memory_usage\": [70.1, 71.2, 72.8, 72.3, 71.5, 71.2, 72.0, 72.3]\n    }\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#user-management",
    "href": "build-notes/api_design.html#user-management",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/users\nGet paginated list of all users\n\n\nGET\n/users/{user_id}\nGet details of a specific user\n\n\nPOST\n/users\nCreate a new user\n\n\nPUT\n/users/{user_id}\nUpdate user information\n\n\nDELETE\n/users/{user_id}\nDelete a user\n\n\nGET\n/users/{user_id}/permissions\nGet user permissions\n\n\nPUT\n/users/{user_id}/permissions\nUpdate user permissions\n\n\nGET\n/users/{user_id}/activity\nGet user activity log\n\n\n\n\n\n\n\n\nGET /users/usr_123456\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"usr_123456\",\n    \"username\": \"john.doe@example.com\",\n    \"name\": \"John Doe\",\n    \"role\": \"data_engineer\",\n    \"created_at\": \"2024-11-01T09:00:00Z\",\n    \"updated_at\": \"2025-02-15T14:30:00Z\",\n    \"last_login\": \"2025-03-04T09:15:23Z\",\n    \"status\": \"active\",\n    \"teams\": [\"data_team\", \"infrastructure\"],\n    \"permissions\": {\n      \"dags\": \"read_write\",\n      \"executions\": \"read_write\",\n      \"agents\": \"read\",\n      \"queues\": \"read\",\n      \"config\": \"none\",\n      \"secrets\": \"read\"\n    }\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#team-management",
    "href": "build-notes/api_design.html#team-management",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "Method\nEndpoint\nDescription\n\n\n\n\nGET\n/teams\nGet list of all teams\n\n\nGET\n/teams/{team_id}\nGet details of a specific team\n\n\nPOST\n/teams\nCreate a new team\n\n\nPUT\n/teams/{team_id}\nUpdate team information\n\n\nDELETE\n/teams/{team_id}\nDelete a team\n\n\nGET\n/teams/{team_id}/members\nGet team members\n\n\nPOST\n/teams/{team_id}/members\nAdd user to team\n\n\nDELETE\n/teams/{team_id}/members/{user_id}\nRemove user from team\n\n\nGET\n/teams/{team_id}/permissions\nGet team permissions\n\n\nPUT\n/teams/{team_id}/permissions\nUpdate team permissions\n\n\n\n\n\n\n\n\nGET /teams/team_data\nResponse:\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": \"team_data\",\n    \"name\": \"Data Team\",\n    \"description\": \"Data engineering and analytics team\",\n    \"created_at\": \"2024-10-01T00:00:00Z\",\n    \"updated_at\": \"2025-01-15T10:30:00Z\",\n    \"member_count\": 8,\n    \"owner_id\": \"usr_123456\",\n    \"permissions\": {\n      \"dags\": \"read_write\",\n      \"executions\": \"read_write\",\n      \"agents\": \"read\",\n      \"queues\": \"read\",\n      \"config\": \"none\",\n      \"secrets\": \"read_write\"\n    }\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#error-handling",
    "href": "build-notes/api_design.html#error-handling",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "All API endpoints follow a consistent error format:\n{\n  \"status\": \"error\",\n  \"error\": {\n    \"code\": \"invalid_request\",\n    \"message\": \"The request was invalid for the following reasons\",\n    \"details\": [\n      \"Field 'username' is required\",\n      \"Password must be at least 8 characters long\"\n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\nError Code\nHTTP Status\nDescription\n\n\n\n\nunauthorized\n401\nAuthentication required or invalid token\n\n\nforbidden\n403\nInsufficient permissions for the requested operation\n\n\nnot_found\n404\nThe requested resource was not found\n\n\ninvalid_request\n400\nInvalid request parameters or body\n\n\nvalidation_error\n400\nValidation failed for the provided data\n\n\nconflict\n409\nResource conflict (e.g., duplicate name)\n\n\ninternal_error\n500\nServer encountered an unexpected error\n\n\nservice_unavailable\n503\nService is temporarily unavailable"
  },
  {
    "objectID": "build-notes/api_design.html#pagination",
    "href": "build-notes/api_design.html#pagination",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "All list endpoints support pagination with the following query parameters:\n\npage: Page number (default: 1)\nlimit: Items per page (default: 20, max: 100)\nsort: Field to sort by\norder: Sort order (asc or desc)\n\nPagination metadata is included in the response:\n{\n  \"pagination\": {\n    \"page\": 2,\n    \"limit\": 20,\n    \"total\": 256,\n    \"pages\": 13\n  }\n}"
  },
  {
    "objectID": "build-notes/api_design.html#rate-limiting",
    "href": "build-notes/api_design.html#rate-limiting",
    "title": "Cyclonetix API Specification",
    "section": "",
    "text": "API requests are subject to rate limiting. The current limits are included in the HTTP headers of each response:\nX-RateLimit-Limit: 5000\nX-RateLimit-Remaining: 4995\nX-RateLimit-Reset: 1583247600\nWhen the rate limit is exceeded, a 429 Too Many Requests status is returned with a Retry-After header."
  },
  {
    "objectID": "build-notes/additional-design-thinking.html",
    "href": "build-notes/additional-design-thinking.html",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "This document captures additional design considerations for the Cyclonetix orchestration framework, focusing on practical enhancements that deliver maximum value with minimal implementation effort. These ideas prioritize flexibility, user experience, and integration with existing developer workflows rather than reinventing solved problems.\n\n\n\n\n\nFor users bringing their own tasks, data lineage can be captured through a standardized reporting mechanism:\n\nStandardized JSON Schema:\n{\n  \"task_id\": \"extract_customer_data\",\n  \"inputs\": [\n    {\"source\": \"postgres://main_db/customers\", \"type\": \"table\"},\n    {\"source\": \"s3://config-bucket/extraction_rules.json\", \"type\": \"file\"}\n  ],\n  \"outputs\": [\n    {\"destination\": \"s3://data-lake/customers/2025-03-04/\", \"type\": \"directory\"},\n    {\"destination\": \"redis://cache/customer_stats\", \"type\": \"cache\"}\n  ],\n  \"transformation\": \"extract_and_partition\",\n  \"metadata\": {\n    \"records_processed\": 12456,\n    \"bytes_processed\": 45678912\n  }\n}\nMultiple Reporting Methods:\n\nStdout markers: ##CYCLONETIX_LINEAGE_BEGIN## and ##CYCLONETIX_LINEAGE_END##\nLocal HTTP endpoint: POST to agent-provided endpoint\nEnvironment variables: Pre-populated task context information\n\nHelper Libraries:\nfrom cyclonetix import report_lineage\n\n# Simple reporting function\nreport_lineage(\n    inputs=[\"postgres://main_db/customers\"],\n    outputs=[\"s3://data-lake/customers/\"]\n)\n\n\n\n\n\nToggle between task view and data view\nLineage queries: “What’s the source of this data?” or “What consumes this data?”\nImpact analysis: “If I change this dataset, what downstream processes are affected?”\n\n\n\n\n\n\n\nProvide a simple API for tasks to save and retrieve checkpoint information:\nfrom cyclonetix import checkpoints\n\n# Save current progress\ncheckpoints.save({\n    \"processed_items\": 500,\n    \"last_processed_id\": \"item_123\",\n    \"calculation_state\": {\"sum\": 12345, \"count\": 500}\n})\n\n# Later, retrieve checkpoint\ncheckpoint = checkpoints.get()\nif checkpoint:\n    # Resume processing\n    start_from_id = checkpoint[\"last_processed_id\"]\n\n\n\n\nVersioned checkpoints: Store multiple checkpoints with labels\nTTL options: Automatically expire old checkpoints\nSize limits: Prevent excessive storage use\nEncryption: Securely store sensitive checkpoint data\n\n\n\n\n\nRestart options: “From beginning” vs “From checkpoint”\nCheckpoint browser: View available checkpoints and their metadata\nManual checkpoint creation: For administrative intervention\n\n\n\n\n\n\n\nProvide context menu options in the UI:\n\nSkip this task: Mark as successful without running\nSkip all downstream: Skip this task and all dependent tasks\nRun only this branch: Execute just this task and its dependencies\nRun from here: Skip upstream tasks (if outputs available)\n\n\n\n\n\nPin task version: Lock a specific task implementation while iterating on others\nPin task output: Use cached output even when re-running\n\n\n\n\n\nPause execution: Temporarily halt a running DAG\nResource throttling: Limit resource usage during execution\nPriority adjustment: Change priority mid-execution\n\n\n\n\n\n\n\n\nContextual enrichment: Automatically add DAG/task context to all logs\nStructured logging: Encourage JSON-formatted logs for better parsing\nLog levels: Support standard levels (DEBUG, INFO, WARN, ERROR)\n\n\n\n\n\nExecution trace IDs: Automatically generate and propagate trace IDs\nCross-service tracing: Helpers to propagate IDs across API calls\n\nExample library usage:\nfrom cyclonetix import logging\n\n# Logs automatically include execution context\nlogging.info(\"Processing batch\", extra={\"batch_size\": 1000})\n\n# Create a correlation context\nwith logging.correlation_context(operation=\"data_import\"):\n    # All logs within this block share correlation ID\n    result = process_data()\n    \n    # HTTP requests automatically propagate the correlation ID\n    response = requests.get(\"https://api.example.com/data\")\n\n\n\n\nStructured log explorer: Filter, search, and visualize logs\nPattern detection: Highlight anomalies and common error patterns\nContext-aware grouping: Group related logs across components\n\n\n\n\n\n\n\n\nAlert builder: Visual interface for alert conditions\nMulti-channel delivery: Email, Slack, webhook, PagerDuty, etc.\nAlert templates: Pre-configured alert patterns\n\n\n\n\n\nAction buttons: Add custom actions to alert notifications\nRunbook links: Connect alerts to documentation\nResolution tracking: Record alert resolution steps\n\n\n\n\n\nAlert frequency tracking: Identify noisy alerts\nMTTR analysis: Measure time to resolve different alert types\nAlert correlation: Group related alerts for easier triage\n\n\n\n\n\n\n\nImplement simple but effective versioning:\n\nAuto-versioning: Increment version on DAG changes\nVersion tagging: Allow custom tags (e.g., “prod-release-1.2”)\nCurrent flag: Mark one version as current/active\n\n\n\n\n\nVersion history: Show execution stats for each version\nDiff view: Highlight changes between versions\nRollback option: Restore previous version as current\n\n\n\n\n\nRun specific version: Execute non-current versions\nA/B testing: Run multiple versions simultaneously\nGradual rollout: Transition traffic between versions\n\n\n\n\n\n\n\n\nTemplate library: Searchable repository of task templates\nTemplate parameters: Configurable parameters for customization\nCategories and tags: Organize templates by function/domain\n\n\n\n\n\nTeam templates: Shared within specific teams\nPermission model: Control who can use/edit templates\nTemplate export/import: Share across environments\n\n\n\n\n\nCookbook examples: Link templates to documentation\nUsage statistics: Show which templates are most used\nVersion tracking: Notify users of template updates\n\n\n\n\n\n\n\n\nDAG import: Reference existing DAGs as components\nInput/output mapping: Connect parent/child data flows\nFailure handling: Configure propagation of failures\n\n\n\n\n\nCollapsible sub-workflows: Expand/collapse in the DAG view\nCross-linking: Navigate between parent/child DAGs\nStatistics rollup: Aggregate metrics across levels\n\n\n\n\n\nParameter passing: Pass context from parent to child\nConditional execution: Run sub-workflows based on conditions\nLibrary of common sub-workflows: E.g., approval, notification, retry patterns\n\n\n\n\n\n\n\n\nTime-based metrics: Maximum duration, deadlines, time windows\nSuccess criteria: Completion rate, error budget\nRecovery parameters: MTTR requirements, retry policy\n\nExample SLA configuration:\n{\n  \"name\": \"Daily Analytics SLA\",\n  \"time_window\": {\n    \"start\": \"01:00\",\n    \"end\": \"03:00\",\n    \"timezone\": \"UTC\"\n  },\n  \"max_duration_minutes\": 90,\n  \"critical_path_tasks\": {\n    \"data_load\": {\n      \"max_duration_minutes\": 30\n    },\n    \"report_generation\": {\n      \"max_duration_minutes\": 20\n    }\n  },\n  \"success_criteria\": {\n    \"completion_rate_percent\": 99.5,\n    \"error_budget_monthly_failures\": 1\n  },\n  \"recovery\": {\n    \"max_mttr_minutes\": 60,\n    \"auto_retry_count\": 2,\n    \"auto_retry_delay_seconds\": 300\n  }\n}\n\n\n\n\nReal-time compliance indicators: Green/yellow/red status\nHistorical compliance charts: Track SLA performance over time\nTrending analysis: Identify degrading performance before violations\n\n\n\n\n\nProgressive alerting: Warning at 75% of threshold, critical at 90%\nProjected violation alerts: “At current pace, SLA will be violated in 15 minutes”\nBusiness impact classification: Tag alerts with business criticality\n\n\n\n\n\n\n\n\nTeam ownership: Flag DAGs and tasks with team ownership\nTeam dashboards: Filtered views of team assets\nPermission inheritance: Role-based access control at team level\n\n\n\n\n\nComments & annotations: Add notes to DAGs and tasks\nSharing controls: Share assets with individuals or teams\nActivity feed: See recent changes to team assets\n\n\n\n\n\nDocumentation integration: Link to wiki or documentation\nRunbook creation: Create and manage operational procedures\nHistorical context: Record decisions and changes\n\n\n\n\n\n\n\n\nAlert notifications: Receive and respond to alerts\nExecution monitoring: Track critical DAG executions\nQuick actions: Approve, retry, skip, or fail tasks\nStatus dashboard: See overall system health\n\n\n\n\n\nResponsive core views: Essential views adapt to mobile\nNative notifications: Integration with mobile notification systems\nOffline capabilities: View critical information offline\n\n\n\n\n\n\n\nImplement tiered data retention policy:\n\nDetailed task logs: 7 days\nTask execution metrics: 30 days\nDAG execution metrics: 90 days\nAggregated performance data: 1 year\n\n\n\n\n\nLazy loading: Load DAG details on demand\nData summarization: Pre-aggregate metrics for dashboard views\nPagination controls: Manage large result sets efficiently\n\n\n\n\n\nHorizontal scaling: Design components to scale independently\nResource quotas: Prevent resource monopolization\nDecoupled architecture: Minimize dependencies between components\n\n\n\n\n\n\n\n\nConsistent resource tagging: cyclonetix:execution-id, cyclonetix:dag-id, cyclonetix:task-id\nTag propagation: Helper functions to apply tags in user tasks\nDocumentation: Guide for effective tagging\n\n\n\n\n\nCloud provider APIs: Pull cost data from existing billing APIs\nThird-party integrations: Support popular cost management tools\nApproximate attribution: Acknowledge limitations in shared resource attribution\n\n\n\n\nFor long-running K8s containers used by multiple tasks:\n\nTime-based allocation: Divide costs by execution time\nResource-weighted allocation: Factor in CPU/memory utilization\nIdle cost handling: Track and allocate container idle time\n\n\n\n\n\nCost summary widget: Quick view of execution costs\nCost breakdown: By task, resource type, team\nHistorical trending: Track cost patterns over time\n\n\n\n\n\n\n\n\nScreen reader support: Proper ARIA attributes and labels\nKeyboard navigation: Comprehensive keyboard shortcuts\nColor contrast: Meet WCAG AA standards at minimum\nText scaling: Support browser text size adjustments\n\n\n\n\n\nDark/light mode: Based on Quasar’s theming support\nContextual help: Inline documentation and tooltips\nPersonalization: User-specific UI customization\nProgressive disclosure: Show complexity progressively\n\n\n\n\n\n\n\n\nBusiness KPI mapping: Connect technical workflows to business metrics\nImpact classification: Tag DAGs with business criticality\nService dependency mapping: Show which business services depend on which DAGs\n\n\n\n\n\nBusiness-oriented SLA dashboard: Show compliance by business area\nCustomer impact tracking: Estimate affected customers during incidents\nExecutive summary reports: Business-friendly performance overview\n\n\n\n\n\nThe design recommendations in this document follow the principle of achieving 80% of the value with 20% of the effort, through:\n\nLeverage existing solutions rather than rebuilding\nStart simple, allow growth for complex features\nStandardize where possible, but be flexible where needed\nFocus on integration over reinvention\nPrioritize user experience over technical elegance\n\nBy focusing on these principles, Cyclonetix can deliver a highly competitive orchestration framework that addresses real-world needs without overinvesting in esoteric features or reinventing solved problems."
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#introduction",
    "href": "build-notes/additional-design-thinking.html#introduction",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "This document captures additional design considerations for the Cyclonetix orchestration framework, focusing on practical enhancements that deliver maximum value with minimal implementation effort. These ideas prioritize flexibility, user experience, and integration with existing developer workflows rather than reinventing solved problems."
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#pragmatic-data-lineage",
    "href": "build-notes/additional-design-thinking.html#pragmatic-data-lineage",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "For users bringing their own tasks, data lineage can be captured through a standardized reporting mechanism:\n\nStandardized JSON Schema:\n{\n  \"task_id\": \"extract_customer_data\",\n  \"inputs\": [\n    {\"source\": \"postgres://main_db/customers\", \"type\": \"table\"},\n    {\"source\": \"s3://config-bucket/extraction_rules.json\", \"type\": \"file\"}\n  ],\n  \"outputs\": [\n    {\"destination\": \"s3://data-lake/customers/2025-03-04/\", \"type\": \"directory\"},\n    {\"destination\": \"redis://cache/customer_stats\", \"type\": \"cache\"}\n  ],\n  \"transformation\": \"extract_and_partition\",\n  \"metadata\": {\n    \"records_processed\": 12456,\n    \"bytes_processed\": 45678912\n  }\n}\nMultiple Reporting Methods:\n\nStdout markers: ##CYCLONETIX_LINEAGE_BEGIN## and ##CYCLONETIX_LINEAGE_END##\nLocal HTTP endpoint: POST to agent-provided endpoint\nEnvironment variables: Pre-populated task context information\n\nHelper Libraries:\nfrom cyclonetix import report_lineage\n\n# Simple reporting function\nreport_lineage(\n    inputs=[\"postgres://main_db/customers\"],\n    outputs=[\"s3://data-lake/customers/\"]\n)\n\n\n\n\n\nToggle between task view and data view\nLineage queries: “What’s the source of this data?” or “What consumes this data?”\nImpact analysis: “If I change this dataset, what downstream processes are affected?”"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#task-recovery-checkpoints",
    "href": "build-notes/additional-design-thinking.html#task-recovery-checkpoints",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Provide a simple API for tasks to save and retrieve checkpoint information:\nfrom cyclonetix import checkpoints\n\n# Save current progress\ncheckpoints.save({\n    \"processed_items\": 500,\n    \"last_processed_id\": \"item_123\",\n    \"calculation_state\": {\"sum\": 12345, \"count\": 500}\n})\n\n# Later, retrieve checkpoint\ncheckpoint = checkpoints.get()\nif checkpoint:\n    # Resume processing\n    start_from_id = checkpoint[\"last_processed_id\"]\n\n\n\n\nVersioned checkpoints: Store multiple checkpoints with labels\nTTL options: Automatically expire old checkpoints\nSize limits: Prevent excessive storage use\nEncryption: Securely store sensitive checkpoint data\n\n\n\n\n\nRestart options: “From beginning” vs “From checkpoint”\nCheckpoint browser: View available checkpoints and their metadata\nManual checkpoint creation: For administrative intervention"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#enhanced-dag-control",
    "href": "build-notes/additional-design-thinking.html#enhanced-dag-control",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Provide context menu options in the UI:\n\nSkip this task: Mark as successful without running\nSkip all downstream: Skip this task and all dependent tasks\nRun only this branch: Execute just this task and its dependencies\nRun from here: Skip upstream tasks (if outputs available)\n\n\n\n\n\nPin task version: Lock a specific task implementation while iterating on others\nPin task output: Use cached output even when re-running\n\n\n\n\n\nPause execution: Temporarily halt a running DAG\nResource throttling: Limit resource usage during execution\nPriority adjustment: Change priority mid-execution"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#smart-logging",
    "href": "build-notes/additional-design-thinking.html#smart-logging",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Contextual enrichment: Automatically add DAG/task context to all logs\nStructured logging: Encourage JSON-formatted logs for better parsing\nLog levels: Support standard levels (DEBUG, INFO, WARN, ERROR)\n\n\n\n\n\nExecution trace IDs: Automatically generate and propagate trace IDs\nCross-service tracing: Helpers to propagate IDs across API calls\n\nExample library usage:\nfrom cyclonetix import logging\n\n# Logs automatically include execution context\nlogging.info(\"Processing batch\", extra={\"batch_size\": 1000})\n\n# Create a correlation context\nwith logging.correlation_context(operation=\"data_import\"):\n    # All logs within this block share correlation ID\n    result = process_data()\n    \n    # HTTP requests automatically propagate the correlation ID\n    response = requests.get(\"https://api.example.com/data\")\n\n\n\n\nStructured log explorer: Filter, search, and visualize logs\nPattern detection: Highlight anomalies and common error patterns\nContext-aware grouping: Group related logs across components"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#practical-alerting",
    "href": "build-notes/additional-design-thinking.html#practical-alerting",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Alert builder: Visual interface for alert conditions\nMulti-channel delivery: Email, Slack, webhook, PagerDuty, etc.\nAlert templates: Pre-configured alert patterns\n\n\n\n\n\nAction buttons: Add custom actions to alert notifications\nRunbook links: Connect alerts to documentation\nResolution tracking: Record alert resolution steps\n\n\n\n\n\nAlert frequency tracking: Identify noisy alerts\nMTTR analysis: Measure time to resolve different alert types\nAlert correlation: Group related alerts for easier triage"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#versioned-dags",
    "href": "build-notes/additional-design-thinking.html#versioned-dags",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Implement simple but effective versioning:\n\nAuto-versioning: Increment version on DAG changes\nVersion tagging: Allow custom tags (e.g., “prod-release-1.2”)\nCurrent flag: Mark one version as current/active\n\n\n\n\n\nVersion history: Show execution stats for each version\nDiff view: Highlight changes between versions\nRollback option: Restore previous version as current\n\n\n\n\n\nRun specific version: Execute non-current versions\nA/B testing: Run multiple versions simultaneously\nGradual rollout: Transition traffic between versions"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#task-templates-libraries",
    "href": "build-notes/additional-design-thinking.html#task-templates-libraries",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Template library: Searchable repository of task templates\nTemplate parameters: Configurable parameters for customization\nCategories and tags: Organize templates by function/domain\n\n\n\n\n\nTeam templates: Shared within specific teams\nPermission model: Control who can use/edit templates\nTemplate export/import: Share across environments\n\n\n\n\n\nCookbook examples: Link templates to documentation\nUsage statistics: Show which templates are most used\nVersion tracking: Notify users of template updates"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#sub-workflows-composition",
    "href": "build-notes/additional-design-thinking.html#sub-workflows-composition",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "DAG import: Reference existing DAGs as components\nInput/output mapping: Connect parent/child data flows\nFailure handling: Configure propagation of failures\n\n\n\n\n\nCollapsible sub-workflows: Expand/collapse in the DAG view\nCross-linking: Navigate between parent/child DAGs\nStatistics rollup: Aggregate metrics across levels\n\n\n\n\n\nParameter passing: Pass context from parent to child\nConditional execution: Run sub-workflows based on conditions\nLibrary of common sub-workflows: E.g., approval, notification, retry patterns"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#sla-management",
    "href": "build-notes/additional-design-thinking.html#sla-management",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Time-based metrics: Maximum duration, deadlines, time windows\nSuccess criteria: Completion rate, error budget\nRecovery parameters: MTTR requirements, retry policy\n\nExample SLA configuration:\n{\n  \"name\": \"Daily Analytics SLA\",\n  \"time_window\": {\n    \"start\": \"01:00\",\n    \"end\": \"03:00\",\n    \"timezone\": \"UTC\"\n  },\n  \"max_duration_minutes\": 90,\n  \"critical_path_tasks\": {\n    \"data_load\": {\n      \"max_duration_minutes\": 30\n    },\n    \"report_generation\": {\n      \"max_duration_minutes\": 20\n    }\n  },\n  \"success_criteria\": {\n    \"completion_rate_percent\": 99.5,\n    \"error_budget_monthly_failures\": 1\n  },\n  \"recovery\": {\n    \"max_mttr_minutes\": 60,\n    \"auto_retry_count\": 2,\n    \"auto_retry_delay_seconds\": 300\n  }\n}\n\n\n\n\nReal-time compliance indicators: Green/yellow/red status\nHistorical compliance charts: Track SLA performance over time\nTrending analysis: Identify degrading performance before violations\n\n\n\n\n\nProgressive alerting: Warning at 75% of threshold, critical at 90%\nProjected violation alerts: “At current pace, SLA will be violated in 15 minutes”\nBusiness impact classification: Tag alerts with business criticality"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#team-collaboration-features",
    "href": "build-notes/additional-design-thinking.html#team-collaboration-features",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Team ownership: Flag DAGs and tasks with team ownership\nTeam dashboards: Filtered views of team assets\nPermission inheritance: Role-based access control at team level\n\n\n\n\n\nComments & annotations: Add notes to DAGs and tasks\nSharing controls: Share assets with individuals or teams\nActivity feed: See recent changes to team assets\n\n\n\n\n\nDocumentation integration: Link to wiki or documentation\nRunbook creation: Create and manage operational procedures\nHistorical context: Record decisions and changes"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#mobile-experience",
    "href": "build-notes/additional-design-thinking.html#mobile-experience",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Alert notifications: Receive and respond to alerts\nExecution monitoring: Track critical DAG executions\nQuick actions: Approve, retry, skip, or fail tasks\nStatus dashboard: See overall system health\n\n\n\n\n\nResponsive core views: Essential views adapt to mobile\nNative notifications: Integration with mobile notification systems\nOffline capabilities: View critical information offline"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#performance-scalability",
    "href": "build-notes/additional-design-thinking.html#performance-scalability",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Implement tiered data retention policy:\n\nDetailed task logs: 7 days\nTask execution metrics: 30 days\nDAG execution metrics: 90 days\nAggregated performance data: 1 year\n\n\n\n\n\nLazy loading: Load DAG details on demand\nData summarization: Pre-aggregate metrics for dashboard views\nPagination controls: Manage large result sets efficiently\n\n\n\n\n\nHorizontal scaling: Design components to scale independently\nResource quotas: Prevent resource monopolization\nDecoupled architecture: Minimize dependencies between components"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#practical-cloud-cost-tracking",
    "href": "build-notes/additional-design-thinking.html#practical-cloud-cost-tracking",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Consistent resource tagging: cyclonetix:execution-id, cyclonetix:dag-id, cyclonetix:task-id\nTag propagation: Helper functions to apply tags in user tasks\nDocumentation: Guide for effective tagging\n\n\n\n\n\nCloud provider APIs: Pull cost data from existing billing APIs\nThird-party integrations: Support popular cost management tools\nApproximate attribution: Acknowledge limitations in shared resource attribution\n\n\n\n\nFor long-running K8s containers used by multiple tasks:\n\nTime-based allocation: Divide costs by execution time\nResource-weighted allocation: Factor in CPU/memory utilization\nIdle cost handling: Track and allocate container idle time\n\n\n\n\n\nCost summary widget: Quick view of execution costs\nCost breakdown: By task, resource type, team\nHistorical trending: Track cost patterns over time"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#accessibility-user-experience",
    "href": "build-notes/additional-design-thinking.html#accessibility-user-experience",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Screen reader support: Proper ARIA attributes and labels\nKeyboard navigation: Comprehensive keyboard shortcuts\nColor contrast: Meet WCAG AA standards at minimum\nText scaling: Support browser text size adjustments\n\n\n\n\n\nDark/light mode: Based on Quasar’s theming support\nContextual help: Inline documentation and tooltips\nPersonalization: User-specific UI customization\nProgressive disclosure: Show complexity progressively"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#business-metrics-connection",
    "href": "build-notes/additional-design-thinking.html#business-metrics-connection",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "Business KPI mapping: Connect technical workflows to business metrics\nImpact classification: Tag DAGs with business criticality\nService dependency mapping: Show which business services depend on which DAGs\n\n\n\n\n\nBusiness-oriented SLA dashboard: Show compliance by business area\nCustomer impact tracking: Estimate affected customers during incidents\nExecutive summary reports: Business-friendly performance overview"
  },
  {
    "objectID": "build-notes/additional-design-thinking.html#implementation-philosophy-the-8020-approach",
    "href": "build-notes/additional-design-thinking.html#implementation-philosophy-the-8020-approach",
    "title": "Cyclonetix: Additional Design Thinking",
    "section": "",
    "text": "The design recommendations in this document follow the principle of achieving 80% of the value with 20% of the effort, through:\n\nLeverage existing solutions rather than rebuilding\nStart simple, allow growth for complex features\nStandardize where possible, but be flexible where needed\nFocus on integration over reinvention\nPrioritize user experience over technical elegance\n\nBy focusing on these principles, Cyclonetix can deliver a highly competitive orchestration framework that addresses real-world needs without overinvesting in esoteric features or reinventing solved problems."
  },
  {
    "objectID": "roadmap.html",
    "href": "roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "This roadmap outlines the planned development for Cyclonetix, from immediate priorities to long-term vision. Our goal is to create a workflow orchestrator that excels in ease of use, performance, and flexibility.\n\n\nCyclonetix is currently in active development with a functioning core that includes:\n\nBasic task execution and dependency management\nSelf-assembling DAGs via outcome-based scheduling\nRedis-based state management\nWeb UI for monitoring and management\nAgent-based task execution\n\n\n\n\nOur immediate focus is on stabilizing the core functionality and improving user experience:\n\n\n\nFinalize task recovery logic: Enhance the mechanism for recovering from agent failures and retrying failed tasks\nImprove orchestrator state handling: Refine how the orchestrator manages and persists state across restarts\nComplete Redis-based backend: Ensure all features work correctly with the Redis backend\nDevelop PostgreSQL-based StateManager: Add support for PostgreSQL as an alternative backend for high-scale deployments\n\n\n\n\n\nEnhance the first version of the UI: Improve the Axum SSR + Tabler.js + Cytoscape.js based user interface\nAdd search and filtering in the UI: Make it easier to find specific tasks and DAGs\nImprove DAG visualization: Enhance the graph visualization with more interactive features and details\n\n\n\n\n\nExpand documentation: Create comprehensive guides for all features\nAdd examples: Provide example workflows for common use cases\nImprove API documentation: Document all public APIs with detailed explanations\n\n\n\n\n\n\n\n\nEvent-driven execution triggers: Support for Kafka, webhooks, and API calls to trigger workflows\nREST API for external integration: Expose a comprehensive REST API for integrating with other systems\nKubernetes auto-scaling: Refine the logic for scaling agents based on queue depth\nGit-based execution: Enhance support for executing code directly from Git repositories\n\n\n\n\n\nParameter sets: Formalize the concept of parameter sets for task configuration\nContext inheritance: Implement hierarchical contexts with overrides\nImproved logging: Enhanced logging and tracing for better debugging\nDashboard enhancements: Add more metrics and visualizations to the dashboard\n\n\n\n\n\nOptimized graph traversal: Improve the performance of dependency resolution\nReduced Redis operations: Minimize the number of Redis operations for better scalability\nTask batching: Support for batching small tasks for more efficient execution\n\n\n\n\n\n\n\n\nWorkflow versioning: Track changes to workflows over time\nA/B testing support: Run multiple variants of workflows for comparison\nConditional branching: Enhanced support for complex conditional logic\nTime-based scheduling: Cron-like scheduling capabilities\nResource management: Better control over resource allocation for tasks\nTask timeouts and error handling: More sophisticated error handling and timeout mechanisms\n\n\n\n\n\nRole-based access control (RBAC): Granular permissions for different users\nAudit logging: Track all actions for compliance and security\nSecret management: Secure handling of credentials and sensitive information\nEnhanced OAuth integration: Support for more OAuth providers\n\n\n\n\n\nMetrics collection: Gather performance metrics for tasks and workflows\nPrometheus integration: Export metrics to Prometheus\nDetailed execution history: Improved tracking of past executions\nAdvanced filtering and search: Better tools for finding specific executions\n\n\n\n\n\n\n\n\nCloud deployment automation: Terraform/Pulumi integration for easy deployment\nMulti-cloud support: Run workflows across different cloud providers\nServerless execution: Support for serverless task execution\nCost optimization: Intelligent scheduling to minimize cloud costs\n\n\n\n\n\nJupyter notebook execution as DAGs: Convert annotated notebooks into production-ready workflows\nWASM execution support: Ultra-lightweight task execution using WebAssembly\nAI-driven optimization: Use AI to optimize workflow scheduling and resource allocation\nCross-organization workflows: Support for workflows that span multiple organizations\n\n\n\n\n\nMulti-tenancy: Support for multiple isolated tenants sharing the same infrastructure\nHigh availability: Enhanced fault tolerance and failover capabilities\nDisaster recovery: Automated backup and recovery procedures\nCompliance features: Support for regulatory compliance requirements\n\n\n\n\n\nLive execution tracking with WebSockets: Real-time updates on execution state\nAdvanced visualization: More sophisticated visualization of complex workflows\nMobile support: Responsive design for mobile monitoring\nCustomizable dashboards: User-definable dashboards and views\n\n\n\n\n\n\nPlugin system: Support for community-developed plugins\nIntegration marketplace: Pre-built integrations with popular tools and services\nExpanded documentation: Comprehensive guides, tutorials, and examples\nCommunity forums: Dedicated space for user discussions and support\n\n\n\n\nWe welcome contributions in the following areas:\n\nBackend implementations: Additional state manager backends (e.g., MongoDB, DynamoDB)\nUI improvements: Enhanced visualizations and usability features\nDocumentation: Tutorials, examples, and clarifications\nTesting: Additional test cases and improved test coverage\nPerformance optimizations: Identifying and addressing bottlenecks\n\n\n\n\nThis roadmap is not set in stone. We actively adjust our priorities based on:\n\nUser feedback: What problems are users trying to solve?\nCommunity contributions: Where is the community showing interest?\nIndustry trends: How is the workflow orchestration landscape evolving?\n\nIf you have suggestions or want to contribute to a specific area, please:\n\nOpen an issue: Describe your idea or problem\nStart a discussion: Share your thoughts on the direction of the project\nSubmit a PR: Contribute code or documentation improvements\n\n\n\n\nThe roadmap will be updated quarterly to reflect evolving priorities and progress. Check back regularly for the latest information on Cyclonetix’s development direction.\n\n\n\n\nExplore the Developer Guide to understand the code\nCheck the GitHub repository for current issues and PRs\nJoin the discussion in the Discussions section",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#current-status",
    "href": "roadmap.html#current-status",
    "title": "Roadmap",
    "section": "",
    "text": "Cyclonetix is currently in active development with a functioning core that includes:\n\nBasic task execution and dependency management\nSelf-assembling DAGs via outcome-based scheduling\nRedis-based state management\nWeb UI for monitoring and management\nAgent-based task execution",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#immediate-priorities-next-4-weeks",
    "href": "roadmap.html#immediate-priorities-next-4-weeks",
    "title": "Roadmap",
    "section": "",
    "text": "Our immediate focus is on stabilizing the core functionality and improving user experience:\n\n\n\nFinalize task recovery logic: Enhance the mechanism for recovering from agent failures and retrying failed tasks\nImprove orchestrator state handling: Refine how the orchestrator manages and persists state across restarts\nComplete Redis-based backend: Ensure all features work correctly with the Redis backend\nDevelop PostgreSQL-based StateManager: Add support for PostgreSQL as an alternative backend for high-scale deployments\n\n\n\n\n\nEnhance the first version of the UI: Improve the Axum SSR + Tabler.js + Cytoscape.js based user interface\nAdd search and filtering in the UI: Make it easier to find specific tasks and DAGs\nImprove DAG visualization: Enhance the graph visualization with more interactive features and details\n\n\n\n\n\nExpand documentation: Create comprehensive guides for all features\nAdd examples: Provide example workflows for common use cases\nImprove API documentation: Document all public APIs with detailed explanations",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#short-term-goals-next-2-3-months",
    "href": "roadmap.html#short-term-goals-next-2-3-months",
    "title": "Roadmap",
    "section": "",
    "text": "Event-driven execution triggers: Support for Kafka, webhooks, and API calls to trigger workflows\nREST API for external integration: Expose a comprehensive REST API for integrating with other systems\nKubernetes auto-scaling: Refine the logic for scaling agents based on queue depth\nGit-based execution: Enhance support for executing code directly from Git repositories\n\n\n\n\n\nParameter sets: Formalize the concept of parameter sets for task configuration\nContext inheritance: Implement hierarchical contexts with overrides\nImproved logging: Enhanced logging and tracing for better debugging\nDashboard enhancements: Add more metrics and visualizations to the dashboard\n\n\n\n\n\nOptimized graph traversal: Improve the performance of dependency resolution\nReduced Redis operations: Minimize the number of Redis operations for better scalability\nTask batching: Support for batching small tasks for more efficient execution",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#medium-term-goals-3-6-months",
    "href": "roadmap.html#medium-term-goals-3-6-months",
    "title": "Roadmap",
    "section": "",
    "text": "Workflow versioning: Track changes to workflows over time\nA/B testing support: Run multiple variants of workflows for comparison\nConditional branching: Enhanced support for complex conditional logic\nTime-based scheduling: Cron-like scheduling capabilities\nResource management: Better control over resource allocation for tasks\nTask timeouts and error handling: More sophisticated error handling and timeout mechanisms\n\n\n\n\n\nRole-based access control (RBAC): Granular permissions for different users\nAudit logging: Track all actions for compliance and security\nSecret management: Secure handling of credentials and sensitive information\nEnhanced OAuth integration: Support for more OAuth providers\n\n\n\n\n\nMetrics collection: Gather performance metrics for tasks and workflows\nPrometheus integration: Export metrics to Prometheus\nDetailed execution history: Improved tracking of past executions\nAdvanced filtering and search: Better tools for finding specific executions",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#long-term-vision-6-months",
    "href": "roadmap.html#long-term-vision-6-months",
    "title": "Roadmap",
    "section": "",
    "text": "Cloud deployment automation: Terraform/Pulumi integration for easy deployment\nMulti-cloud support: Run workflows across different cloud providers\nServerless execution: Support for serverless task execution\nCost optimization: Intelligent scheduling to minimize cloud costs\n\n\n\n\n\nJupyter notebook execution as DAGs: Convert annotated notebooks into production-ready workflows\nWASM execution support: Ultra-lightweight task execution using WebAssembly\nAI-driven optimization: Use AI to optimize workflow scheduling and resource allocation\nCross-organization workflows: Support for workflows that span multiple organizations\n\n\n\n\n\nMulti-tenancy: Support for multiple isolated tenants sharing the same infrastructure\nHigh availability: Enhanced fault tolerance and failover capabilities\nDisaster recovery: Automated backup and recovery procedures\nCompliance features: Support for regulatory compliance requirements\n\n\n\n\n\nLive execution tracking with WebSockets: Real-time updates on execution state\nAdvanced visualization: More sophisticated visualization of complex workflows\nMobile support: Responsive design for mobile monitoring\nCustomizable dashboards: User-definable dashboards and views",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#community-and-ecosystem",
    "href": "roadmap.html#community-and-ecosystem",
    "title": "Roadmap",
    "section": "",
    "text": "Plugin system: Support for community-developed plugins\nIntegration marketplace: Pre-built integrations with popular tools and services\nExpanded documentation: Comprehensive guides, tutorials, and examples\nCommunity forums: Dedicated space for user discussions and support",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#contribution-opportunities",
    "href": "roadmap.html#contribution-opportunities",
    "title": "Roadmap",
    "section": "",
    "text": "We welcome contributions in the following areas:\n\nBackend implementations: Additional state manager backends (e.g., MongoDB, DynamoDB)\nUI improvements: Enhanced visualizations and usability features\nDocumentation: Tutorials, examples, and clarifications\nTesting: Additional test cases and improved test coverage\nPerformance optimizations: Identifying and addressing bottlenecks",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#feedback-and-prioritization",
    "href": "roadmap.html#feedback-and-prioritization",
    "title": "Roadmap",
    "section": "",
    "text": "This roadmap is not set in stone. We actively adjust our priorities based on:\n\nUser feedback: What problems are users trying to solve?\nCommunity contributions: Where is the community showing interest?\nIndustry trends: How is the workflow orchestration landscape evolving?\n\nIf you have suggestions or want to contribute to a specific area, please:\n\nOpen an issue: Describe your idea or problem\nStart a discussion: Share your thoughts on the direction of the project\nSubmit a PR: Contribute code or documentation improvements",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#keeping-up-to-date",
    "href": "roadmap.html#keeping-up-to-date",
    "title": "Roadmap",
    "section": "",
    "text": "The roadmap will be updated quarterly to reflect evolving priorities and progress. Check back regularly for the latest information on Cyclonetix’s development direction.",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#next-steps",
    "href": "roadmap.html#next-steps",
    "title": "Roadmap",
    "section": "",
    "text": "Explore the Developer Guide to understand the code\nCheck the GitHub repository for current issues and PRs\nJoin the discussion in the Discussions section",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "developer-guide.html",
    "href": "developer-guide.html",
    "title": "Developer Guide",
    "section": "",
    "text": "This guide is intended for developers who want to contribute to Cyclonetix or understand its internal architecture for extending its functionality.\n\n\nThe Cyclonetix codebase is organized as follows:\ncyclonetix/\n├── src/\n│   ├── agent/            # Agent implementation\n│   ├── graph/            # Execution graph and orchestrator\n│   ├── models/           # Core data models\n│   ├── server/           # Web UI and API\n│   ├── state/            # State management\n│   └── utils/            # Utility functions\n├── static/               # Static web assets\n├── templates/            # Tera templates for UI\n├── docs/                 # Documentation\n└── data/                 # Example data files\n\n\n\nCyclonetix follows a modular architecture with clear separation of concerns:\n\nCore Models (src/models/): Define the data structures used throughout the system.\nState Management (src/state/): Abstracts storage backends and provides a unified interface.\nAgent (src/agent/): Handles task execution and reporting.\nGraph (src/graph/): Manages task dependencies and execution flow.\nServer (src/server/): Provides the web UI and API.\n\n\n\n\n\n\nThe StateManager trait (src/state/state_manager.rs) is the central abstraction for state persistence:\n#[async_trait]\npub trait StateManager: Sync + Send {\n    // Queue operations\n    async fn get_work_from_queue(&self, queue: &str) -&gt; Option&lt;TaskPayload&gt;;\n    async fn put_work_on_queue(&self, task_payload: &TaskPayload, queue: &str);\n\n    // Task operations\n    async fn save_task(&self, task: &TaskTemplate);\n    async fn load_task(&self, task_id: &str) -&gt; Option&lt;TaskTemplate&gt;;\n\n    // DAG operations\n    async fn save_dag_instance(&self, dag_instance: &DagInstance);\n    async fn load_dag_instance(&self, run_id: &str) -&gt; Option&lt;DagInstance&gt;;\n\n    // many other methods...\n}\nThis trait is implemented for different backends like Redis and in-memory storage.\n\n\n\nThe ExecutionGraph struct (src/graph/graph_manager.rs) manages task dependencies and execution order:\npub struct ExecutionGraph {\n    pub graph: DiGraph&lt;String, ()&gt;,\n    pub node_map: HashMap&lt;String, NodeIndex&gt;,\n}\nIt provides methods for building and traversing the execution graph.\n\n\n\nThe Agent struct (src/agent/agent.rs) is responsible for executing tasks:\npub struct Agent&lt;S: 'static + StateManager + ?Sized&gt; {\n    state_manager: Arc&lt;S&gt;,\n    agent_id: String,\n}\n\n\n\n\n\n\n\nRust 1.84+ with Cargo\nRedis (optional, for testing with Redis backend)\nGit\n\n\n\n\n\nClone the repository:\n\ngit clone https://github.com/neural-chilli/Cyclonetix.git\ncd Cyclonetix\n\nBuild the project:\n\ncargo build\n\nRun tests:\n\ncargo test\n\nRun with development features:\n\nDEV_MODE=true cargo run\n\n\n\nThe UI is built with:\n\nTera for templating\nTabler for UI components\nCytoscape.js for graph visualization\n\nWhen DEV_MODE=true, templates are reloaded on each request, making UI development faster.\n\n\n\n\n\n\nCheck the GitHub issues labeled with “good first issue” or “help wanted”.\n\n\n\n\nCreate a branch:\ngit checkout -b feature/your-feature-name\nImplement your changes:\n\nKeep each commit focused on a single change\nFollow the existing code style\nAdd appropriate tests\n\nUpdate documentation:\n\nUpdate or add Quarto documentation\nAdd inline code comments for complex logic\n\nSubmit a PR:\n\nDescribe your changes\nReference any related issues\nInclude screenshots for UI changes\n\n\n\n\n\n\nFollow Rust’s official style guidelines\nUse async/await consistently for asynchronous code\nEnsure proper error handling\nAdd debug and info logging at appropriate points\nDocument public interfaces with doc comments\n\n\n\n\n\nTo implement a new state backend (e.g., PostgreSQL):\n\nCreate a new file src/state/postgres_state_manager.rs\nImplement the StateManager trait\nAdd appropriate tests\nUpdate main.rs to include the new backend option\n\nExample skeleton:\npub struct PostgresStateManager {\n    pool: PgPool,\n    cluster_id: String,\n}\n\nimpl PostgresStateManager {\n    pub async fn new(db_url: &str, cluster_id: &str) -&gt; Self {\n        let pool = PgPool::connect(db_url).await.expect(\"Failed to connect to PostgreSQL\");\n        PostgresStateManager {\n            pool,\n            cluster_id: String::from(cluster_id),\n        }\n    }\n}\n\n#[async_trait]\nimpl StateManager for PostgresStateManager {\n    // Implement all required methods\n    async fn get_work_from_queue(&self, queue: &str) -&gt; Option&lt;TaskPayload&gt; {\n        // Implementation\n    }\n\n    // ... other methods\n}\n\n\n\n\n\n# Run all tests\ncargo test\n\n# Run specific tests\ncargo test state_manager\n\n# Run with feature flags\ncargo test --features postgresql\n\n\n\n\nUnit Tests: Test individual components in isolation\nIntegration Tests: Test component interactions\nEnd-to-End Tests: Test complete workflows\n\n\n\n\n\nTest both success and error paths\nUse descriptive test names\nSet up appropriate fixtures\nClean up after tests (especially important for Redis/DB tests)\n\n\n\n\n\n\n\nCyclonetix uses the tracing crate for structured logging. To enable verbose logging:\nRUST_LOG=debug cargo run\n\n\n\n\nRedis Connection: Ensure Redis is running and accessible\nTask Execution: Check agent logs for command execution errors\nUI Issues: Check browser console and server logs\n\n\n\n\n\nRun cargo doc --open to generate and view the API documentation.\n\n\n\n\nUpdate Version: Update version in Cargo.toml\nUpdate Changelog: Document changes in CHANGELOG.md\nCreate Tag: Tag the release in Git\nBuild Release: Run cargo build --release\nCreate GitHub Release: Upload the binary\n\n\n\n\n\nReview the Roadmap to see planned features\nCheck out Troubleshooting & FAQ for common issues\nExplore the Reference section for detailed specifications",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#project-structure",
    "href": "developer-guide.html#project-structure",
    "title": "Developer Guide",
    "section": "",
    "text": "The Cyclonetix codebase is organized as follows:\ncyclonetix/\n├── src/\n│   ├── agent/            # Agent implementation\n│   ├── graph/            # Execution graph and orchestrator\n│   ├── models/           # Core data models\n│   ├── server/           # Web UI and API\n│   ├── state/            # State management\n│   └── utils/            # Utility functions\n├── static/               # Static web assets\n├── templates/            # Tera templates for UI\n├── docs/                 # Documentation\n└── data/                 # Example data files",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#architecture-overview",
    "href": "developer-guide.html#architecture-overview",
    "title": "Developer Guide",
    "section": "",
    "text": "Cyclonetix follows a modular architecture with clear separation of concerns:\n\nCore Models (src/models/): Define the data structures used throughout the system.\nState Management (src/state/): Abstracts storage backends and provides a unified interface.\nAgent (src/agent/): Handles task execution and reporting.\nGraph (src/graph/): Manages task dependencies and execution flow.\nServer (src/server/): Provides the web UI and API.",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#key-abstractions",
    "href": "developer-guide.html#key-abstractions",
    "title": "Developer Guide",
    "section": "",
    "text": "The StateManager trait (src/state/state_manager.rs) is the central abstraction for state persistence:\n#[async_trait]\npub trait StateManager: Sync + Send {\n    // Queue operations\n    async fn get_work_from_queue(&self, queue: &str) -&gt; Option&lt;TaskPayload&gt;;\n    async fn put_work_on_queue(&self, task_payload: &TaskPayload, queue: &str);\n\n    // Task operations\n    async fn save_task(&self, task: &TaskTemplate);\n    async fn load_task(&self, task_id: &str) -&gt; Option&lt;TaskTemplate&gt;;\n\n    // DAG operations\n    async fn save_dag_instance(&self, dag_instance: &DagInstance);\n    async fn load_dag_instance(&self, run_id: &str) -&gt; Option&lt;DagInstance&gt;;\n\n    // many other methods...\n}\nThis trait is implemented for different backends like Redis and in-memory storage.\n\n\n\nThe ExecutionGraph struct (src/graph/graph_manager.rs) manages task dependencies and execution order:\npub struct ExecutionGraph {\n    pub graph: DiGraph&lt;String, ()&gt;,\n    pub node_map: HashMap&lt;String, NodeIndex&gt;,\n}\nIt provides methods for building and traversing the execution graph.\n\n\n\nThe Agent struct (src/agent/agent.rs) is responsible for executing tasks:\npub struct Agent&lt;S: 'static + StateManager + ?Sized&gt; {\n    state_manager: Arc&lt;S&gt;,\n    agent_id: String,\n}",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#development-environment-setup",
    "href": "developer-guide.html#development-environment-setup",
    "title": "Developer Guide",
    "section": "",
    "text": "Rust 1.84+ with Cargo\nRedis (optional, for testing with Redis backend)\nGit\n\n\n\n\n\nClone the repository:\n\ngit clone https://github.com/neural-chilli/Cyclonetix.git\ncd Cyclonetix\n\nBuild the project:\n\ncargo build\n\nRun tests:\n\ncargo test\n\nRun with development features:\n\nDEV_MODE=true cargo run\n\n\n\nThe UI is built with:\n\nTera for templating\nTabler for UI components\nCytoscape.js for graph visualization\n\nWhen DEV_MODE=true, templates are reloaded on each request, making UI development faster.",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#how-to-contribute",
    "href": "developer-guide.html#how-to-contribute",
    "title": "Developer Guide",
    "section": "",
    "text": "Check the GitHub issues labeled with “good first issue” or “help wanted”.\n\n\n\n\nCreate a branch:\ngit checkout -b feature/your-feature-name\nImplement your changes:\n\nKeep each commit focused on a single change\nFollow the existing code style\nAdd appropriate tests\n\nUpdate documentation:\n\nUpdate or add Quarto documentation\nAdd inline code comments for complex logic\n\nSubmit a PR:\n\nDescribe your changes\nReference any related issues\nInclude screenshots for UI changes\n\n\n\n\n\n\nFollow Rust’s official style guidelines\nUse async/await consistently for asynchronous code\nEnsure proper error handling\nAdd debug and info logging at appropriate points\nDocument public interfaces with doc comments",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#adding-a-new-backend",
    "href": "developer-guide.html#adding-a-new-backend",
    "title": "Developer Guide",
    "section": "",
    "text": "To implement a new state backend (e.g., PostgreSQL):\n\nCreate a new file src/state/postgres_state_manager.rs\nImplement the StateManager trait\nAdd appropriate tests\nUpdate main.rs to include the new backend option\n\nExample skeleton:\npub struct PostgresStateManager {\n    pool: PgPool,\n    cluster_id: String,\n}\n\nimpl PostgresStateManager {\n    pub async fn new(db_url: &str, cluster_id: &str) -&gt; Self {\n        let pool = PgPool::connect(db_url).await.expect(\"Failed to connect to PostgreSQL\");\n        PostgresStateManager {\n            pool,\n            cluster_id: String::from(cluster_id),\n        }\n    }\n}\n\n#[async_trait]\nimpl StateManager for PostgresStateManager {\n    // Implement all required methods\n    async fn get_work_from_queue(&self, queue: &str) -&gt; Option&lt;TaskPayload&gt; {\n        // Implementation\n    }\n\n    // ... other methods\n}",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#testing",
    "href": "developer-guide.html#testing",
    "title": "Developer Guide",
    "section": "",
    "text": "# Run all tests\ncargo test\n\n# Run specific tests\ncargo test state_manager\n\n# Run with feature flags\ncargo test --features postgresql\n\n\n\n\nUnit Tests: Test individual components in isolation\nIntegration Tests: Test component interactions\nEnd-to-End Tests: Test complete workflows\n\n\n\n\n\nTest both success and error paths\nUse descriptive test names\nSet up appropriate fixtures\nClean up after tests (especially important for Redis/DB tests)",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#debugging",
    "href": "developer-guide.html#debugging",
    "title": "Developer Guide",
    "section": "",
    "text": "Cyclonetix uses the tracing crate for structured logging. To enable verbose logging:\nRUST_LOG=debug cargo run\n\n\n\n\nRedis Connection: Ensure Redis is running and accessible\nTask Execution: Check agent logs for command execution errors\nUI Issues: Check browser console and server logs",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#api-documentation",
    "href": "developer-guide.html#api-documentation",
    "title": "Developer Guide",
    "section": "",
    "text": "Run cargo doc --open to generate and view the API documentation.",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#release-process",
    "href": "developer-guide.html#release-process",
    "title": "Developer Guide",
    "section": "",
    "text": "Update Version: Update version in Cargo.toml\nUpdate Changelog: Document changes in CHANGELOG.md\nCreate Tag: Tag the release in Git\nBuild Release: Run cargo build --release\nCreate GitHub Release: Upload the binary",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer-guide.html#next-steps",
    "href": "developer-guide.html#next-steps",
    "title": "Developer Guide",
    "section": "",
    "text": "Review the Roadmap to see planned features\nCheck out Troubleshooting & FAQ for common issues\nExplore the Reference section for detailed specifications",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html",
    "href": "user-guide/ui-overview.html",
    "title": "UI Overview",
    "section": "",
    "text": "Cyclonetix provides a web-based user interface for managing, monitoring, and visualizing workflows. This guide gives you an overview of the main UI components and how to use them effectively.\n\n\nThe dashboard is the main landing page, providing a high-level overview of your Cyclonetix environment.\ntodo - provide a screenshot of the dashboard\nKey elements include:\n\nSummary Cards: Quick statistics on agents, queues, and running DAGs\nAgents Table: List of active agents with their status\nQueue Tasks: Overview of tasks in different queues\nRunning DAGs: Recently active DAG executions with progress indicators\n\n\n\n\nThe Tasks page allows you to manage and schedule individual tasks.\ntodo - provide a screenshot of the tasks page\nFeatures include:\n\nTask Browser: Filterable, searchable list of available tasks\nTask Details: View task definitions, parameters, and dependencies\nSchedule Task: Quickly schedule a task with custom parameters\nRecent Runs: Historical list of task executions\n\n\n\nTo schedule a task:\n\nFind the task in the task list\nClick the “Schedule” button\nConfigure any environment variables or parameters\nClick “Schedule Task” to submit\n\n\n\n\n\nThe DAGs page lets you manage and schedule pre-defined DAGs.\ntodo - provide a screenshot of the DAGs page\nFeatures include:\n\nDAG Browser: View and filter available DAGs\nDAG Details: Examine DAG configuration and included tasks\nSchedule DAG: Execute a DAG with custom settings\nDAG History: View past executions of the DAG\n\n\n\n\nThe Running DAGs page shows all currently active workflow executions.\ntodo - provide a screenshot of the running DAGs page\nFor each execution, you can see:\n\nStatus: Current execution status\nProgress: Visual progress bar showing completion percentage\nTask Breakdown: Number of completed/total tasks\nLast Updated: When the execution was last updated\nActions: Buttons to view, pause, or cancel the execution\n\n\n\n\nThe DAG Visualization page provides a real-time graphical view of a workflow’s execution.\ntodo - provide a screenshot of the DAG visualization page\nKey features include:\n\nGraph View: Interactive visualization of tasks and dependencies\nTask Status: Color-coded nodes showing execution status\nTask Details: Click on a task to view detailed information\nLive Updates: Real-time updates as tasks complete\nNavigation: Pan and zoom controls for exploring complex DAGs\n\nThe color coding for tasks is:\n\nGray: Pending\nBlue: Queued\nOrange: Running\nGreen: Completed\nRed: Failed\n\n\n\n\nThe Agents page shows information about all worker agents in your Cyclonetix environment.\ntodo - provide a screenshot of the agents page\nYou can:\n\nMonitor Agent Status: See which agents are active\nView Assigned Tasks: Check which tasks are currently being processed by each agent\nAgent Health: Review heartbeat information and resource utilization\n\n\n\n\nThe Configuration section allows you to manage system-wide settings.\ntodo - provide a screenshot of the configuration page\nAvailable configuration areas include:\n\nTask Configuration: Manage task definitions\nDAG Configuration: Manage DAG definitions\nContext Configuration: Set up environment contexts\nQueue Configuration: Configure task queues\nScheduling Settings: Set up default scheduling behavior\n\n\n\n\nCustomize your Cyclonetix UI experience:\n\nTheme Toggle: Switch between light and dark mode\nRefresh Rate: Control how often the UI updates\nTable Density: Adjust the compactness of data tables\nTime Zone: Set your preferred time zone for timestamps\n\n\n\n\nThe Cyclonetix UI is designed to work well on various screen sizes, from desktop monitors to tablets and smartphones. The layout automatically adjusts to provide the best experience for your device.\n\n\n\nFor power users, the UI supports several keyboard shortcuts:\n\n?: Show keyboard shortcuts help\nd: Go to Dashboard\nt: Go to Tasks page\ng: Go to DAGs page\nr: Go to Running DAGs\na: Go to Agents page\ns: Open Search\nf: Toggle fullscreen mode for DAG visualization\n\n\n\n\nThe UI includes a notification system that alerts you to important events:\n\nTask failures\nCompleted DAGs\nSystem warnings\nAgent disconnections\n\nNotifications appear in the top-right corner and can be reviewed in the notifications panel.\n\n\n\n\nLearn how to Deploy Cyclonetix in your environment\nExplore Advanced Features for more capabilities\nCheck the Troubleshooting & FAQ if you encounter issues",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#dashboard",
    "href": "user-guide/ui-overview.html#dashboard",
    "title": "UI Overview",
    "section": "",
    "text": "The dashboard is the main landing page, providing a high-level overview of your Cyclonetix environment.\ntodo - provide a screenshot of the dashboard\nKey elements include:\n\nSummary Cards: Quick statistics on agents, queues, and running DAGs\nAgents Table: List of active agents with their status\nQueue Tasks: Overview of tasks in different queues\nRunning DAGs: Recently active DAG executions with progress indicators",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#tasks-page",
    "href": "user-guide/ui-overview.html#tasks-page",
    "title": "UI Overview",
    "section": "",
    "text": "The Tasks page allows you to manage and schedule individual tasks.\ntodo - provide a screenshot of the tasks page\nFeatures include:\n\nTask Browser: Filterable, searchable list of available tasks\nTask Details: View task definitions, parameters, and dependencies\nSchedule Task: Quickly schedule a task with custom parameters\nRecent Runs: Historical list of task executions\n\n\n\nTo schedule a task:\n\nFind the task in the task list\nClick the “Schedule” button\nConfigure any environment variables or parameters\nClick “Schedule Task” to submit",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#dags-page",
    "href": "user-guide/ui-overview.html#dags-page",
    "title": "UI Overview",
    "section": "",
    "text": "The DAGs page lets you manage and schedule pre-defined DAGs.\ntodo - provide a screenshot of the DAGs page\nFeatures include:\n\nDAG Browser: View and filter available DAGs\nDAG Details: Examine DAG configuration and included tasks\nSchedule DAG: Execute a DAG with custom settings\nDAG History: View past executions of the DAG",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#running-dags",
    "href": "user-guide/ui-overview.html#running-dags",
    "title": "UI Overview",
    "section": "",
    "text": "The Running DAGs page shows all currently active workflow executions.\ntodo - provide a screenshot of the running DAGs page\nFor each execution, you can see:\n\nStatus: Current execution status\nProgress: Visual progress bar showing completion percentage\nTask Breakdown: Number of completed/total tasks\nLast Updated: When the execution was last updated\nActions: Buttons to view, pause, or cancel the execution",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#dag-visualization",
    "href": "user-guide/ui-overview.html#dag-visualization",
    "title": "UI Overview",
    "section": "",
    "text": "The DAG Visualization page provides a real-time graphical view of a workflow’s execution.\ntodo - provide a screenshot of the DAG visualization page\nKey features include:\n\nGraph View: Interactive visualization of tasks and dependencies\nTask Status: Color-coded nodes showing execution status\nTask Details: Click on a task to view detailed information\nLive Updates: Real-time updates as tasks complete\nNavigation: Pan and zoom controls for exploring complex DAGs\n\nThe color coding for tasks is:\n\nGray: Pending\nBlue: Queued\nOrange: Running\nGreen: Completed\nRed: Failed",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#agents-management",
    "href": "user-guide/ui-overview.html#agents-management",
    "title": "UI Overview",
    "section": "",
    "text": "The Agents page shows information about all worker agents in your Cyclonetix environment.\ntodo - provide a screenshot of the agents page\nYou can:\n\nMonitor Agent Status: See which agents are active\nView Assigned Tasks: Check which tasks are currently being processed by each agent\nAgent Health: Review heartbeat information and resource utilization",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#settings-configuration",
    "href": "user-guide/ui-overview.html#settings-configuration",
    "title": "UI Overview",
    "section": "",
    "text": "The Configuration section allows you to manage system-wide settings.\ntodo - provide a screenshot of the configuration page\nAvailable configuration areas include:\n\nTask Configuration: Manage task definitions\nDAG Configuration: Manage DAG definitions\nContext Configuration: Set up environment contexts\nQueue Configuration: Configure task queues\nScheduling Settings: Set up default scheduling behavior",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#user-preferences",
    "href": "user-guide/ui-overview.html#user-preferences",
    "title": "UI Overview",
    "section": "",
    "text": "Customize your Cyclonetix UI experience:\n\nTheme Toggle: Switch between light and dark mode\nRefresh Rate: Control how often the UI updates\nTable Density: Adjust the compactness of data tables\nTime Zone: Set your preferred time zone for timestamps",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#mobile-responsiveness",
    "href": "user-guide/ui-overview.html#mobile-responsiveness",
    "title": "UI Overview",
    "section": "",
    "text": "The Cyclonetix UI is designed to work well on various screen sizes, from desktop monitors to tablets and smartphones. The layout automatically adjusts to provide the best experience for your device.",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#keyboard-shortcuts",
    "href": "user-guide/ui-overview.html#keyboard-shortcuts",
    "title": "UI Overview",
    "section": "",
    "text": "For power users, the UI supports several keyboard shortcuts:\n\n?: Show keyboard shortcuts help\nd: Go to Dashboard\nt: Go to Tasks page\ng: Go to DAGs page\nr: Go to Running DAGs\na: Go to Agents page\ns: Open Search\nf: Toggle fullscreen mode for DAG visualization",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#notifications",
    "href": "user-guide/ui-overview.html#notifications",
    "title": "UI Overview",
    "section": "",
    "text": "The UI includes a notification system that alerts you to important events:\n\nTask failures\nCompleted DAGs\nSystem warnings\nAgent disconnections\n\nNotifications appear in the top-right corner and can be reviewed in the notifications panel.",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/ui-overview.html#next-steps",
    "href": "user-guide/ui-overview.html#next-steps",
    "title": "UI Overview",
    "section": "",
    "text": "Learn how to Deploy Cyclonetix in your environment\nExplore Advanced Features for more capabilities\nCheck the Troubleshooting & FAQ if you encounter issues",
    "crumbs": [
      "User Guide",
      "UI Overview"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html",
    "href": "user-guide/task-definition.html",
    "title": "Task Definition",
    "section": "",
    "text": "This guide will show you how to define tasks in Cyclonetix, from basic task definitions to more complex configurations with parameters, dependencies, and advanced features.\n\n\nTasks in Cyclonetix are defined using YAML files. Here’s the structure of a basic task:\nid: \"task_id\"                    # Unique identifier for the task\nname: \"Human-readable name\"      # Display name\ndescription: \"Task description\"  # Optional description\ncommand: \"echo 'Hello World'\"    # Command to execute\ndependencies: []                 # List of prerequisite tasks\nparameters: {}                   # Task-specific parameters\nqueue: \"default\"                 # Optional queue name for execution\n\n\n\nBy default, Cyclonetix looks for task definitions in the data/tasks directory. You can organize tasks in subdirectories for better management:\ndata/\n└── tasks/\n    ├── data_processing/\n    │   ├── extract.yaml\n    │   ├── transform.yaml\n    │   └── load.yaml\n    ├── ml/\n    │   ├── train.yaml\n    │   └── evaluate.yaml\n    └── deployment/\n        ├── build.yaml\n        └── deploy.yaml\n\n\n\nThe command field specifies what will be executed when the task runs. This can be:\n\nA simple shell command\nA complex script with pipes and redirections\nA reference to an executable file\n\nExamples:\n# Simple command\ncommand: \"echo 'Task completed'\"\n\n# Multi-line script\ncommand: |\n  echo \"Starting task\"\n  python /path/to/script.py --arg1 value1 --arg2 value2\n  echo \"Task finished with status $?\"\n\n# Using environment variables\ncommand: \"python train.py --data-path ${DATA_PATH} --epochs ${EPOCHS}\"\n\n\n\nDependencies are specified as a list of task IDs that must complete successfully before this task can start:\ndependencies:\n  - \"data_preparation\"\n  - \"feature_engineering\"\n\n\nYou can specify parameter-specific dependencies for more granular control:\ndependencies:\n  - \"data_preparation:daily\"  # Depends on data_preparation with parameter set \"daily\"\n  - \"feature_engineering\"\n\n\n\n\nParameters allow you to make tasks configurable and reusable:\nparameters:\n  inputPath: \"/data/input\"\n  outputPath: \"/data/output\"\n  mode: \"incremental\"\n  maxThreads: 4\nParameters can be: - Referenced in the command using environment variables - Overridden at scheduling time - Used to create task variants\n\n\n\nAssigning tasks to specific queues allows for resource allocation and specialization:\nqueue: \"gpu_tasks\"  # Assign to a GPU-specific queue\nIf not specified, tasks will use the default queue.\n\n\n\nTasks can be designated as evaluation points, which allow for dynamic decision-making during execution:\nid: \"evaluate_model\"\nname: \"Evaluate Model Performance\"\ncommand: \"python evaluate.py --model ${MODEL_PATH} --threshold ${THRESHOLD}\"\ndependencies:\n  - \"train_model\"\nevaluation_point: true\nparameters:\n  threshold: 0.85\nEvaluation points can: - Determine which downstream tasks to execute - Modify the execution graph at runtime - Implement conditional branching - Serve as approval gates\n\n\n\nHere’s a comprehensive example of a task definition:\nid: \"train_model\"\nname: \"Train Machine Learning Model\"\ndescription: \"Trains a machine learning model using prepared data\"\ncommand: |\n  python train.py \\\n    --data-path ${DATA_PATH} \\\n    --model-type ${MODEL_TYPE} \\\n    --epochs ${EPOCHS} \\\n    --batch-size ${BATCH_SIZE} \\\n    --output-path ${OUTPUT_PATH} \\\n    --log-level ${LOG_LEVEL}\ndependencies:\n  - \"prepare_data\"\n  - \"feature_engineering\"\nparameters:\n  DATA_PATH: \"/data/processed\"\n  MODEL_TYPE: \"random_forest\"\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"32\"\n  OUTPUT_PATH: \"/models/latest\"\n  LOG_LEVEL: \"INFO\"",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html#basic-task-structure",
    "href": "user-guide/task-definition.html#basic-task-structure",
    "title": "Task Definition",
    "section": "",
    "text": "Tasks in Cyclonetix are defined using YAML files. Here’s the structure of a basic task:\nid: \"task_id\"                    # Unique identifier for the task\nname: \"Human-readable name\"      # Display name\ndescription: \"Task description\"  # Optional description\ncommand: \"echo 'Hello World'\"    # Command to execute\ndependencies: []                 # List of prerequisite tasks\nparameters: {}                   # Task-specific parameters\nqueue: \"default\"                 # Optional queue name for execution",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html#task-file-organization",
    "href": "user-guide/task-definition.html#task-file-organization",
    "title": "Task Definition",
    "section": "",
    "text": "By default, Cyclonetix looks for task definitions in the data/tasks directory. You can organize tasks in subdirectories for better management:\ndata/\n└── tasks/\n    ├── data_processing/\n    │   ├── extract.yaml\n    │   ├── transform.yaml\n    │   └── load.yaml\n    ├── ml/\n    │   ├── train.yaml\n    │   └── evaluate.yaml\n    └── deployment/\n        ├── build.yaml\n        └── deploy.yaml",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html#command-specification",
    "href": "user-guide/task-definition.html#command-specification",
    "title": "Task Definition",
    "section": "",
    "text": "The command field specifies what will be executed when the task runs. This can be:\n\nA simple shell command\nA complex script with pipes and redirections\nA reference to an executable file\n\nExamples:\n# Simple command\ncommand: \"echo 'Task completed'\"\n\n# Multi-line script\ncommand: |\n  echo \"Starting task\"\n  python /path/to/script.py --arg1 value1 --arg2 value2\n  echo \"Task finished with status $?\"\n\n# Using environment variables\ncommand: \"python train.py --data-path ${DATA_PATH} --epochs ${EPOCHS}\"",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html#defining-dependencies",
    "href": "user-guide/task-definition.html#defining-dependencies",
    "title": "Task Definition",
    "section": "",
    "text": "Dependencies are specified as a list of task IDs that must complete successfully before this task can start:\ndependencies:\n  - \"data_preparation\"\n  - \"feature_engineering\"\n\n\nYou can specify parameter-specific dependencies for more granular control:\ndependencies:\n  - \"data_preparation:daily\"  # Depends on data_preparation with parameter set \"daily\"\n  - \"feature_engineering\"",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html#parameter-configuration",
    "href": "user-guide/task-definition.html#parameter-configuration",
    "title": "Task Definition",
    "section": "",
    "text": "Parameters allow you to make tasks configurable and reusable:\nparameters:\n  inputPath: \"/data/input\"\n  outputPath: \"/data/output\"\n  mode: \"incremental\"\n  maxThreads: 4\nParameters can be: - Referenced in the command using environment variables - Overridden at scheduling time - Used to create task variants",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html#queue-assignment",
    "href": "user-guide/task-definition.html#queue-assignment",
    "title": "Task Definition",
    "section": "",
    "text": "Assigning tasks to specific queues allows for resource allocation and specialization:\nqueue: \"gpu_tasks\"  # Assign to a GPU-specific queue\nIf not specified, tasks will use the default queue.",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html#evaluation-points",
    "href": "user-guide/task-definition.html#evaluation-points",
    "title": "Task Definition",
    "section": "",
    "text": "Tasks can be designated as evaluation points, which allow for dynamic decision-making during execution:\nid: \"evaluate_model\"\nname: \"Evaluate Model Performance\"\ncommand: \"python evaluate.py --model ${MODEL_PATH} --threshold ${THRESHOLD}\"\ndependencies:\n  - \"train_model\"\nevaluation_point: true\nparameters:\n  threshold: 0.85\nEvaluation points can: - Determine which downstream tasks to execute - Modify the execution graph at runtime - Implement conditional branching - Serve as approval gates",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/task-definition.html#complete-task-example",
    "href": "user-guide/task-definition.html#complete-task-example",
    "title": "Task Definition",
    "section": "",
    "text": "Here’s a comprehensive example of a task definition:\nid: \"train_model\"\nname: \"Train Machine Learning Model\"\ndescription: \"Trains a machine learning model using prepared data\"\ncommand: |\n  python train.py \\\n    --data-path ${DATA_PATH} \\\n    --model-type ${MODEL_TYPE} \\\n    --epochs ${EPOCHS} \\\n    --batch-size ${BATCH_SIZE} \\\n    --output-path ${OUTPUT_PATH} \\\n    --log-level ${LOG_LEVEL}\ndependencies:\n  - \"prepare_data\"\n  - \"feature_engineering\"\nparameters:\n  DATA_PATH: \"/data/processed\"\n  MODEL_TYPE: \"random_forest\"\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"32\"\n  OUTPUT_PATH: \"/models/latest\"\n  LOG_LEVEL: \"INFO\"",
    "crumbs": [
      "User Guide",
      "Task Definition"
    ]
  },
  {
    "objectID": "user-guide/scheduling-workflows.html",
    "href": "user-guide/scheduling-workflows.html",
    "title": "Scheduling Workflows",
    "section": "",
    "text": "This guide covers the different ways to schedule and execute workflows in Cyclonetix.\n\n\n\n\nCyclonetix provides a CLI for scheduling workflows:\n\n\nTo schedule a workflow by specifying the desired outcome (final task):\n./cyclonetix schedule-task &lt;task_id&gt;\nExample:\n./cyclonetix schedule-task deploy_model\nThis will automatically resolve all dependencies of deploy_model and execute them in the correct order.\n\n\n\nTo schedule a predefined DAG:\n./cyclonetix schedule-dag &lt;dag_id&gt;\nExample:\n./cyclonetix schedule-dag ml_training_pipeline\n\n\n\nYou can provide a context when scheduling:\n./cyclonetix schedule-dag ml_training_pipeline --context production\n\n\n\n\nThe Cyclonetix web UI provides a user-friendly way to schedule workflows:\n\nNavigate to the “Tasks” or “DAGs” page\nFind the task or DAG you want to schedule\nClick the “Schedule” button\nConfigure any parameters or contexts\nSubmit the form to start the execution\n\n\n\n\nUI Scheduling\n\n\n\n\n\n\n\n\nCyclonetix supports time-based scheduling similar to cron:\n# In your DAG definition\nschedule:\n  cron: \"0 5 * * *\"        # Run daily at 5:00 AM\n  timezone: \"UTC\"\n  catchup: false           # Don't run missed executions\nCommon schedule patterns:\n\n\n\nDescription\nCron Expression\n\n\n\n\nDaily at midnight\n0 0 * * *\n\n\nEvery hour\n0 * * * *\n\n\nWeekly on Monday\n0 0 * * 1\n\n\nMonthly on the 1st\n0 0 1 * *\n\n\n\n\n\n\nCyclonetix can trigger workflows based on external events:\n\n\nTrigger a workflow via the REST API:\ncurl -X POST http://your-cyclonetix-server:3000/api/schedule-task \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"task_id\": \"process_data\", \"env_vars\": {\"SOURCE\": \"api_call\"}}'\n\n\n\nConfigure a workflow to start when files arrive in a monitored location:\ntriggers:\n  file_arrival:\n    path: \"/data/incoming/\"\n    pattern: \"*.csv\"\n    wait_seconds: 60\n\n\n\n\n\n\n\nYou can pass environment variables when scheduling a task or DAG:\n./cyclonetix schedule-task generate_report \\\n  --env DATA_DATE=2023-11-01 \\\n  --env REPORT_TYPE=monthly\nThese variables will be available to the task command.\n\n\n\nOverride task parameters at scheduling time:\n./cyclonetix schedule-dag etl_pipeline \\\n  --param data_extraction.limit=5000 \\\n  --param data_loading.mode=incremental\n\n\n\n\n\n\nOnce scheduled, you can monitor the execution:\n\nNavigate to the “Running DAGs” page in the UI\nSelect the execution you want to monitor\nView the DAG visualization with real-time status updates\nAccess task logs and details\n\n\n\n\nTo cancel a running execution:\n\nFind the execution in the UI\nClick “Cancel” and confirm\nAll running tasks will be terminated, and pending tasks will not be executed\n\n\n\n\nIf a task fails, you can rerun it:\n\nNavigate to the execution details\nFind the failed task\nClick “Rerun” to attempt execution again\n\n\n\n\nTo clear execution history:\n./cyclonetix clear-history --days 30\nThis removes execution records older than 30 days.\n\n\n\n\n\n\nFor time-based workflows, you can run executions for past time periods:\n./cyclonetix backfill ml_training_pipeline \\\n  --start-date 2023-01-01 \\\n  --end-date 2023-01-31\nThis will create one execution per day in the specified range.\n\n\n\nControl the number of parallel task executions:\n./cyclonetix schedule-dag large_workflow --max-parallel 5\n\n\n\nOverride normal dependency behavior for special cases:\n./cyclonetix schedule-task final_step --ignore-dependencies\n\n\n\n\n\nLearn how to use the UI Overview to monitor executions\nUnderstand Contexts & Parameters for more control\nExplore Evaluation Points for dynamic workflows",
    "crumbs": [
      "User Guide",
      "Scheduling Workflows"
    ]
  },
  {
    "objectID": "user-guide/scheduling-workflows.html#manual-scheduling",
    "href": "user-guide/scheduling-workflows.html#manual-scheduling",
    "title": "Scheduling Workflows",
    "section": "",
    "text": "Cyclonetix provides a CLI for scheduling workflows:\n\n\nTo schedule a workflow by specifying the desired outcome (final task):\n./cyclonetix schedule-task &lt;task_id&gt;\nExample:\n./cyclonetix schedule-task deploy_model\nThis will automatically resolve all dependencies of deploy_model and execute them in the correct order.\n\n\n\nTo schedule a predefined DAG:\n./cyclonetix schedule-dag &lt;dag_id&gt;\nExample:\n./cyclonetix schedule-dag ml_training_pipeline\n\n\n\nYou can provide a context when scheduling:\n./cyclonetix schedule-dag ml_training_pipeline --context production\n\n\n\n\nThe Cyclonetix web UI provides a user-friendly way to schedule workflows:\n\nNavigate to the “Tasks” or “DAGs” page\nFind the task or DAG you want to schedule\nClick the “Schedule” button\nConfigure any parameters or contexts\nSubmit the form to start the execution\n\n\n\n\nUI Scheduling",
    "crumbs": [
      "User Guide",
      "Scheduling Workflows"
    ]
  },
  {
    "objectID": "user-guide/scheduling-workflows.html#automatic-scheduling",
    "href": "user-guide/scheduling-workflows.html#automatic-scheduling",
    "title": "Scheduling Workflows",
    "section": "",
    "text": "Cyclonetix supports time-based scheduling similar to cron:\n# In your DAG definition\nschedule:\n  cron: \"0 5 * * *\"        # Run daily at 5:00 AM\n  timezone: \"UTC\"\n  catchup: false           # Don't run missed executions\nCommon schedule patterns:\n\n\n\nDescription\nCron Expression\n\n\n\n\nDaily at midnight\n0 0 * * *\n\n\nEvery hour\n0 * * * *\n\n\nWeekly on Monday\n0 0 * * 1\n\n\nMonthly on the 1st\n0 0 1 * *\n\n\n\n\n\n\nCyclonetix can trigger workflows based on external events:\n\n\nTrigger a workflow via the REST API:\ncurl -X POST http://your-cyclonetix-server:3000/api/schedule-task \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"task_id\": \"process_data\", \"env_vars\": {\"SOURCE\": \"api_call\"}}'\n\n\n\nConfigure a workflow to start when files arrive in a monitored location:\ntriggers:\n  file_arrival:\n    path: \"/data/incoming/\"\n    pattern: \"*.csv\"\n    wait_seconds: 60",
    "crumbs": [
      "User Guide",
      "Scheduling Workflows"
    ]
  },
  {
    "objectID": "user-guide/scheduling-workflows.html#parameterized-execution",
    "href": "user-guide/scheduling-workflows.html#parameterized-execution",
    "title": "Scheduling Workflows",
    "section": "",
    "text": "You can pass environment variables when scheduling a task or DAG:\n./cyclonetix schedule-task generate_report \\\n  --env DATA_DATE=2023-11-01 \\\n  --env REPORT_TYPE=monthly\nThese variables will be available to the task command.\n\n\n\nOverride task parameters at scheduling time:\n./cyclonetix schedule-dag etl_pipeline \\\n  --param data_extraction.limit=5000 \\\n  --param data_loading.mode=incremental",
    "crumbs": [
      "User Guide",
      "Scheduling Workflows"
    ]
  },
  {
    "objectID": "user-guide/scheduling-workflows.html#managing-executions",
    "href": "user-guide/scheduling-workflows.html#managing-executions",
    "title": "Scheduling Workflows",
    "section": "",
    "text": "Once scheduled, you can monitor the execution:\n\nNavigate to the “Running DAGs” page in the UI\nSelect the execution you want to monitor\nView the DAG visualization with real-time status updates\nAccess task logs and details\n\n\n\n\nTo cancel a running execution:\n\nFind the execution in the UI\nClick “Cancel” and confirm\nAll running tasks will be terminated, and pending tasks will not be executed\n\n\n\n\nIf a task fails, you can rerun it:\n\nNavigate to the execution details\nFind the failed task\nClick “Rerun” to attempt execution again\n\n\n\n\nTo clear execution history:\n./cyclonetix clear-history --days 30\nThis removes execution records older than 30 days.",
    "crumbs": [
      "User Guide",
      "Scheduling Workflows"
    ]
  },
  {
    "objectID": "user-guide/scheduling-workflows.html#advanced-scheduling",
    "href": "user-guide/scheduling-workflows.html#advanced-scheduling",
    "title": "Scheduling Workflows",
    "section": "",
    "text": "For time-based workflows, you can run executions for past time periods:\n./cyclonetix backfill ml_training_pipeline \\\n  --start-date 2023-01-01 \\\n  --end-date 2023-01-31\nThis will create one execution per day in the specified range.\n\n\n\nControl the number of parallel task executions:\n./cyclonetix schedule-dag large_workflow --max-parallel 5\n\n\n\nOverride normal dependency behavior for special cases:\n./cyclonetix schedule-task final_step --ignore-dependencies",
    "crumbs": [
      "User Guide",
      "Scheduling Workflows"
    ]
  },
  {
    "objectID": "user-guide/scheduling-workflows.html#next-steps",
    "href": "user-guide/scheduling-workflows.html#next-steps",
    "title": "Scheduling Workflows",
    "section": "",
    "text": "Learn how to use the UI Overview to monitor executions\nUnderstand Contexts & Parameters for more control\nExplore Evaluation Points for dynamic workflows",
    "crumbs": [
      "User Guide",
      "Scheduling Workflows"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html",
    "href": "user-guide/building-dags.html",
    "title": "Building DAGs",
    "section": "",
    "text": "This guide explains how to create and organize Directed Acyclic Graphs (DAGs) in Cyclonetix. A DAG is a collection of tasks with dependencies that form a workflow.\n\n\nIn Cyclonetix, you can define DAGs explicitly using YAML files. Here’s the basic structure:\nid: \"dag_id\"                     # Unique identifier for the DAG\nname: \"Human-readable name\"      # Display name\ndescription: \"DAG description\"   # Optional description\ntasks:                           # List of tasks to include\n  - id: \"task_1\"\n  - id: \"task_2\"\n  - id: \"task_3\"\ntags: [\"tag1\", \"tag2\"]           # Optional categorization tags\n\n\n\nDAG definitions are typically stored in the data/dags directory:\ndata/\n└── dags/\n    ├── etl_pipeline.yaml\n    ├── ml_training.yaml\n    └── model_deployment.yaml\n\n\n\nThe tasks section lists all tasks that should be included in the DAG:\ntasks:\n  - id: \"data_extraction\"\n  - id: \"data_transformation\"\n  - id: \"data_loading\"\n  - id: \"data_validation\"\nEach task in the list must have a corresponding task definition file in the tasks directory.\n\n\n\nWhen you include tasks in a DAG, Cyclonetix automatically respects the dependencies defined in each task. You don’t need to redefine the dependencies in the DAG file.\nFor example, if data_transformation depends on data_extraction in their task definitions, this dependency will be automatically respected when you include both tasks in your DAG.\n\n\n\nYou can override default task parameters at the DAG level:\ntasks:\n  - id: \"data_extraction\"\n    parameters:\n      source: \"production_db\"\n      limit: 1000\n  - id: \"data_transformation\"\n    parameters:\n      mode: \"full\"\nThese overrides take precedence over the default values defined in task files.\n\n\n\nYou can assign a specific context to a DAG:\nid: \"etl_pipeline\"\nname: \"ETL Pipeline\"\ncontext: \"production\"\ntasks:\n  - id: \"data_extraction\"\n  - id: \"data_transformation\"\n  - id: \"data_loading\"\nThe specified context will be used for all tasks in the DAG, providing shared environment variables and configuration.\n\n\n\nDAGs can include scheduling information:\nid: \"daily_report\"\nname: \"Daily Reporting Pipeline\"\nschedule:\n  cron: \"0 5 * * *\"        # Run daily at 5:00 AM\n  timezone: \"UTC\"\n  catchup: false           # Don't run missed executions\ntasks:\n  - id: \"generate_report\"\n  - id: \"send_email\"\n\n\n\nYou can include other DAGs as components of a larger DAG:\nid: \"end_to_end_pipeline\"\nname: \"End-to-End Data Pipeline\"\ndags:\n  - id: \"data_ingestion\"\n  - id: \"data_processing\"\n  - id: \"reporting\"\ntasks:\n  - id: \"final_notification\"\nNested DAGs maintain their internal dependencies while also respecting dependencies between DAGs.\n\n\n\nOnce defined, you can visualize your DAG structure in the Cyclonetix UI:\n\nNavigate to the DAGs page in the UI\nSelect the DAG you want to visualize\nView the graphical representation showing tasks and dependencies\n\n\n\n\nDAG Visualization Example\n\n\n\n\n\nHere’s a comprehensive example of a DAG definition:\nid: \"ml_training_pipeline\"\nname: \"ML Training Pipeline\"\ndescription: \"Complete pipeline for training and evaluating ML models\"\ncontext: \"ml_training\"\nschedule:\n  cron: \"0 1 * * 0\"        # Weekly on Sunday at 1:00 AM\n  timezone: \"UTC\"\n  catchup: false\ntasks:\n  - id: \"data_extraction\"\n    parameters:\n      source: \"production_db\"\n      tables: [\"users\", \"transactions\", \"products\"]\n\n  - id: \"data_cleaning\"\n    parameters:\n      mode: \"strict\"\n\n  - id: \"feature_engineering\"\n    parameters:\n      features_config: \"/configs/features_v2.json\"\n\n  - id: \"train_model\"\n    parameters:\n      model_type: \"gradient_boosting\"\n      hyperparams_file: \"/configs/hyperparams.json\"\n\n  - id: \"evaluate_model\"\n    parameters:\n      metrics: [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n\n  - id: \"register_model\"\n    parameters:\n      registry: \"mlflow\"\n      model_name: \"transaction_classifier\"\n\ntags: [\"ml\", \"production\", \"weekly\"]\n\n\n\n\nLearn about Scheduling Workflows\nUnderstand UI Overview\nExplore Contexts & Parameters for more advanced configurations",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#dag-definition",
    "href": "user-guide/building-dags.html#dag-definition",
    "title": "Building DAGs",
    "section": "",
    "text": "In Cyclonetix, you can define DAGs explicitly using YAML files. Here’s the basic structure:\nid: \"dag_id\"                     # Unique identifier for the DAG\nname: \"Human-readable name\"      # Display name\ndescription: \"DAG description\"   # Optional description\ntasks:                           # List of tasks to include\n  - id: \"task_1\"\n  - id: \"task_2\"\n  - id: \"task_3\"\ntags: [\"tag1\", \"tag2\"]           # Optional categorization tags",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#dag-file-organization",
    "href": "user-guide/building-dags.html#dag-file-organization",
    "title": "Building DAGs",
    "section": "",
    "text": "DAG definitions are typically stored in the data/dags directory:\ndata/\n└── dags/\n    ├── etl_pipeline.yaml\n    ├── ml_training.yaml\n    └── model_deployment.yaml",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#including-tasks-in-a-dag",
    "href": "user-guide/building-dags.html#including-tasks-in-a-dag",
    "title": "Building DAGs",
    "section": "",
    "text": "The tasks section lists all tasks that should be included in the DAG:\ntasks:\n  - id: \"data_extraction\"\n  - id: \"data_transformation\"\n  - id: \"data_loading\"\n  - id: \"data_validation\"\nEach task in the list must have a corresponding task definition file in the tasks directory.",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#task-dependencies",
    "href": "user-guide/building-dags.html#task-dependencies",
    "title": "Building DAGs",
    "section": "",
    "text": "When you include tasks in a DAG, Cyclonetix automatically respects the dependencies defined in each task. You don’t need to redefine the dependencies in the DAG file.\nFor example, if data_transformation depends on data_extraction in their task definitions, this dependency will be automatically respected when you include both tasks in your DAG.",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#parameter-overrides",
    "href": "user-guide/building-dags.html#parameter-overrides",
    "title": "Building DAGs",
    "section": "",
    "text": "You can override default task parameters at the DAG level:\ntasks:\n  - id: \"data_extraction\"\n    parameters:\n      source: \"production_db\"\n      limit: 1000\n  - id: \"data_transformation\"\n    parameters:\n      mode: \"full\"\nThese overrides take precedence over the default values defined in task files.",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#context-assignment",
    "href": "user-guide/building-dags.html#context-assignment",
    "title": "Building DAGs",
    "section": "",
    "text": "You can assign a specific context to a DAG:\nid: \"etl_pipeline\"\nname: \"ETL Pipeline\"\ncontext: \"production\"\ntasks:\n  - id: \"data_extraction\"\n  - id: \"data_transformation\"\n  - id: \"data_loading\"\nThe specified context will be used for all tasks in the DAG, providing shared environment variables and configuration.",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#scheduling-configuration",
    "href": "user-guide/building-dags.html#scheduling-configuration",
    "title": "Building DAGs",
    "section": "",
    "text": "DAGs can include scheduling information:\nid: \"daily_report\"\nname: \"Daily Reporting Pipeline\"\nschedule:\n  cron: \"0 5 * * *\"        # Run daily at 5:00 AM\n  timezone: \"UTC\"\n  catchup: false           # Don't run missed executions\ntasks:\n  - id: \"generate_report\"\n  - id: \"send_email\"",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#nested-dags",
    "href": "user-guide/building-dags.html#nested-dags",
    "title": "Building DAGs",
    "section": "",
    "text": "You can include other DAGs as components of a larger DAG:\nid: \"end_to_end_pipeline\"\nname: \"End-to-End Data Pipeline\"\ndags:\n  - id: \"data_ingestion\"\n  - id: \"data_processing\"\n  - id: \"reporting\"\ntasks:\n  - id: \"final_notification\"\nNested DAGs maintain their internal dependencies while also respecting dependencies between DAGs.",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#visualizing-dags",
    "href": "user-guide/building-dags.html#visualizing-dags",
    "title": "Building DAGs",
    "section": "",
    "text": "Once defined, you can visualize your DAG structure in the Cyclonetix UI:\n\nNavigate to the DAGs page in the UI\nSelect the DAG you want to visualize\nView the graphical representation showing tasks and dependencies\n\n\n\n\nDAG Visualization Example",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#complete-dag-example",
    "href": "user-guide/building-dags.html#complete-dag-example",
    "title": "Building DAGs",
    "section": "",
    "text": "Here’s a comprehensive example of a DAG definition:\nid: \"ml_training_pipeline\"\nname: \"ML Training Pipeline\"\ndescription: \"Complete pipeline for training and evaluating ML models\"\ncontext: \"ml_training\"\nschedule:\n  cron: \"0 1 * * 0\"        # Weekly on Sunday at 1:00 AM\n  timezone: \"UTC\"\n  catchup: false\ntasks:\n  - id: \"data_extraction\"\n    parameters:\n      source: \"production_db\"\n      tables: [\"users\", \"transactions\", \"products\"]\n\n  - id: \"data_cleaning\"\n    parameters:\n      mode: \"strict\"\n\n  - id: \"feature_engineering\"\n    parameters:\n      features_config: \"/configs/features_v2.json\"\n\n  - id: \"train_model\"\n    parameters:\n      model_type: \"gradient_boosting\"\n      hyperparams_file: \"/configs/hyperparams.json\"\n\n  - id: \"evaluate_model\"\n    parameters:\n      metrics: [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n\n  - id: \"register_model\"\n    parameters:\n      registry: \"mlflow\"\n      model_name: \"transaction_classifier\"\n\ntags: [\"ml\", \"production\", \"weekly\"]",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "user-guide/building-dags.html#next-steps",
    "href": "user-guide/building-dags.html#next-steps",
    "title": "Building DAGs",
    "section": "",
    "text": "Learn about Scheduling Workflows\nUnderstand UI Overview\nExplore Contexts & Parameters for more advanced configurations",
    "crumbs": [
      "User Guide",
      "Building DAGs"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "This guide will help you quickly set up Cyclonetix and run your first workflow.\n\n\n\n\n\nRust (1.84 or later)\nRedis (for production deployments)\n\n\n\n\n\n\ngit clone https://github.com/neural-chilli/Cyclonetix.git\ncd Cyclonetix\n\n\n\ncargo build --release\n\n\n\n./target/release/cyclonetix\nThis will start Cyclonetix with an in-memory backend, perfect for testing and development.\n\n\n\n\n\nLet’s create a simple workflow with three tasks:\n\nData preparation\nModel training\nResult evaluation\n\n\n\nCreate a new directory called data/tasks and add the following YAML files:\ndata/tasks/prepare_data.yaml:\nid: \"prepare_data\"\nname: \"Prepare Data\"\ncommand: \"echo 'Preparing data...'; sleep 2; echo 'Data ready'\"\ndependencies: []\nparameters: {}\ndata/tasks/train_model.yaml:\nid: \"train_model\"\nname: \"Train Model\"\ncommand: \"echo 'Training model...'; sleep 3; echo 'Model trained'\"\ndependencies: [\"prepare_data\"]\nparameters: {}\ndata/tasks/evaluate_results.yaml:\nid: \"evaluate_results\"\nname: \"Evaluate Results\"\ncommand: \"echo 'Evaluating results...'; sleep 1; echo 'Evaluation complete'\"\ndependencies: [\"train_model\"]\nparameters: {}\n\n\n\nYou can schedule your workflow in one of two ways:\n\n\n./target/release/cyclonetix schedule-task evaluate_results\nCyclonetix will automatically determine that evaluate_results depends on train_model, which depends on prepare_data, and will execute them in the correct order.\n\n\n\nCreate a file data/dags/model_training.yaml:\nid: \"model_training\"\nname: \"Model Training Pipeline\"\ndescription: \"A simple ML model training pipeline\"\ntasks:\n  - id: \"prepare_data\"\n  - id: \"train_model\"\n  - id: \"evaluate_results\"\ntags: [\"ml\", \"training\"]\nThen schedule the DAG:\n./target/release/cyclonetix schedule-dag model_training\n\n\n\n\nOpen your browser and navigate to http://localhost:3000 to see the Cyclonetix UI. You should see your DAG executing with tasks progressing from “pending” to “running” to “completed.”\n\n\n\n\nFor development and UI work, you can enable hot-reloading of templates:\nDEV_MODE=true cargo run\n\n\n\n\nLearn about the core concepts of Cyclonetix\nExplore the UI to monitor and manage your workflows\nDefine more complex tasks with parameters and contexts\nSet up a production deployment with Redis or PostgreSQL",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started.html#installation",
    "href": "getting-started.html#installation",
    "title": "Getting Started",
    "section": "",
    "text": "Rust (1.84 or later)\nRedis (for production deployments)\n\n\n\n\n\n\ngit clone https://github.com/neural-chilli/Cyclonetix.git\ncd Cyclonetix\n\n\n\ncargo build --release\n\n\n\n./target/release/cyclonetix\nThis will start Cyclonetix with an in-memory backend, perfect for testing and development.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started.html#your-first-workflow",
    "href": "getting-started.html#your-first-workflow",
    "title": "Getting Started",
    "section": "",
    "text": "Let’s create a simple workflow with three tasks:\n\nData preparation\nModel training\nResult evaluation\n\n\n\nCreate a new directory called data/tasks and add the following YAML files:\ndata/tasks/prepare_data.yaml:\nid: \"prepare_data\"\nname: \"Prepare Data\"\ncommand: \"echo 'Preparing data...'; sleep 2; echo 'Data ready'\"\ndependencies: []\nparameters: {}\ndata/tasks/train_model.yaml:\nid: \"train_model\"\nname: \"Train Model\"\ncommand: \"echo 'Training model...'; sleep 3; echo 'Model trained'\"\ndependencies: [\"prepare_data\"]\nparameters: {}\ndata/tasks/evaluate_results.yaml:\nid: \"evaluate_results\"\nname: \"Evaluate Results\"\ncommand: \"echo 'Evaluating results...'; sleep 1; echo 'Evaluation complete'\"\ndependencies: [\"train_model\"]\nparameters: {}\n\n\n\nYou can schedule your workflow in one of two ways:\n\n\n./target/release/cyclonetix schedule-task evaluate_results\nCyclonetix will automatically determine that evaluate_results depends on train_model, which depends on prepare_data, and will execute them in the correct order.\n\n\n\nCreate a file data/dags/model_training.yaml:\nid: \"model_training\"\nname: \"Model Training Pipeline\"\ndescription: \"A simple ML model training pipeline\"\ntasks:\n  - id: \"prepare_data\"\n  - id: \"train_model\"\n  - id: \"evaluate_results\"\ntags: [\"ml\", \"training\"]\nThen schedule the DAG:\n./target/release/cyclonetix schedule-dag model_training\n\n\n\n\nOpen your browser and navigate to http://localhost:3000 to see the Cyclonetix UI. You should see your DAG executing with tasks progressing from “pending” to “running” to “completed.”",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started.html#development-mode",
    "href": "getting-started.html#development-mode",
    "title": "Getting Started",
    "section": "",
    "text": "For development and UI work, you can enable hot-reloading of templates:\nDEV_MODE=true cargo run",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started.html#next-steps",
    "href": "getting-started.html#next-steps",
    "title": "Getting Started",
    "section": "",
    "text": "Learn about the core concepts of Cyclonetix\nExplore the UI to monitor and manage your workflows\nDefine more complex tasks with parameters and contexts\nSet up a production deployment with Redis or PostgreSQL",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Cyclonetix is a lightweight, Rust-based workflow orchestrator designed to be fast, easy to use, and highly scalable. It is built for both cloud-native and on-premises deployments, supporting outcome-based scheduling, self-assembling DAGs, and manual DAG execution.\nWith Cyclonetix, you can:\n\nDefine tasks with dependencies, commands, and parameters\nBuild execution graphs explicitly or let them self-assemble\nSchedule outcomes and let the system determine the execution path\nMonitor executions in real-time through a modern web UI\nScale from development environments to large-scale production deployments\n\n\n\n\nCyclonetix aims to be:\n\nEffortlessly simple to get started with\nExceptionally fast and lightweight\nFlexible enough for both beginners and enterprise-scale workloads\nClever by design, reducing user cognitive load\nCapable of advanced scheduling mechanisms (outcome-based scheduling + explicit DAG execution)\n\n\n\n\nCyclonetix was designed with several core principles in mind:\n\nReduce cognitive load - Users shouldn’t need to understand complex concepts to orchestrate simple workflows\nStart small, scale big - The same system should work for both local development and massive production deployments\nSimple things should be simple, complex things should be possible - Basic use cases should require minimal effort\nDesign for the real world - Built based on experience with real-world orchestration challenges\nPerformance matters - A lightweight core that executes jobs efficiently with minimal overhead\n\nWhether you’re running data pipelines, ML training jobs, or software deployment processes, Cyclonetix provides the flexibility, performance, and ease of use to make your workflow orchestration simpler and more efficient.\n\n\n\n\nGetting Started - Install and run your first workflow\nCore Concepts - Learn about Cyclonetix’s architecture\nUser Guide - Practical guides for using Cyclonetix\nDeveloper Guide - Contributing to Cyclonetix",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-is-cyclonetix",
    "href": "index.html#what-is-cyclonetix",
    "title": "Introduction",
    "section": "",
    "text": "Cyclonetix is a lightweight, Rust-based workflow orchestrator designed to be fast, easy to use, and highly scalable. It is built for both cloud-native and on-premises deployments, supporting outcome-based scheduling, self-assembling DAGs, and manual DAG execution.\nWith Cyclonetix, you can:\n\nDefine tasks with dependencies, commands, and parameters\nBuild execution graphs explicitly or let them self-assemble\nSchedule outcomes and let the system determine the execution path\nMonitor executions in real-time through a modern web UI\nScale from development environments to large-scale production deployments",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Introduction",
    "section": "",
    "text": "Cyclonetix aims to be:\n\nEffortlessly simple to get started with\nExceptionally fast and lightweight\nFlexible enough for both beginners and enterprise-scale workloads\nClever by design, reducing user cognitive load\nCapable of advanced scheduling mechanisms (outcome-based scheduling + explicit DAG execution)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#project-philosophy",
    "href": "index.html#project-philosophy",
    "title": "Introduction",
    "section": "",
    "text": "Cyclonetix was designed with several core principles in mind:\n\nReduce cognitive load - Users shouldn’t need to understand complex concepts to orchestrate simple workflows\nStart small, scale big - The same system should work for both local development and massive production deployments\nSimple things should be simple, complex things should be possible - Basic use cases should require minimal effort\nDesign for the real world - Built based on experience with real-world orchestration challenges\nPerformance matters - A lightweight core that executes jobs efficiently with minimal overhead\n\nWhether you’re running data pipelines, ML training jobs, or software deployment processes, Cyclonetix provides the flexibility, performance, and ease of use to make your workflow orchestration simpler and more efficient.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#where-to-go-next",
    "href": "index.html#where-to-go-next",
    "title": "Introduction",
    "section": "",
    "text": "Getting Started - Install and run your first workflow\nCore Concepts - Learn about Cyclonetix’s architecture\nUser Guide - Practical guides for using Cyclonetix\nDeveloper Guide - Contributing to Cyclonetix",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "build-notes/ui_wireframes.html",
    "href": "build-notes/ui_wireframes.html",
    "title": "Cyclonetix UI Design Specification",
    "section": "",
    "text": "Cyclonetix is an orchestration framework requiring an intuitive and efficient UI for managing workflows, tasks, agents, and execution monitoring. The UI will be built using a combination of SSR and ReactFlow, ensuring a responsive and interactive user experience.\n\n\n\n\n\nPurpose: Authenticate users into the system.\n\nFields: Email/Username, Password\nActions: Login Button, Forgot Password Link\nAPI Calls: POST /api/auth/login\nBehavior: Redirects to Dashboard on success; shows error message on failure.\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Login Form] --&gt;|Submit| B{Auth API};\n    B --&gt;|Success| C[Redirect to Dashboard];\n    B --&gt;|Failure| D[Show Error Message];\n\n\n\n\n\n\n\n\n\n\nPurpose: Overview of system state, active workflows, and real-time execution stats.\n\nSections:\n\nSummary Metrics\nTable of Queues (Sparklines for activity trends)\nNavigation Links for DAG filtering\n\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Dashboard] --&gt; B[Summary Metrics];\n    A --&gt; C[Queue Activity Sparklines];\n    A --&gt; D[Running DAGs List];\n    A --&gt; E[Failed DAGs List];\n    A --&gt; F[Completed DAGs List];\n\n\n\n\n\n\n\n\n\n\nPurpose: View, filter, and manage DAGs (predefined & dynamic).\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[DAG List Page] --&gt; B[Search & Filters];\n    A --&gt; C[Table of DAGs];\n    C --&gt;|Click DAG| D[DAG Execution View];\n    C --&gt;|Edit DAG| E[DAG Designer];\n\n\n\n\n\n\n\n\n\n\nPurpose: Display real-time execution of DAGs with a graph-based visualization.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Live DAG Execution] --&gt; B[Cytoscape DAG Graph];\n    B --&gt;|Click Task| C[Task Logs & Execution Details];\n    B --&gt;|Retry Task| D[Task Retry Options];\n\n\n\n\n\n\n\n\n\n\nPurpose: Edit and manage task scripts inline.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Task Editor] --&gt; B[Code Editor Panel];\n    A --&gt; C[Execution Preview];\n    A --&gt; D[Validation & Save];\n\n\n\n\n\n\n\n\n\n\nPurpose: Drag-and-drop DAG creation.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[DAG Designer] --&gt; B[Graph Canvas (CyEditor)];\n    A --&gt; C[Task Properties Panel];\n    A --&gt; D[Save & Validate DAG];\n\n\n\n\n\n\n\n\n\n\nPurpose: Manage stored credentials for DAGs.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Secrets Management] --&gt; B[Stored Secrets List];\n    A --&gt; C[Secret Details & Permissions];\n    A --&gt; D[Add/Edit/Delete Secret];\n\n\n\n\n\n\n\n\n\n\nPurpose: View and control execution agents.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Agent Management] --&gt; B[Agent List & Load Metrics];\n    A --&gt; C[Agent Details];\n    A --&gt; D[Scale/Restart Agent];\n\n\n\n\n\n\n\n\n\n\nPurpose: Monitor task queues and performance.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Queue Inspection] --&gt; B[Active Queue List];\n    A --&gt; C[Pending Tasks & Queue Depth];\n    A --&gt; D[Priority Adjustment];\n\n\n\n\n\n\n\n\n\n\nPurpose: View and modify system-wide configuration settings.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Config Inspection] --&gt; B[Configuration Categories];\n    A --&gt; C[Editable Configurations];\n    A --&gt; D[Save & Apply];\n\n\n\n\n\n\n\n\n\n\nPurpose: Track orchestration engine performance.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[State Management] --&gt; B[System Metrics (Redis, Memory)];\n    A --&gt; C[Execution Latency Graphs];\n    A --&gt; D[Worker Health & Failures];\n\n\n\n\n\n\n\n\n\n\n\nCollapsible Sidebar with icon links to all sections.\nTop Navbar:\n\nDropdown menus for grouped sections.\nDark/Light mode toggle.\nLink to Documentation.\nUser profile menu for preferences.\n\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Sidebar] --&gt;|Icon Links| B[Navigation Pages];\n    A --&gt; C[Dashboard];\n    A --&gt; D[DAGs];\n    A --&gt; E[Agents];\n    A --&gt; F[Queues];\n    A --&gt; G[Experiments];\n    A --&gt; H[Config];\n    A --&gt; I[Metrics];"
  },
  {
    "objectID": "build-notes/ui_wireframes.html#overview",
    "href": "build-notes/ui_wireframes.html#overview",
    "title": "Cyclonetix UI Design Specification",
    "section": "",
    "text": "Cyclonetix is an orchestration framework requiring an intuitive and efficient UI for managing workflows, tasks, agents, and execution monitoring. The UI will be built using a combination of SSR and ReactFlow, ensuring a responsive and interactive user experience."
  },
  {
    "objectID": "build-notes/ui_wireframes.html#core-pages-features",
    "href": "build-notes/ui_wireframes.html#core-pages-features",
    "title": "Cyclonetix UI Design Specification",
    "section": "",
    "text": "Purpose: Authenticate users into the system.\n\nFields: Email/Username, Password\nActions: Login Button, Forgot Password Link\nAPI Calls: POST /api/auth/login\nBehavior: Redirects to Dashboard on success; shows error message on failure.\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Login Form] --&gt;|Submit| B{Auth API};\n    B --&gt;|Success| C[Redirect to Dashboard];\n    B --&gt;|Failure| D[Show Error Message];\n\n\n\n\n\n\n\n\n\n\nPurpose: Overview of system state, active workflows, and real-time execution stats.\n\nSections:\n\nSummary Metrics\nTable of Queues (Sparklines for activity trends)\nNavigation Links for DAG filtering\n\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Dashboard] --&gt; B[Summary Metrics];\n    A --&gt; C[Queue Activity Sparklines];\n    A --&gt; D[Running DAGs List];\n    A --&gt; E[Failed DAGs List];\n    A --&gt; F[Completed DAGs List];\n\n\n\n\n\n\n\n\n\n\nPurpose: View, filter, and manage DAGs (predefined & dynamic).\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[DAG List Page] --&gt; B[Search & Filters];\n    A --&gt; C[Table of DAGs];\n    C --&gt;|Click DAG| D[DAG Execution View];\n    C --&gt;|Edit DAG| E[DAG Designer];\n\n\n\n\n\n\n\n\n\n\nPurpose: Display real-time execution of DAGs with a graph-based visualization.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Live DAG Execution] --&gt; B[Cytoscape DAG Graph];\n    B --&gt;|Click Task| C[Task Logs & Execution Details];\n    B --&gt;|Retry Task| D[Task Retry Options];\n\n\n\n\n\n\n\n\n\n\nPurpose: Edit and manage task scripts inline.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Task Editor] --&gt; B[Code Editor Panel];\n    A --&gt; C[Execution Preview];\n    A --&gt; D[Validation & Save];\n\n\n\n\n\n\n\n\n\n\nPurpose: Drag-and-drop DAG creation.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[DAG Designer] --&gt; B[Graph Canvas (CyEditor)];\n    A --&gt; C[Task Properties Panel];\n    A --&gt; D[Save & Validate DAG];\n\n\n\n\n\n\n\n\n\n\nPurpose: Manage stored credentials for DAGs.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Secrets Management] --&gt; B[Stored Secrets List];\n    A --&gt; C[Secret Details & Permissions];\n    A --&gt; D[Add/Edit/Delete Secret];\n\n\n\n\n\n\n\n\n\n\nPurpose: View and control execution agents.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Agent Management] --&gt; B[Agent List & Load Metrics];\n    A --&gt; C[Agent Details];\n    A --&gt; D[Scale/Restart Agent];\n\n\n\n\n\n\n\n\n\n\nPurpose: Monitor task queues and performance.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Queue Inspection] --&gt; B[Active Queue List];\n    A --&gt; C[Pending Tasks & Queue Depth];\n    A --&gt; D[Priority Adjustment];\n\n\n\n\n\n\n\n\n\n\nPurpose: View and modify system-wide configuration settings.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Config Inspection] --&gt; B[Configuration Categories];\n    A --&gt; C[Editable Configurations];\n    A --&gt; D[Save & Apply];\n\n\n\n\n\n\n\n\n\n\nPurpose: Track orchestration engine performance.\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[State Management] --&gt; B[System Metrics (Redis, Memory)];\n    A --&gt; C[Execution Latency Graphs];\n    A --&gt; D[Worker Health & Failures];\n\n\n\n\n\n\n\n\n\n\n\nCollapsible Sidebar with icon links to all sections.\nTop Navbar:\n\nDropdown menus for grouped sections.\nDark/Light mode toggle.\nLink to Documentation.\nUser profile menu for preferences.\n\n\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\ngraph TD;\n    A[Sidebar] --&gt;|Icon Links| B[Navigation Pages];\n    A --&gt; C[Dashboard];\n    A --&gt; D[DAGs];\n    A --&gt; E[Agents];\n    A --&gt; F[Queues];\n    A --&gt; G[Experiments];\n    A --&gt; H[Config];\n    A --&gt; I[Metrics];"
  },
  {
    "objectID": "core-concepts/tasks.html",
    "href": "core-concepts/tasks.html",
    "title": "Tasks and Dependencies",
    "section": "",
    "text": "Tasks are the fundamental building blocks in Cyclonetix. Understanding how they work and how dependencies between them are managed is essential to effectively use the system.\n\n\nA task in Cyclonetix represents a single unit of work that needs to be executed. Each task has:\n\nA unique identifier\nA command to execute\nOptional dependencies on other tasks\nOptional parameters for customization\nA specified queue for execution\n\n\n\nCyclonetix distinguishes between task definitions and their execution instances:\n\nTask Template: The immutable definition of a task, stored as YAML.\nTask Instance: A specific execution of a task with its runtime state.\n\nWhen a task is scheduled, Cyclonetix creates a Task Instance from the Task Template, generating a unique run ID for tracking.\n\n\n\nEach task goes through a series of states during its lifecycle:\n\nPending: Task is defined but not yet scheduled\nQueued: Task is ready for execution and waiting in a queue\nRunning: Task is currently being executed by an agent\nCompleted: Task finished execution successfully\nFailed: Task execution failed\n\n\n\n\nHere’s an example of a task definition in YAML format:\nid: \"data_preprocessing\"\nname: \"Data Preprocessing\"\ndescription: \"Preprocess raw data for model training\"\ncommand: \"python3 scripts/preprocess.py --input ${INPUT_FILE} --output ${OUTPUT_FILE}\"\nparameters:\n  inputPath: \"/data/raw\"\n  outputPath: \"/data/processed\"\n  mode: \"full\"\ndependencies:\n  - \"data_download\"\nqueue: \"data_processing\"\n\n\n\n\nDependencies define the relationships between tasks, establishing the execution order.\n\n\nThe most common form of dependency is a direct dependency where one task requires another to complete before it can start:\nid: \"model_training\"\n# ...\ndependencies:\n  - \"data_preprocessing\"\nIn this example, model_training will only start after data_preprocessing has successfully completed.\n\n\n\nCyclonetix supports parameterized dependencies, allowing you to depend on a specific variant of a task:\nid: \"model_deployment\"\n# ...\ndependencies:\n  - \"model_training:production\"\nThis creates a dependency on the model_training task specifically when run with the production parameter set.\n\n\n\nWhen using outcome-based scheduling, Cyclonetix automatically resolves the full dependency chain:\n\nIdentify the target task (the “outcome”)\nRecursively gather all dependencies\nBuild a directed acyclic graph (DAG) representing the execution order\nExecute tasks in topological order, respecting dependencies\n\n\n\n\nThe Cyclonetix UI provides visualization of task dependencies as a graph, making it easy to understand workflow structure:\n\n\n\nDependency Graph Example\n\n\n\n\n\n\nTasks can be assigned to specific queues, allowing for:\n\nWorkload categorization\nResource allocation\nParallel execution across multiple agents\n\nid: \"gpu_training\"\n# ...\nqueue: \"gpu_tasks\"\nAgents can be configured to listen to specific queues, enabling specialized workers for different task types.\n\n\n\n\nLearn about Execution Flow\nUnderstand Scheduling Models\nSee how to define Tasks in practice",
    "crumbs": [
      "Core Concepts",
      "Tasks and Dependencies"
    ]
  },
  {
    "objectID": "core-concepts/tasks.html#tasks",
    "href": "core-concepts/tasks.html#tasks",
    "title": "Tasks and Dependencies",
    "section": "",
    "text": "A task in Cyclonetix represents a single unit of work that needs to be executed. Each task has:\n\nA unique identifier\nA command to execute\nOptional dependencies on other tasks\nOptional parameters for customization\nA specified queue for execution\n\n\n\nCyclonetix distinguishes between task definitions and their execution instances:\n\nTask Template: The immutable definition of a task, stored as YAML.\nTask Instance: A specific execution of a task with its runtime state.\n\nWhen a task is scheduled, Cyclonetix creates a Task Instance from the Task Template, generating a unique run ID for tracking.\n\n\n\nEach task goes through a series of states during its lifecycle:\n\nPending: Task is defined but not yet scheduled\nQueued: Task is ready for execution and waiting in a queue\nRunning: Task is currently being executed by an agent\nCompleted: Task finished execution successfully\nFailed: Task execution failed\n\n\n\n\nHere’s an example of a task definition in YAML format:\nid: \"data_preprocessing\"\nname: \"Data Preprocessing\"\ndescription: \"Preprocess raw data for model training\"\ncommand: \"python3 scripts/preprocess.py --input ${INPUT_FILE} --output ${OUTPUT_FILE}\"\nparameters:\n  inputPath: \"/data/raw\"\n  outputPath: \"/data/processed\"\n  mode: \"full\"\ndependencies:\n  - \"data_download\"\nqueue: \"data_processing\"",
    "crumbs": [
      "Core Concepts",
      "Tasks and Dependencies"
    ]
  },
  {
    "objectID": "core-concepts/tasks.html#dependencies",
    "href": "core-concepts/tasks.html#dependencies",
    "title": "Tasks and Dependencies",
    "section": "",
    "text": "Dependencies define the relationships between tasks, establishing the execution order.\n\n\nThe most common form of dependency is a direct dependency where one task requires another to complete before it can start:\nid: \"model_training\"\n# ...\ndependencies:\n  - \"data_preprocessing\"\nIn this example, model_training will only start after data_preprocessing has successfully completed.\n\n\n\nCyclonetix supports parameterized dependencies, allowing you to depend on a specific variant of a task:\nid: \"model_deployment\"\n# ...\ndependencies:\n  - \"model_training:production\"\nThis creates a dependency on the model_training task specifically when run with the production parameter set.\n\n\n\nWhen using outcome-based scheduling, Cyclonetix automatically resolves the full dependency chain:\n\nIdentify the target task (the “outcome”)\nRecursively gather all dependencies\nBuild a directed acyclic graph (DAG) representing the execution order\nExecute tasks in topological order, respecting dependencies\n\n\n\n\nThe Cyclonetix UI provides visualization of task dependencies as a graph, making it easy to understand workflow structure:\n\n\n\nDependency Graph Example",
    "crumbs": [
      "Core Concepts",
      "Tasks and Dependencies"
    ]
  },
  {
    "objectID": "core-concepts/tasks.html#execution-queues",
    "href": "core-concepts/tasks.html#execution-queues",
    "title": "Tasks and Dependencies",
    "section": "",
    "text": "Tasks can be assigned to specific queues, allowing for:\n\nWorkload categorization\nResource allocation\nParallel execution across multiple agents\n\nid: \"gpu_training\"\n# ...\nqueue: \"gpu_tasks\"\nAgents can be configured to listen to specific queues, enabling specialized workers for different task types.",
    "crumbs": [
      "Core Concepts",
      "Tasks and Dependencies"
    ]
  },
  {
    "objectID": "core-concepts/tasks.html#next-steps",
    "href": "core-concepts/tasks.html#next-steps",
    "title": "Tasks and Dependencies",
    "section": "",
    "text": "Learn about Execution Flow\nUnderstand Scheduling Models\nSee how to define Tasks in practice",
    "crumbs": [
      "Core Concepts",
      "Tasks and Dependencies"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html",
    "href": "core-concepts/scheduling-models.html",
    "title": "Scheduling Models",
    "section": "",
    "text": "Cyclonetix supports multiple scheduling models to accommodate different workflow needs and user preferences. Each model has its advantages and use cases.\n\n\nCyclonetix provides two fundamentally different approaches to workflow scheduling:\n\nOutcome-Based Scheduling (Self-assembling DAGs)\nExplicit DAG Execution (Manual DAG definition)\n\n\n\n\nIn outcome-based scheduling, you specify the final task (the “outcome”) you want to achieve, and Cyclonetix automatically determines and executes all necessary prerequisite tasks.\n\n\n\nYou identify a target task (e.g., deploy_model)\nCyclonetix examines the task’s dependencies\nIt recursively resolves all dependencies to build a complete execution graph\nTasks are executed in the correct order to achieve the desired outcome\n\n\n\n\n\nSimplicity: Users only need to specify what they want, not how to get it\nReduced maintenance: Dependencies are defined once, at the task level\nFlexibility: The same tasks can participate in different workflows\nAutomatic updates: Adding a dependency to a task automatically updates all workflows that use it\n\n\n\n\n# Schedule by outcome\n./cyclonetix schedule-task deploy_model\nThis command tells Cyclonetix to ensure the deploy_model task completes successfully, automatically handling any prerequisite tasks like data preparation, model training, and validation.\n\n\n\n\nIn explicit DAG execution, you define a complete Directed Acyclic Graph (DAG) that specifies all tasks and their relationships.\n\n\n\nYou create a DAG definition file specifying all tasks\nThe DAG is scheduled as a unit\nTasks within the DAG are executed according to their dependencies\n\n\n\n\n\nPredictability: The exact execution plan is known in advance\nVersioning: DAGs can be versioned independently of task definitions\nIsolation: Changes to task dependencies don’t affect existing DAGs\nDocumentation: DAG files serve as explicit workflow documentation\n\n\n\n\nDefine a DAG in YAML:\n# model_deployment.yaml\nid: \"model_deployment\"\nname: \"Model Deployment Pipeline\"\ndescription: \"End-to-end pipeline for deploying ML models\"\ntasks:\n  - id: \"data_preparation\"\n  - id: \"feature_engineering\"\n  - id: \"model_training\"\n  - id: \"model_validation\"\n  - id: \"model_deployment\"\ntags: [\"ml\", \"deployment\"]\nSchedule the DAG:\n./cyclonetix schedule-dag model_deployment\n\n\n\n\nCyclonetix allows you to mix and match these approaches:\n\nUse outcome-based scheduling for quick, ad-hoc executions\nUse explicit DAGs for production pipelines and critical workflows\nReference the same underlying tasks with both approaches\n\n\n\n\nIn addition to manual scheduling, Cyclonetix supports event-driven execution:\n\nSchedule tasks or DAGs based on external triggers\nConfigure time-based schedules (like cron jobs)\nReact to data arrival or system events\n\n\n\n\nBoth scheduling models support:\n\nTask prioritization\nQueue assignment\nResource allocation\nConcurrency controls\n\n\n\n\nConsider these factors when deciding which scheduling model to use:\n\n\n\nFactor\nOutcome-Based\nExplicit DAG\n\n\n\n\nWorkflow complexity\nSimple to moderate\nAny complexity\n\n\nChange frequency\nFrequent changes\nStable workflows\n\n\nDocumentation needs\nMinimal\nComprehensive\n\n\nUser expertise\nBeginners friendly\nMore advanced\n\n\nVersioning needs\nTask-level\nWorkflow-level\n\n\n\n\n\n\n\nLearn how to Define Tasks\nUnderstand Building DAGs\nExplore Scheduling Workflows",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html#two-primary-scheduling-models",
    "href": "core-concepts/scheduling-models.html#two-primary-scheduling-models",
    "title": "Scheduling Models",
    "section": "",
    "text": "Cyclonetix provides two fundamentally different approaches to workflow scheduling:\n\nOutcome-Based Scheduling (Self-assembling DAGs)\nExplicit DAG Execution (Manual DAG definition)",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html#outcome-based-scheduling",
    "href": "core-concepts/scheduling-models.html#outcome-based-scheduling",
    "title": "Scheduling Models",
    "section": "",
    "text": "In outcome-based scheduling, you specify the final task (the “outcome”) you want to achieve, and Cyclonetix automatically determines and executes all necessary prerequisite tasks.\n\n\n\nYou identify a target task (e.g., deploy_model)\nCyclonetix examines the task’s dependencies\nIt recursively resolves all dependencies to build a complete execution graph\nTasks are executed in the correct order to achieve the desired outcome\n\n\n\n\n\nSimplicity: Users only need to specify what they want, not how to get it\nReduced maintenance: Dependencies are defined once, at the task level\nFlexibility: The same tasks can participate in different workflows\nAutomatic updates: Adding a dependency to a task automatically updates all workflows that use it\n\n\n\n\n# Schedule by outcome\n./cyclonetix schedule-task deploy_model\nThis command tells Cyclonetix to ensure the deploy_model task completes successfully, automatically handling any prerequisite tasks like data preparation, model training, and validation.",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html#explicit-dag-execution",
    "href": "core-concepts/scheduling-models.html#explicit-dag-execution",
    "title": "Scheduling Models",
    "section": "",
    "text": "In explicit DAG execution, you define a complete Directed Acyclic Graph (DAG) that specifies all tasks and their relationships.\n\n\n\nYou create a DAG definition file specifying all tasks\nThe DAG is scheduled as a unit\nTasks within the DAG are executed according to their dependencies\n\n\n\n\n\nPredictability: The exact execution plan is known in advance\nVersioning: DAGs can be versioned independently of task definitions\nIsolation: Changes to task dependencies don’t affect existing DAGs\nDocumentation: DAG files serve as explicit workflow documentation\n\n\n\n\nDefine a DAG in YAML:\n# model_deployment.yaml\nid: \"model_deployment\"\nname: \"Model Deployment Pipeline\"\ndescription: \"End-to-end pipeline for deploying ML models\"\ntasks:\n  - id: \"data_preparation\"\n  - id: \"feature_engineering\"\n  - id: \"model_training\"\n  - id: \"model_validation\"\n  - id: \"model_deployment\"\ntags: [\"ml\", \"deployment\"]\nSchedule the DAG:\n./cyclonetix schedule-dag model_deployment",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html#combining-both-approaches",
    "href": "core-concepts/scheduling-models.html#combining-both-approaches",
    "title": "Scheduling Models",
    "section": "",
    "text": "Cyclonetix allows you to mix and match these approaches:\n\nUse outcome-based scheduling for quick, ad-hoc executions\nUse explicit DAGs for production pipelines and critical workflows\nReference the same underlying tasks with both approaches",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html#event-driven-scheduling",
    "href": "core-concepts/scheduling-models.html#event-driven-scheduling",
    "title": "Scheduling Models",
    "section": "",
    "text": "In addition to manual scheduling, Cyclonetix supports event-driven execution:\n\nSchedule tasks or DAGs based on external triggers\nConfigure time-based schedules (like cron jobs)\nReact to data arrival or system events",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html#priority-and-resource-allocation",
    "href": "core-concepts/scheduling-models.html#priority-and-resource-allocation",
    "title": "Scheduling Models",
    "section": "",
    "text": "Both scheduling models support:\n\nTask prioritization\nQueue assignment\nResource allocation\nConcurrency controls",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html#choosing-the-right-model",
    "href": "core-concepts/scheduling-models.html#choosing-the-right-model",
    "title": "Scheduling Models",
    "section": "",
    "text": "Consider these factors when deciding which scheduling model to use:\n\n\n\nFactor\nOutcome-Based\nExplicit DAG\n\n\n\n\nWorkflow complexity\nSimple to moderate\nAny complexity\n\n\nChange frequency\nFrequent changes\nStable workflows\n\n\nDocumentation needs\nMinimal\nComprehensive\n\n\nUser expertise\nBeginners friendly\nMore advanced\n\n\nVersioning needs\nTask-level\nWorkflow-level",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "core-concepts/scheduling-models.html#next-steps",
    "href": "core-concepts/scheduling-models.html#next-steps",
    "title": "Scheduling Models",
    "section": "",
    "text": "Learn how to Define Tasks\nUnderstand Building DAGs\nExplore Scheduling Workflows",
    "crumbs": [
      "Core Concepts",
      "Scheduling Models"
    ]
  },
  {
    "objectID": "cookbook.html",
    "href": "cookbook.html",
    "title": "Cookbook",
    "section": "",
    "text": "This cookbook provides practical recipes for implementing common workflow patterns with Cyclonetix. Each recipe includes task definitions, DAG configurations, and execution instructions to help you solve real-world orchestration challenges.\n\n\n\nAI-Driven Research Pipeline\nLow-Latency Trade Pricing with Kafka\nStandard ETL Pipeline\nModel Training and Deployment Pipeline\nAgents on Cloud Run\nMulti-Cloud Deployment\n\n\n\n\nThis recipe demonstrates how to implement an AI-driven research workflow with dynamic decision-making. The pattern consists of an initial research phase that gathers information, followed by an evaluation point that determines which subsequent analysis paths to take.\n\n\n\nInitial Research Phase: Collects and processes information\nEvaluation Point: Analyzes results and decides on further actions\nSpecialized Analysis Paths: Different analytical workflows triggered based on evaluation\n\n\n\n\nFirst, let’s define the core research tasks:\n# data/tasks/research/collect_data.yaml\nid: \"collect_data\"\nname: \"Collect Research Data\"\ncommand: \"python research/collect.py --source ${DATA_SOURCE} --output ${OUTPUT_PATH}\"\ndependencies: []\nparameters:\n  DATA_SOURCE: \"api\"\n  OUTPUT_PATH: \"/data/research/raw\"\n  QUERY: \"default query\"\n# data/tasks/research/process_data.yaml\nid: \"process_data\"\nname: \"Process Research Data\"\ncommand: \"python research/process.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\ndependencies:\n  - \"collect_data\"\nparameters:\n  INPUT_PATH: \"/data/research/raw\"\n  OUTPUT_PATH: \"/data/research/processed\"\nNow, let’s create the evaluation point task using Langchain and GPT:\n# data/tasks/research/evaluate_findings.yaml\nid: \"evaluate_findings\"\nname: \"Evaluate Research Findings with GPT\"\ncommand: \"python research/evaluate_with_gpt.py --input ${INPUT_PATH} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"process_data\"\nevaluation_point: true\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  GPT_MODEL: \"gpt-4\"\n  OPENAI_API_KEY: \"${OPENAI_API_KEY}\"\n  CONFIDENCE_THRESHOLD: \"0.7\"\nThe evaluate_with_gpt.py script uses Langchain and GPT to analyze research findings:\nimport json\nimport sys\nimport os\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.output_parsers import PydanticOutputParser\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\n# Define the output structure\nclass AnalysisDecision(BaseModel):\n    topics: List[str] = Field(description=\"List of detected topics in the research\")\n    confidence_score: float = Field(description=\"Overall confidence score (0-1)\")\n    recommended_analyses: List[str] = Field(description=\"List of recommended analysis types to perform\")\n    summary: str = Field(description=\"Brief summary of the key findings\")\n    additional_context: dict = Field(description=\"Additional context to pass to subsequent analyses\")\n\n# Set up the parser\nparser = PydanticOutputParser(pydantic_object=AnalysisDecision)\n\n# Create the prompt template\ntemplate = \"\"\"\nYou are a research analysis expert. Based on the following research findings, please:\n1. Identify the main topics covered\n2. Assess the overall confidence and quality of the findings\n3. Recommend which specialized analyses should be conducted\n4. Provide a brief summary of the key points\n5. Suggest additional context that would be valuable for subsequent analyses\n\nResearch findings:\n{research_findings}\n\n{format_instructions}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(\n    template=template,\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\ndef evaluate_with_gpt(input_path, api_key, model_name, confidence_threshold):\n    # Load research findings\n    with open(input_path, 'r') as f:\n        research_findings = f.read()\n\n    # Initialize the LLM\n    llm = ChatOpenAI(temperature=0, model_name=model_name, openai_api_key=api_key)\n\n    # Format the prompt with our research findings\n    messages = prompt.format_messages(research_findings=research_findings)\n\n    # Get a response from the LLM\n    llm_response = llm(messages)\n\n    # Parse the response\n    analysis = parser.parse(llm_response.content)\n\n    # Build result for Cyclonetix\n    result = {\n        \"metadata\": {\n            \"confidence_score\": analysis.confidence_score,\n            \"topics\": analysis.topics,\n            \"summary\": analysis.summary\n        },\n        \"context_updates\": analysis.additional_context\n    }\n\n    # Determine which subsequent analyses to run based on GPT's recommendations\n    next_tasks = []\n\n    # Only proceed if confidence is above threshold\n    if analysis.confidence_score &gt;= float(confidence_threshold):\n        for analysis_type in analysis.recommended_analyses:\n            # Map recommended analyses to task IDs\n            if analysis_type.lower() == \"deep\" or analysis_type.lower() == \"detailed\":\n                next_tasks.append(\"deep_analysis\")\n            elif analysis_type.lower() == \"financial\":\n                next_tasks.append(\"financial_analysis\")\n            elif analysis_type.lower() == \"technical\":\n                next_tasks.append(\"technical_analysis\")\n            elif analysis_type.lower() == \"competitive\" or analysis_type.lower() == \"competitor\":\n                next_tasks.append(\"competitor_analysis\")\n\n    result[\"next_tasks\"] = next_tasks\n\n    return result\n\ndef main():\n    input_path = os.environ.get(\"INPUT_PATH\")\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    model_name = os.environ.get(\"GPT_MODEL\", \"gpt-4\")\n    confidence_threshold = os.environ.get(\"CONFIDENCE_THRESHOLD\", \"0.7\")\n\n    result = evaluate_with_gpt(input_path, api_key, model_name, confidence_threshold)\n\n    eval_result_path = os.environ.get(\"CYCLO_EVAL_RESULT\", \"eval_result.json\")\n    with open(eval_result_path, 'w') as f:\n        json.dump(result, f)\n\n    # Exit with status code 0 for success\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\nFinally, define the specialized analysis tasks:\n# data/tasks/research/deep_analysis.yaml\nid: \"deep_analysis\"\nname: \"Deep Analysis\"\ncommand: \"python research/deep_analysis.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  OUTPUT_PATH: \"/data/research/deep_analysis\"\n# data/tasks/research/financial_analysis.yaml\nid: \"financial_analysis\"\nname: \"Financial Analysis\"\ncommand: \"python research/financial_analysis.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  OUTPUT_PATH: \"/data/research/financial_analysis\"\n# data/tasks/research/technical_analysis.yaml\nid: \"technical_analysis\"\nname: \"Technical Analysis\"\ncommand: \"python research/technical_analysis.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  OUTPUT_PATH: \"/data/research/technical_analysis\"\n# data/tasks/research/competitor_analysis.yaml\nid: \"competitor_analysis\"\nname: \"Competitor Analysis\"\ncommand: \"python research/competitor_analysis.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  OUTPUT_PATH: \"/data/research/competitor_analysis\"\n\n\n\nCreate a DAG for the research pipeline:\n# data/dags/ai_research_pipeline.yaml\nid: \"ai_research_pipeline\"\nname: \"AI-Driven Research Pipeline\"\ndescription: \"Dynamic research workflow with AI-based path selection\"\ntasks:\n  - id: \"collect_data\"\n  - id: \"process_data\"\n  - id: \"evaluate_findings\"\ntags: [\"research\", \"ai\", \"dynamic\"]\nWe would create other DAGs for the specialized analysis paths, such as deep_analysis, financial_analysis, etc.\n\n\n\nOne of the key advantages of this approach is that the context from the research pipeline is automatically propagated to the subsequent DAGs. In the evaluation point, we add context updates:\nresult = {\n    \"metadata\": {\n        \"confidence_score\": analysis.confidence_score,\n        \"topics\": analysis.topics,\n        \"summary\": analysis.summary\n    },\n    \"context_updates\": analysis.additional_context  # This propagates to downstream tasks\n}\nThese context updates become available as environment variables in all subsequent tasks. For example, the financial analysis task could access GPT-identified financial metrics that weren’t explicitly defined in the original task parameters:\n# In financial_analysis.py\nimport os\n\n# Access context propagated from GPT evaluation\nkey_financial_metrics = os.environ.get(\"KEY_FINANCIAL_METRICS\")\nrelevant_companies = os.environ.get(\"RELEVANT_COMPANIES\")\ntime_period = os.environ.get(\"ANALYSIS_TIME_PERIOD\")\n\n# Use these context variables in the analysis\n\n\n\nSchedule the research pipeline:\n./cyclonetix schedule-dag ai_research_pipeline \\\n  --param collect_data.QUERY=\"AI trends in financial services\" \\\n  --param evaluate_findings.OPENAI_API_KEY=\"your-openai-api-key\"\nThe workflow will: 1. Collect and process the research data 2. Use Langchain and GPT to evaluate the findings and decide on next steps 3. Dynamically execute only the relevant analysis paths based on GPT’s recommendations 4. Propagate GPT-generated context to all downstream tasks\nThis pattern can be extended with: - Additional evaluation points after each analysis that use GPT to refine the research direction - Aggregation steps that combine results from multiple analyses - Notification tasks that alert researchers about significant findings - Iterative research loops where GPT can request additional information collection\n\n\n\n\nThis recipe demonstrates how to build a high-performance trade pricing system with Cyclonetix and Kafka, focusing on low latency and high throughput.\n\n\n\nKafka Consumer: On the orchestrator, listens for incoming pricing requests and adds to relevant agent queue\nPricing Engine: Calculates prices for trade requests and publishes results back to Kafka\nGPU Pricing Engine: Agent running on system with GPU for highly parallelized pricing calculations e.g., Monte Carlo simulations, calculates results and publishes back to Kafka\n\n\n\n\nFirst, let’s define a pricing request consumer task:\n# data/tasks/pricing/calculate_price.yaml\nid: \"calculate_price\"\nname: \"Calculate Trade Price\"\ncommand: \"rust-pricer --trade-id ${TRADE_ID} --instrument ${INSTRUMENT} --quantity ${QUANTITY} --side ${SIDE}\"\ndependencies: []\nparameters:\n  TRADE_ID: \"\"\n  INSTRUMENT: \"\"\n  QUANTITY: \"\"\n  SIDE: \"BUY\"\n  MARKET_DATA_SERVICE: \"http://market-data:8080\"\nqueue: \"pricing\"\nNow let’s define a GPU-accelerated pricing task:\n# data/tasks/pricing/calculate_price.yaml\nid: \"calculate_price\"\nname: \"Calculate Portfolio Price (GPU)\"\ncommand: \"rust-wgpu-pricer --portfolio ${PORTFOLIO_ID} --simulations ${SIMULATIONS} --mu ${MU} --sigma ${SIGMA}\"\ndependencies: []\nparameters:\n  MARKET_DATA_SERVICE: \"http://market-data:8080\"\nqueue: \"gpu-pricing\"",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#contents",
    "href": "cookbook.html#contents",
    "title": "Cookbook",
    "section": "",
    "text": "AI-Driven Research Pipeline\nLow-Latency Trade Pricing with Kafka\nStandard ETL Pipeline\nModel Training and Deployment Pipeline\nAgents on Cloud Run\nMulti-Cloud Deployment",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#ai-driven-research-pipeline",
    "href": "cookbook.html#ai-driven-research-pipeline",
    "title": "Cookbook",
    "section": "",
    "text": "This recipe demonstrates how to implement an AI-driven research workflow with dynamic decision-making. The pattern consists of an initial research phase that gathers information, followed by an evaluation point that determines which subsequent analysis paths to take.\n\n\n\nInitial Research Phase: Collects and processes information\nEvaluation Point: Analyzes results and decides on further actions\nSpecialized Analysis Paths: Different analytical workflows triggered based on evaluation\n\n\n\n\nFirst, let’s define the core research tasks:\n# data/tasks/research/collect_data.yaml\nid: \"collect_data\"\nname: \"Collect Research Data\"\ncommand: \"python research/collect.py --source ${DATA_SOURCE} --output ${OUTPUT_PATH}\"\ndependencies: []\nparameters:\n  DATA_SOURCE: \"api\"\n  OUTPUT_PATH: \"/data/research/raw\"\n  QUERY: \"default query\"\n# data/tasks/research/process_data.yaml\nid: \"process_data\"\nname: \"Process Research Data\"\ncommand: \"python research/process.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\ndependencies:\n  - \"collect_data\"\nparameters:\n  INPUT_PATH: \"/data/research/raw\"\n  OUTPUT_PATH: \"/data/research/processed\"\nNow, let’s create the evaluation point task using Langchain and GPT:\n# data/tasks/research/evaluate_findings.yaml\nid: \"evaluate_findings\"\nname: \"Evaluate Research Findings with GPT\"\ncommand: \"python research/evaluate_with_gpt.py --input ${INPUT_PATH} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"process_data\"\nevaluation_point: true\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  GPT_MODEL: \"gpt-4\"\n  OPENAI_API_KEY: \"${OPENAI_API_KEY}\"\n  CONFIDENCE_THRESHOLD: \"0.7\"\nThe evaluate_with_gpt.py script uses Langchain and GPT to analyze research findings:\nimport json\nimport sys\nimport os\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.output_parsers import PydanticOutputParser\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\n# Define the output structure\nclass AnalysisDecision(BaseModel):\n    topics: List[str] = Field(description=\"List of detected topics in the research\")\n    confidence_score: float = Field(description=\"Overall confidence score (0-1)\")\n    recommended_analyses: List[str] = Field(description=\"List of recommended analysis types to perform\")\n    summary: str = Field(description=\"Brief summary of the key findings\")\n    additional_context: dict = Field(description=\"Additional context to pass to subsequent analyses\")\n\n# Set up the parser\nparser = PydanticOutputParser(pydantic_object=AnalysisDecision)\n\n# Create the prompt template\ntemplate = \"\"\"\nYou are a research analysis expert. Based on the following research findings, please:\n1. Identify the main topics covered\n2. Assess the overall confidence and quality of the findings\n3. Recommend which specialized analyses should be conducted\n4. Provide a brief summary of the key points\n5. Suggest additional context that would be valuable for subsequent analyses\n\nResearch findings:\n{research_findings}\n\n{format_instructions}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(\n    template=template,\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\ndef evaluate_with_gpt(input_path, api_key, model_name, confidence_threshold):\n    # Load research findings\n    with open(input_path, 'r') as f:\n        research_findings = f.read()\n\n    # Initialize the LLM\n    llm = ChatOpenAI(temperature=0, model_name=model_name, openai_api_key=api_key)\n\n    # Format the prompt with our research findings\n    messages = prompt.format_messages(research_findings=research_findings)\n\n    # Get a response from the LLM\n    llm_response = llm(messages)\n\n    # Parse the response\n    analysis = parser.parse(llm_response.content)\n\n    # Build result for Cyclonetix\n    result = {\n        \"metadata\": {\n            \"confidence_score\": analysis.confidence_score,\n            \"topics\": analysis.topics,\n            \"summary\": analysis.summary\n        },\n        \"context_updates\": analysis.additional_context\n    }\n\n    # Determine which subsequent analyses to run based on GPT's recommendations\n    next_tasks = []\n\n    # Only proceed if confidence is above threshold\n    if analysis.confidence_score &gt;= float(confidence_threshold):\n        for analysis_type in analysis.recommended_analyses:\n            # Map recommended analyses to task IDs\n            if analysis_type.lower() == \"deep\" or analysis_type.lower() == \"detailed\":\n                next_tasks.append(\"deep_analysis\")\n            elif analysis_type.lower() == \"financial\":\n                next_tasks.append(\"financial_analysis\")\n            elif analysis_type.lower() == \"technical\":\n                next_tasks.append(\"technical_analysis\")\n            elif analysis_type.lower() == \"competitive\" or analysis_type.lower() == \"competitor\":\n                next_tasks.append(\"competitor_analysis\")\n\n    result[\"next_tasks\"] = next_tasks\n\n    return result\n\ndef main():\n    input_path = os.environ.get(\"INPUT_PATH\")\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    model_name = os.environ.get(\"GPT_MODEL\", \"gpt-4\")\n    confidence_threshold = os.environ.get(\"CONFIDENCE_THRESHOLD\", \"0.7\")\n\n    result = evaluate_with_gpt(input_path, api_key, model_name, confidence_threshold)\n\n    eval_result_path = os.environ.get(\"CYCLO_EVAL_RESULT\", \"eval_result.json\")\n    with open(eval_result_path, 'w') as f:\n        json.dump(result, f)\n\n    # Exit with status code 0 for success\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\nFinally, define the specialized analysis tasks:\n# data/tasks/research/deep_analysis.yaml\nid: \"deep_analysis\"\nname: \"Deep Analysis\"\ncommand: \"python research/deep_analysis.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  OUTPUT_PATH: \"/data/research/deep_analysis\"\n# data/tasks/research/financial_analysis.yaml\nid: \"financial_analysis\"\nname: \"Financial Analysis\"\ncommand: \"python research/financial_analysis.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  OUTPUT_PATH: \"/data/research/financial_analysis\"\n# data/tasks/research/technical_analysis.yaml\nid: \"technical_analysis\"\nname: \"Technical Analysis\"\ncommand: \"python research/technical_analysis.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  OUTPUT_PATH: \"/data/research/technical_analysis\"\n# data/tasks/research/competitor_analysis.yaml\nid: \"competitor_analysis\"\nname: \"Competitor Analysis\"\ncommand: \"python research/competitor_analysis.py --input ${INPUT_PATH} --output ${OUTPUT_PATH}\"\nparameters:\n  INPUT_PATH: \"/data/research/processed\"\n  OUTPUT_PATH: \"/data/research/competitor_analysis\"\n\n\n\nCreate a DAG for the research pipeline:\n# data/dags/ai_research_pipeline.yaml\nid: \"ai_research_pipeline\"\nname: \"AI-Driven Research Pipeline\"\ndescription: \"Dynamic research workflow with AI-based path selection\"\ntasks:\n  - id: \"collect_data\"\n  - id: \"process_data\"\n  - id: \"evaluate_findings\"\ntags: [\"research\", \"ai\", \"dynamic\"]\nWe would create other DAGs for the specialized analysis paths, such as deep_analysis, financial_analysis, etc.\n\n\n\nOne of the key advantages of this approach is that the context from the research pipeline is automatically propagated to the subsequent DAGs. In the evaluation point, we add context updates:\nresult = {\n    \"metadata\": {\n        \"confidence_score\": analysis.confidence_score,\n        \"topics\": analysis.topics,\n        \"summary\": analysis.summary\n    },\n    \"context_updates\": analysis.additional_context  # This propagates to downstream tasks\n}\nThese context updates become available as environment variables in all subsequent tasks. For example, the financial analysis task could access GPT-identified financial metrics that weren’t explicitly defined in the original task parameters:\n# In financial_analysis.py\nimport os\n\n# Access context propagated from GPT evaluation\nkey_financial_metrics = os.environ.get(\"KEY_FINANCIAL_METRICS\")\nrelevant_companies = os.environ.get(\"RELEVANT_COMPANIES\")\ntime_period = os.environ.get(\"ANALYSIS_TIME_PERIOD\")\n\n# Use these context variables in the analysis\n\n\n\nSchedule the research pipeline:\n./cyclonetix schedule-dag ai_research_pipeline \\\n  --param collect_data.QUERY=\"AI trends in financial services\" \\\n  --param evaluate_findings.OPENAI_API_KEY=\"your-openai-api-key\"\nThe workflow will: 1. Collect and process the research data 2. Use Langchain and GPT to evaluate the findings and decide on next steps 3. Dynamically execute only the relevant analysis paths based on GPT’s recommendations 4. Propagate GPT-generated context to all downstream tasks\nThis pattern can be extended with: - Additional evaluation points after each analysis that use GPT to refine the research direction - Aggregation steps that combine results from multiple analyses - Notification tasks that alert researchers about significant findings - Iterative research loops where GPT can request additional information collection",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#low-latency-trade-pricing-with-kafka",
    "href": "cookbook.html#low-latency-trade-pricing-with-kafka",
    "title": "Cookbook",
    "section": "",
    "text": "This recipe demonstrates how to build a high-performance trade pricing system with Cyclonetix and Kafka, focusing on low latency and high throughput.\n\n\n\nKafka Consumer: On the orchestrator, listens for incoming pricing requests and adds to relevant agent queue\nPricing Engine: Calculates prices for trade requests and publishes results back to Kafka\nGPU Pricing Engine: Agent running on system with GPU for highly parallelized pricing calculations e.g., Monte Carlo simulations, calculates results and publishes back to Kafka\n\n\n\n\nFirst, let’s define a pricing request consumer task:\n# data/tasks/pricing/calculate_price.yaml\nid: \"calculate_price\"\nname: \"Calculate Trade Price\"\ncommand: \"rust-pricer --trade-id ${TRADE_ID} --instrument ${INSTRUMENT} --quantity ${QUANTITY} --side ${SIDE}\"\ndependencies: []\nparameters:\n  TRADE_ID: \"\"\n  INSTRUMENT: \"\"\n  QUANTITY: \"\"\n  SIDE: \"BUY\"\n  MARKET_DATA_SERVICE: \"http://market-data:8080\"\nqueue: \"pricing\"\nNow let’s define a GPU-accelerated pricing task:\n# data/tasks/pricing/calculate_price.yaml\nid: \"calculate_price\"\nname: \"Calculate Portfolio Price (GPU)\"\ncommand: \"rust-wgpu-pricer --portfolio ${PORTFOLIO_ID} --simulations ${SIMULATIONS} --mu ${MU} --sigma ${SIGMA}\"\ndependencies: []\nparameters:\n  MARKET_DATA_SERVICE: \"http://market-data:8080\"\nqueue: \"gpu-pricing\"",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#standard-etl-pipeline",
    "href": "cookbook.html#standard-etl-pipeline",
    "title": "Cookbook",
    "section": "Standard ETL Pipeline",
    "text": "Standard ETL Pipeline\nThis recipe demonstrates a typical Extract, Transform, Load (ETL) pipeline implemented with Cyclonetix.\n\nComponents\n\nExtract: Pull data from source systems\nTransform: Clean, validate, and transform the data\nLoad: Load processed data into target systems\nValidation: Verify data quality and integrity\n\n\n\nTask Definitions\nLet’s define the extraction tasks:\n# data/tasks/etl/extract_customer_data.yaml\nid: \"extract_customer_data\"\nname: \"Extract Customer Data\"\ncommand: \"python etl/extract.py --source ${SOURCE} --type customer --output ${OUTPUT_PATH}\"\ndependencies: []\nparameters:\n  SOURCE: \"postgres://user:pass@db:5432/source_db\"\n  OUTPUT_PATH: \"/data/raw/customers\"\n  BATCH_SIZE: 10000\nqueue: \"extract\"\n# data/tasks/etl/extract_order_data.yaml\nid: \"extract_order_data\"\nname: \"Extract Order Data\"\ncommand: \"python etl/extract.py --source ${SOURCE} --type order --output ${OUTPUT_PATH}\"\ndependencies: []\nparameters:\n  SOURCE: \"postgres://user:pass@db:5432/source_db\"\n  OUTPUT_PATH: \"/data/raw/orders\"\n  BATCH_SIZE: 10000\nqueue: \"extract\"\nNext, define the validation task:\n# data/tasks/etl/validate_raw_data.yaml\nid: \"validate_raw_data\"\nname: \"Validate Raw Data\"\ncommand: \"python etl/validate.py --customers ${CUSTOMERS_PATH} --orders ${ORDERS_PATH} --output ${OUTPUT_PATH}\"\ndependencies:\n  - \"extract_customer_data\"\n  - \"extract_order_data\"\nparameters:\n  CUSTOMERS_PATH: \"/data/raw/customers\"\n  ORDERS_PATH: \"/data/raw/orders\"\n  OUTPUT_PATH: \"/data/validated\"\n  RULES_CONFIG: \"/config/validation_rules.json\"\nevaluation_point: true\nqueue: \"transform\"\nThe validation script should output a result indicating whether to proceed or handle errors:\n# Example validation logic\nimport json\nimport os\nimport sys\n\ndef validate_customers(customers_path, rules_config):\n    # Implementation of customer data validation\n    # Returns (is_valid, errors)\n    # ...\n    return True, []\n\ndef validate_orders(orders_path, rules_config):\n    # Implementation of order data validation\n    # Returns (is_valid, errors)\n    # ...\n    return True, []\n\ndef validate_data(customers_path, orders_path, rules_config, output_path):\n    # Perform validation checks\n    customers_valid, customers_errors = validate_customers(customers_path, rules_config)\n    orders_valid, orders_errors = validate_orders(orders_path, rules_config)\n\n    # Write validation results\n    validation_result = {\n        \"customers_valid\": customers_valid,\n        \"orders_valid\": orders_valid,\n        \"customers_errors\": customers_errors,\n        \"orders_errors\": orders_errors\n    }\n\n    with open(os.path.join(output_path, \"validation_results.json\"), \"w\") as f:\n        json.dump(validation_result, f)\n\n    # Determine next steps\n    if customers_valid and orders_valid:\n        return {\n            \"next_tasks\": [\"transform_data\"],\n            \"metadata\": {\n                \"validation_status\": \"passed\",\n                \"error_count\": 0\n            }\n        }\n    else:\n        return {\n            \"next_tasks\": [\"handle_validation_errors\"],\n            \"parameters\": {\n                \"handle_validation_errors\": {\n                    \"ERRORS_PATH\": os.path.join(output_path, \"validation_results.json\")\n                }\n            },\n            \"metadata\": {\n                \"validation_status\": \"failed\",\n                \"error_count\": len(customers_errors) + len(orders_errors)\n            }\n        }\n\n# Write result to CYCLO_EVAL_RESULT\nresult = validate_data(\n    os.environ[\"CUSTOMERS_PATH\"],\n    os.environ[\"ORDERS_PATH\"],\n    os.environ[\"RULES_CONFIG\"],\n    os.environ[\"OUTPUT_PATH\"]\n)\n\nwith open(os.environ[\"CYCLO_EVAL_RESULT\"], \"w\") as f:\n    json.dump(result, f)\n\n# Exit with appropriate code\nsys.exit(0 if result[\"metadata\"][\"validation_status\"] == \"passed\" else 1)\nDefine transformation and loading tasks:\n# data/tasks/etl/transform_data.yaml\nid: \"transform_data\"\nname: \"Transform Data\"\ncommand: \"python etl/transform.py --customers ${CUSTOMERS_PATH} --orders ${ORDERS_PATH} --output ${OUTPUT_PATH}\"\ndependencies:\n  - \"validate_raw_data\"\nparameters:\n  CUSTOMERS_PATH: \"/data/validated/customers\"\n  ORDERS_PATH: \"/data/validated/orders\"\n  OUTPUT_PATH: \"/data/transformed\"\n  TRANSFORMATION_CONFIG: \"/config/transformation_rules.json\"\nqueue: \"transform\"\n# data/tasks/etl/handle_validation_errors.yaml\nid: \"handle_validation_errors\"\nname: \"Handle Validation Errors\"\ncommand: \"python etl/handle_errors.py --errors ${ERRORS_PATH} --notification-endpoint ${NOTIFICATION_ENDPOINT}\"\ndependencies:\n  - \"validate_raw_data\"\nparameters:\n  ERRORS_PATH: \"\"  # Will be set by the evaluation point\n  NOTIFICATION_ENDPOINT: \"http://notification-service:8080/api/notify\"\nqueue: \"error_handling\"\n# data/tasks/etl/load_data.yaml\nid: \"load_data\"\nname: \"Load Transformed Data\"\ncommand: \"python etl/load.py --input ${INPUT_PATH} --target ${TARGET_CONNECTION} --mode ${LOAD_MODE}\"\ndependencies:\n  - \"transform_data\"\nparameters:\n  INPUT_PATH: \"/data/transformed\"\n  TARGET_CONNECTION: \"postgres://user:pass@warehouse:5432/dw\"\n  LOAD_MODE: \"append\"\nqueue: \"load\"\n\n\nDAG Definition\nCreate a DAG for the ETL pipeline:\n# data/dags/daily_etl_pipeline.yaml\nid: \"daily_etl_pipeline\"\nname: \"Daily ETL Pipeline\"\ndescription: \"Daily ETL process for customer and order data\"\ntasks:\n  - id: \"extract_customer_data\"\n  - id: \"extract_order_data\"\n  - id: \"validate_raw_data\"\n  - id: \"transform_data\"\n  - id: \"handle_validation_errors\"\n  - id: \"load_data\"\nschedule:\n  cron: \"0 2 * * *\"  # Run daily at 2:00 AM\n  timezone: \"UTC\"\n  catchup: false\ncontext: \"production\"\ntags: [\"etl\", \"daily\", \"production\"]\n\n\nExecution\nSchedule the ETL pipeline:\n./cyclonetix schedule-dag daily_etl_pipeline\nThe ETL pipeline demonstrates: - Parallel data extraction from multiple sources - Data validation with conditional error handling - Transformation of validated data - Loading into target systems\nThis pattern can be extended with: - Additional validation checks and quality gates - Data reconciliation steps - Notifications and alerts - Incremental loading strategies - Multi-source data synchronization",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#model-training-and-deployment-pipeline",
    "href": "cookbook.html#model-training-and-deployment-pipeline",
    "title": "Cookbook",
    "section": "Model Training and Deployment Pipeline",
    "text": "Model Training and Deployment Pipeline\nThis recipe demonstrates a machine learning workflow with dynamic hyperparameter tuning, comprehensive evaluation, and selective deployment.\n\nComponents\n\nData Preparation: Extracts and prepares training data\nHyperparameter Optimization: Dynamically searches for optimal parameters\nModel Training: Trains models with selected parameters\nEvaluation and Approval: Evaluates models and approves deployment\nDeployment: Deploys models to different environments\n\n\n\nTask Definitions\nFirst, let’s define the data preparation tasks:\n# data/tasks/ml/prepare_data.yaml\nid: \"prepare_data\"\nname: \"Prepare Training Data\"\ncommand: \"python ml/prepare_data.py --source ${DATA_SOURCE} --output ${OUTPUT_PATH} --split ${TRAIN_TEST_SPLIT}\"\ndependencies: []\nparameters:\n  DATA_SOURCE: \"s3://data-bucket/raw/data.parquet\"\n  OUTPUT_PATH: \"/data/ml/prepared\"\n  TRAIN_TEST_SPLIT: \"0.8\"\n  FEATURES_CONFIG: \"/config/features.json\"\nNext, define a hyperparameter search task:\n# data/tasks/ml/hyperparameter_search.yaml\nid: \"hyperparameter_search\"\nname: \"Hyperparameter Search\"\ncommand: \"python ml/hyperparameter_search.py --data ${DATA_PATH} --output ${OUTPUT_PATH} --trials ${NUM_TRIALS}\"\ndependencies:\n  - \"prepare_data\"\nparameters:\n  DATA_PATH: \"/data/ml/prepared\"\n  OUTPUT_PATH: \"/data/ml/hyperparams\"\n  NUM_TRIALS: \"20\"\n  SEARCH_SPACE: \"/config/search_space.json\"\n  METRIC: \"f1_score\"\nevaluation_point: true\nThe hyperparameter search script will test multiple configurations and determine the best ones to use:\nimport json\nimport os\nimport sys\nimport numpy as np\nimport optuna\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nimport pickle\n\ndef objective(trial, X_train, y_train, X_test, y_test):\n    # Define hyperparameter search space\n    n_estimators = trial.suggest_int('n_estimators', 10, 100)\n    max_depth = trial.suggest_int('max_depth', 3, 15)\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n\n    # Train model\n    clf = RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        random_state=42\n    )\n    clf.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = clf.predict(X_test)\n    score = f1_score(y_test, y_pred, average='weighted')\n    return score\n\ndef run_hyperparameter_search(data_path, output_path, num_trials, metric):\n    # Load data\n    with open(os.path.join(data_path, \"train.pkl\"), \"rb\") as f:\n        X_train, y_train = pickle.load(f)\n\n    with open(os.path.join(data_path, \"test.pkl\"), \"rb\") as f:\n        X_test, y_test = pickle.load(f)\n\n    # Create study\n    study = optuna.create_study(direction='maximize')\n    study.optimize(\n        lambda trial: objective(trial, X_train, y_train, X_test, y_test),\n        n_trials=num_trials\n    )\n\n    # Get the top 3 trials\n    top_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:3]\n\n    # Prepare results\n    results = []\n    for i, trial in enumerate(top_trials):\n        model_name = f\"model_{i+1}\"\n        params = trial.params\n        score = trial.value\n\n        results.append({\n            \"model_name\": model_name,\n            \"params\": params,\n            \"score\": score\n        })\n\n        # Save hyperparameter set\n        with open(os.path.join(output_path, f\"{model_name}.json\"), \"w\") as f:\n            json.dump(params, f, indent=2)\n\n    return results\n\ndef main():\n    data_path = os.environ.get(\"DATA_PATH\")\n    output_path = os.environ.get(\"OUTPUT_PATH\")\n    num_trials = int(os.environ.get(\"NUM_TRIALS\", \"20\"))\n    metric = os.environ.get(\"METRIC\", \"f1_score\")\n\n    os.makedirs(output_path, exist_ok=True)\n\n    results = run_hyperparameter_search(data_path, output_path, num_trials, metric)\n\n    # Determine which models to train based on scores\n    models_to_train = []\n    for result in results:\n        if result[\"score\"] &gt; 0.7:  # Only proceed with models that meet threshold\n            models_to_train.append(result[\"model_name\"])\n\n    result = {\n        \"next_tasks\": [\"train_model\"] * len(models_to_train),\n        \"parameters\": {\n            f\"train_model_{i}\": {\n                \"MODEL_NAME\": model_name,\n                \"HYPERPARAMS_FILE\": f\"/data/ml/hyperparams/{model_name}.json\"\n            } for i, model_name in enumerate(models_to_train)\n        },\n        \"metadata\": {\n            \"search_results\": results,\n            \"best_score\": results[0][\"score\"] if results else None\n        }\n    }\n\n    # Write result to CYCLO_EVAL_RESULT\n    eval_result_path = os.environ.get(\"CYCLO_EVAL_RESULT\", \"eval_result.json\")\n    with open(eval_result_path, \"w\") as f:\n        json.dump(result, f)\n\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\nNow, define the model training task:\n# data/tasks/ml/train_model.yaml\nid: \"train_model\"\nname: \"Train ML Model\"\ncommand: \"python ml/train_model.py --data ${DATA_PATH} --hyperparams ${HYPERPARAMS_FILE} --output ${OUTPUT_PATH} --model-name ${MODEL_NAME}\"\ndependencies:\n  - \"hyperparameter_search\"\nparameters:\n  DATA_PATH: \"/data/ml/prepared\"\n  HYPERPARAMS_FILE: \"\"  # Will be set by the evaluation point\n  OUTPUT_PATH: \"/data/ml/models\"\n  MODEL_NAME: \"\"  # Will be set by the evaluation point\nDefine a model evaluation task:\n# data/tasks/ml/evaluate_models.yaml\nid: \"evaluate_models\"\nname: \"Evaluate All Models\"\ncommand: \"python ml/evaluate_models.py --models-dir ${MODELS_DIR} --data ${TEST_DATA} --output $CYCLO_EVAL_RESULT\"\ndependencies:\n  - \"train_model\"\nparameters:\n  MODELS_DIR: \"/data/ml/models\"\n  TEST_DATA: \"/data/ml/prepared/test.pkl\"\nevaluation_point: true\nThe evaluation script selects the best model for deployment:\nimport json\nimport os\nimport sys\nimport pickle\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndef evaluate_models(models_dir, test_data_path):\n    # Load test data\n    with open(test_data_path, \"rb\") as f:\n        X_test, y_test = pickle.load(f)\n\n    # Find all model files\n    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl')]\n\n    results = []\n    for model_file in model_files:\n        model_path = os.path.join(models_dir, model_file)\n        model_name = os.path.splitext(model_file)[0]\n\n        # Load model\n        with open(model_path, \"rb\") as f:\n            model = pickle.load(f)\n\n        # Evaluate\n        y_pred = model.predict(X_test)\n        report = classification_report(y_test, y_pred, output_dict=True)\n\n        # Add to results\n        results.append({\n            \"model_name\": model_name,\n            \"accuracy\": report[\"accuracy\"],\n            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n            \"macro_f1\": report[\"macro avg\"][\"f1-score\"]\n        })\n\n    # Sort by weighted F1 score\n    results.sort(key=lambda x: x[\"weighted_f1\"], reverse=True)\n\n    # Select the best model\n    best_model = results[0] if results else None\n\n    return results, best_model\n\ndef main():\n    models_dir = os.environ.get(\"MODELS_DIR\")\n    test_data_path = os.environ.get(\"TEST_DATA\")\n\n    results, best_model = evaluate_models(models_dir, test_data_path)\n\n    # Determine next tasks based on evaluation\n    if best_model and best_model[\"weighted_f1\"] &gt;= 0.8:\n        # Good model found, proceed to deployment\n        result = {\n            \"next_tasks\": [\"deploy_model\"],\n            \"parameters\": {\n                \"deploy_model\": {\n                    \"MODEL_NAME\": best_model[\"model_name\"],\n                    \"MODEL_PATH\": f\"{models_dir}/{best_model['model_name']}.pkl\"\n                }\n            },\n            \"metadata\": {\n                \"evaluation_results\": results,\n                \"best_model\": best_model\n            }\n        }\n    else:\n        # No good model found, request human review\n        result = {\n            \"next_tasks\": [\"request_human_review\"],\n            \"metadata\": {\n                \"evaluation_results\": results,\n                \"best_model\": best_model\n            }\n        }\n\n    # Write result to CYCLO_EVAL_RESULT\n    eval_result_path = os.environ.get(\"CYCLO_EVAL_RESULT\", \"eval_result.json\")\n    with open(eval_result_path, \"w\") as f:\n        json.dump(result, f)\n\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\nDefine deployment and review tasks:\n# data/tasks/ml/deploy_model.yaml\nid: \"deploy_model\"\nname: \"Deploy ML Model\"\ncommand: \"python ml/deploy_model.py --model ${MODEL_PATH} --environment ${ENVIRONMENT} --registry ${MODEL_REGISTRY}\"\ndependencies:\n  - \"evaluate_models\"\nparameters:\n  MODEL_PATH: \"\"  # Will be set by the evaluation point\n  MODEL_NAME: \"\"  # Will be set by the evaluation point\n  ENVIRONMENT: \"staging\"\n  MODEL_REGISTRY: \"mlflow\"\n# data/tasks/ml/request_human_review.yaml\nid: \"request_human_review\"\nname: \"Request Human Review of Models\"\ncommand: \"python ml/request_review.py --models-dir ${MODELS_DIR} --dashboard-url ${DASHBOARD_URL} --notify ${NOTIFY_EMAIL}\"\ndependencies:\n  - \"evaluate_models\"\nparameters:\n  MODELS_DIR: \"/data/ml/models\"\n  DASHBOARD_URL: \"https://ml-dashboard.example.com/models\"\n  NOTIFY_EMAIL: \"data-science-team@example.com\"\n\n\nDAG Definition\nCreate a DAG for the ML pipeline:\n# data/dags/ml_training_pipeline.yaml\nid: \"ml_training_pipeline\"\nname: \"ML Training Pipeline\"\ndescription: \"End-to-end machine learning training, evaluation and deployment\"\ntasks:\n  - id: \"prepare_data\"\n  - id: \"hyperparameter_search\"\n  - id: \"train_model\"\n  - id: \"evaluate_models\"\n  - id: \"deploy_model\"\n  - id: \"request_human_review\"\nschedule:\n  cron: \"0 2 * * 1\"  # Run weekly on Monday at 2:00 AM\n  timezone: \"UTC\"\n  catchup: false\ntags: [\"ml\", \"training\", \"deployment\"]\n\n\nExecution\nSchedule the ML pipeline:\n./cyclonetix schedule-dag ml_training_pipeline \\\n  --param prepare_data.DATA_SOURCE=\"s3://updated-data/latest.parquet\"\nThis ML pipeline demonstrates: - Dynamic hyperparameter optimization - Parallel model training - Automatic model evaluation and selection - Conditional deployment based on quality thresholds - Human-in-the-loop review when needed",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#components-4",
    "href": "cookbook.html#components-4",
    "title": "Cookbook",
    "section": "Components",
    "text": "Components\n\nCloud Coordinator: A Cyclonetix service that forwards tasks to cloud-native messaging systems\nCloud Messaging: Message queues for task distribution (e.g., Pub/Sub, SQS, Service Bus)\nServerless Agents: Cloud Run instances, Lambda functions, or Azure Functions that execute tasks",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#architecture",
    "href": "cookbook.html#architecture",
    "title": "Cookbook",
    "section": "Architecture",
    "text": "Architecture\nIn this serverless pattern:\n\nA cloud coordinator service acts as an intermediary between Cyclonetix and cloud-native messaging systems\nWhen a task is assigned to a specific queue, the coordinator forwards it to the appropriate cloud messaging system\nServerless instances automatically scale based on message queue depth\nThe agents execute tasks and publish results back to a results topic\nThe coordinator updates task status in Cyclonetix state management",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#cloud-run-configuration",
    "href": "cookbook.html#cloud-run-configuration",
    "title": "Cookbook",
    "section": "Cloud Run Configuration",
    "text": "Cloud Run Configuration\n\nDeploying the Cloud Coordinator\nThe cloud coordinator is deployed as a persistent service:\n# Deploy the coordinator to Cloud Run\ngcloud run deploy cyclonetix-cloud-coordinator \\\n  --image gcr.io/your-project/cyclonetix-cloud-coordinator \\\n  --platform managed \\\n  --memory 512Mi \\\n  --cpu 1 \\\n  --min-instances 1 \\\n  --set-env-vars=\"REDIS_URL=redis://10.0.0.1:6379,TASK_TOPIC=cyclonetix-tasks,RESULT_TOPIC=cyclonetix-results\" \\\n  --region us-central1\n\n\nDeploying Cloud Run Agents\nAgents are deployed as auto-scaling services:\n# Deploy the agent to Cloud Run\ngcloud run deploy cyclonetix-cloud-run-agent \\\n  --image gcr.io/your-project/cyclonetix-cloud-run-agent \\\n  --platform managed \\\n  --memory 2Gi \\\n  --cpu 2 \\\n  --min-instances 0 \\\n  --max-instances 100 \\\n  --set-env-vars=\"TASK_SUBSCRIPTION=cyclonetix-tasks,RESULT_TOPIC=cyclonetix-results\" \\\n  --region us-central1\n\n\nSetting Up Pub/Sub and Autoscaling\nConfigure the messaging system and autoscaling:\n# Create Pub/Sub topics and subscriptions\ngcloud pubsub topics create cyclonetix-tasks\ngcloud pubsub topics create cyclonetix-results\n\ngcloud pubsub subscriptions create cyclonetix-tasks-sub \\\n  --topic=cyclonetix-tasks \\\n  --ack-deadline=300\n\ngcloud pubsub subscriptions create cyclonetix-results-sub \\\n  --topic=cyclonetix-results \\\n  --ack-deadline=60\n\n# Configure Cloud Run autoscaling for the agent\ngcloud run services update cyclonetix-cloud-run-agent \\\n  --no-cpu-throttling \\\n  --concurrency=10 \\\n  --min-instances=0 \\\n  --max-instances=100",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#example-task-configuration",
    "href": "cookbook.html#example-task-configuration",
    "title": "Cookbook",
    "section": "Example Task Configuration",
    "text": "Example Task Configuration\nCreate a task that runs on Cloud Run:\n# data/tasks/cloud_run/ml_inference.yaml\nid: \"ml_inference\"\nname: \"ML Model Inference on Cloud Run\"\ncommand: \"python3 /scripts/inference.py --model ${MODEL_PATH} --data ${DATA_PATH} --output ${OUTPUT_PATH}\"\ndependencies: []\nparameters:\n  MODEL_PATH: \"/models/classifier.pkl\"\n  DATA_PATH: \"/data/input.csv\"\n  OUTPUT_PATH: \"/data/predictions.csv\"\nqueue: \"ml\"  # This queue will be processed by Cloud Run agents\nCreate a DAG that includes this task:\n# data/dags/cloud_run_inference.yaml\nid: \"cloud_run_inference\"\nname: \"Cloud Run ML Inference\"\ndescription: \"Run ML inference on Cloud Run\"\ntasks:\n  - id: \"ml_inference\"\ntags: [\"ml\", \"cloud-run\", \"serverless\"]",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#benefits-of-cloud-run-agents",
    "href": "cookbook.html#benefits-of-cloud-run-agents",
    "title": "Cookbook",
    "section": "Benefits of Cloud Run Agents",
    "text": "Benefits of Cloud Run Agents\n\nZero-Cost When Idle: Agents scale to zero when no tasks are running\nRapid Scaling: Automatically scales up to handle task spikes\nResource Efficiency: Only pay for actual compute time used\nSpecialized Hardware: Easy access to GPUs and other specialized hardware\nNo Infrastructure Management: Google handles all infrastructure patching and maintenance",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#components-5",
    "href": "cookbook.html#components-5",
    "title": "Cookbook",
    "section": "Components",
    "text": "Components\n\nCyclonetix Orchestrator: Central orchestration service\nCloud Coordinators: Services that bridge between Cyclonetix and cloud-specific messaging\nCloud-Native Agents: Serverless agents that scale based on queue depth\nPersistent Agents: Always-on agents running on VMs or Kubernetes\nOn-Premises Agents: Agents running in private data centers",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#architecture-1",
    "href": "cookbook.html#architecture-1",
    "title": "Cookbook",
    "section": "Architecture",
    "text": "Architecture\nIn this multi-cloud pattern:\n\nTasks are assigned to queues based on their requirements (cloud provider, hardware, data locality)\nCloud coordinators forward tasks to appropriate cloud messaging systems\nAgents on each platform execute tasks from their designated queues\nResults flow back through the same path to the central Cyclonetix orchestrator\nState management remains consistent across all environments",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#multi-cloud-setup",
    "href": "cookbook.html#multi-cloud-setup",
    "title": "Cookbook",
    "section": "Multi-Cloud Setup",
    "text": "Multi-Cloud Setup\n\nOn-Premises Configuration\nOn-premises agents connect directly to the Cyclonetix state management:\n# On-premises agent configuration (config.yaml)\nagent:\n  enabled: true\n  id: \"on-prem-agent-1\"\n  queues:\n    - \"on-prem\"\n    - \"secure-data\"\n  concurrency: 8\n\nstate_management:\n  type: \"redis\"\n  url: \"redis://cyclonetix-redis:6379\"\n  serialization_format: \"json\"\n\n\nAWS Configuration\nFor AWS, we use a cloud coordinator with SQS:\n# Create SQS queues\naws sqs create-queue --queue-name cyclonetix-tasks\naws sqs create-queue --queue-name cyclonetix-results\n\n# Deploy the coordinator to ECS\naws ecs create-service \\\n  --cluster cyclonetix \\\n  --service-name aws-coordinator \\\n  --task-definition cyclonetix-aws-coordinator:1 \\\n  --desired-count 1 \\\n  --launch-type FARGATE \\\n  --network-configuration \"awsvpcConfiguration={subnets=[subnet-12345],securityGroups=[sg-12345],assignPublicIp=ENABLED}\"\n\n# Deploy agents as Lambda functions\naws lambda create-function \\\n  --function-name cyclonetix-lambda-agent \\\n  --runtime provided.al2 \\\n  --role arn:aws:iam::123456789012:role/cyclonetix-lambda-role \\\n  --handler not.used.in.provided.runtime \\\n  --code S3Bucket=cyclonetix-deployment,S3Key=lambda-agent.zip \\\n  --environment \"Variables={TASK_QUEUE=cyclonetix-tasks,RESULT_QUEUE=cyclonetix-results,AGENT_QUEUE=aws}\"\n\n\nAzure Configuration\nFor Azure, we use a cloud coordinator with Service Bus:\n# Create Azure Service Bus\naz servicebus namespace create \\\n  --resource-group cyclonetix-rg \\\n  --name cyclonetix-bus \\\n  --location eastus\n\naz servicebus queue create \\\n  --resource-group cyclonetix-rg \\\n  --namespace-name cyclonetix-bus \\\n  --name cyclonetix-tasks\n\naz servicebus queue create \\\n  --resource-group cyclonetix-rg \\\n  --namespace-name cyclonetix-bus \\\n  --name cyclonetix-results\n\n# Deploy the coordinator to Azure Container Instances\naz container create \\\n  --resource-group cyclonetix-rg \\\n  --name azure-coordinator \\\n  --image yourregistry.azurecr.io/cyclonetix-azure-coordinator:latest \\\n  --cpu 1 \\\n  --memory 1.5 \\\n  --environment-variables REDIS_URL=redis://cyclonetix-redis:6379 TASK_QUEUE=cyclonetix-tasks RESULT_QUEUE=cyclonetix-results\n\n# Deploy agents as Azure Functions\naz functionapp create \\\n  --resource-group cyclonetix-rg \\\n  --consumption-plan-location eastus \\\n  --runtime custom \\\n  --functions-version 4 \\\n  --name cyclonetix-function-agent \\\n  --storage-account cyclonetixstorage\n\n\nGoogle Cloud Configuration\nFor GCP, we use a cloud coordinator with Pub/Sub:\n# Create Pub/Sub topics\ngcloud pubsub topics create cyclonetix-tasks\ngcloud pubsub topics create cyclonetix-results\n\ngcloud pubsub subscriptions create cyclonetix-tasks-sub \\\n  --topic=cyclonetix-tasks \\\n  --ack-deadline=300\n\n# Deploy the coordinator to Cloud Run\ngcloud run deploy gcp-coordinator \\\n  --image gcr.io/your-project/cyclonetix-gcp-coordinator \\\n  --platform managed \\\n  --memory 512Mi \\\n  --cpu 1 \\\n  --min-instances 1 \\\n  --set-env-vars=\"REDIS_URL=redis://10.0.0.1:6379,TASK_TOPIC=cyclonetix-tasks,RESULT_TOPIC=cyclonetix-results\" \\\n  --region us-central1\n\n# Deploy agents to Cloud Run\ngcloud run deploy gcp-agent \\\n  --image gcr.io/your-project/cyclonetix-cloud-run-agent \\\n  --platform managed \\\n  --memory 2Gi \\\n  --cpu 2 \\\n  --min-instances 0 \\\n  --max-instances 100 \\\n  --set-env-vars=\"TASK_SUBSCRIPTION=cyclonetix-tasks,RESULT_TOPIC=cyclonetix-results\" \\\n  --region us-central1",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#task-queue-routing",
    "href": "cookbook.html#task-queue-routing",
    "title": "Cookbook",
    "section": "Task Queue Routing",
    "text": "Task Queue Routing\nDefine tasks with specific cloud destinations:\n# data/tasks/aws/sagemaker_training.yaml\nid: \"sagemaker_training\"\nname: \"SageMaker Model Training\"\ncommand: \"python3 ml/aws/train_sagemaker.py --data ${DATA_PATH} --model-type ${MODEL_TYPE}\"\ndependencies: []\nparameters:\n  DATA_PATH: \"s3://cyclonetix-data/training/dataset-1\"\n  MODEL_TYPE: \"xgboost\"\n  INSTANCE_TYPE: \"ml.m5.large\"\nqueue: \"aws-ml\"  # Routed to AWS Lambda agents\n# data/tasks/azure/batch_processing.yaml\nid: \"azure_batch_processing\"\nname: \"Azure Batch Data Processing\"\ncommand: \"python3 processing/azure/batch_process.py --input ${INPUT_CONTAINER} --output ${OUTPUT_CONTAINER}\"\ndependencies: []\nparameters:\n  INPUT_CONTAINER: \"https://cyclonetixstorage.blob.core.windows.net/input\"\n  OUTPUT_CONTAINER: \"https://cyclonetixstorage.blob.core.windows.net/output\"\n  POOL_ID: \"cyclonetix-pool\"\nqueue: \"azure-batch\"  # Routed to Azure Function agents\n# data/tasks/gcp/bigquery_etl.yaml\nid: \"bigquery_etl\"\nname: \"BigQuery ETL Process\"\ncommand: \"python3 etl/gcp/bigquery_transform.py --project ${PROJECT_ID} --dataset ${DATASET}\"\ndependencies: []\nparameters:\n  PROJECT_ID: \"cyclonetix-analytics\"\n  DATASET: \"production_data\"\n  SQL_SCRIPT: \"transforms/daily_aggregation.sql\"\nqueue: \"gcp-data\"  # Routed to Cloud Run agents\n# data/tasks/on_prem/secure_processing.yaml\nid: \"secure_processing\"\nname: \"Secure Data Processing\"\ncommand: \"python3 secure/process.py --data ${DATA_PATH} --keys ${KEYS_PATH}\"\ndependencies: []\nparameters:\n  DATA_PATH: \"/secure/data/customer_records\"\n  KEYS_PATH: \"/secure/keys/encryption_keys\"\n  OUTPUT_PATH: \"/secure/processed/customer_insights\"\nqueue: \"secure-data\"  # Routed to on-premises agents",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#cross-cloud-dag",
    "href": "cookbook.html#cross-cloud-dag",
    "title": "Cookbook",
    "section": "Cross-Cloud DAG",
    "text": "Cross-Cloud DAG\nCreate a DAG that spans multiple cloud environments:\n# data/dags/multi_cloud_analytics.yaml\nid: \"multi_cloud_analytics\"\nname: \"Multi-Cloud Analytics Pipeline\"\ndescription: \"Analytics workflow spanning multiple cloud environments\"\ntasks:\n  - id: \"on_prem_data_prep\"\n    queue: \"on-prem\"\n  - id: \"aws_data_analysis\"\n    queue: \"aws-ml\"\n    dependencies:\n      - \"on_prem_data_prep\"\n  - id: \"azure_visualization\"\n    queue: \"azure-batch\"\n    dependencies:\n      - \"aws_data_analysis\"\n  - id: \"gcp_dashboard_update\"\n    queue: \"gcp-data\"\n    dependencies:\n      - \"azure_visualization\"\ntags: [\"multi-cloud\", \"analytics\", \"hybrid\"]",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#cloud-specific-paas-integration",
    "href": "cookbook.html#cloud-specific-paas-integration",
    "title": "Cookbook",
    "section": "Cloud-Specific PaaS Integration",
    "text": "Cloud-Specific PaaS Integration\nAgents can leverage cloud-specific PaaS offerings:\n\nAWS SageMaker Integration\n# Example of AWS Lambda agent invoking SageMaker\nimport boto3\n\ndef start_sagemaker_training(data_path, model_type, instance_type):\n    sagemaker = boto3.client('sagemaker')\n\n    response = sagemaker.create_training_job(\n        TrainingJobName='cyclonetix-training-job',\n        AlgorithmSpecification={\n            'TrainingImage': '123456789012.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n            'TrainingInputMode': 'File'\n        },\n        RoleArn='arn:aws:iam::123456789012:role/SageMakerExecutionRole',\n        InputDataConfig=[\n            {\n                'ChannelName': 'train',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': data_path,\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                }\n            }\n        ],\n        OutputDataConfig={\n            'S3OutputPath': 's3://cyclonetix-models/output'\n        },\n        ResourceConfig={\n            'InstanceType': instance_type,\n            'InstanceCount': 1,\n            'VolumeSizeInGB': 30\n        },\n        StoppingCondition={\n            'MaxRuntimeInSeconds': 86400\n        },\n        HyperParameters={\n            'objective': 'binary:logistic',\n            'max_depth': '5',\n            'eta': '0.2',\n            'num_round': '100'\n        }\n    )\n\n    return response['TrainingJobArn']\n\n\nAzure ML Integration\n# Example of Azure Function agent invoking Azure ML\nfrom azure.identity import DefaultAzureCredential\nfrom azure.ai.ml import MLClient, command\nfrom azure.ai.ml.entities import Environment, BuildContext\nfrom azure.ai.ml.constants import AssetTypes\n\ndef start_azure_ml_job(data_path, model_type):\n    credential = DefaultAzureCredential()\n    ml_client = MLClient(\n        credential=credential,\n        subscription_id=\"subscription-id\",\n        resource_group_name=\"resource-group\",\n        workspace_name=\"workspace-name\"\n    )\n\n    job = command(\n        code=\"./src\",\n        command=\"python train.py --data ${{inputs.data_path}} --model-type ${{inputs.model_type}}\",\n        inputs={\n            \"data_path\": data_path,\n            \"model_type\": model_type\n        },\n        environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n        compute=\"aml-cluster\",\n        display_name=\"cyclonetix-training-job\"\n    )\n\n    returned_job = ml_client.jobs.create_or_update(job)\n    return returned_job.name\n\n\nGCP AI Platform Integration\n# Example of Cloud Run agent invoking AI Platform\nfrom google.cloud import aiplatform\n\ndef start_vertex_training(project_id, region, data_path, model_type):\n    aiplatform.init(project=project_id, location=region)\n\n    custom_job = aiplatform.CustomJob(\n        display_name=\"cyclonetix-training-job\",\n        worker_pool_specs=[\n            {\n                \"machine_spec\": {\n                    \"machine_type\": \"n1-standard-4\",\n                    \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n                    \"accelerator_count\": 1,\n                },\n                \"replica_count\": 1,\n                \"python_package_spec\": {\n                    \"executor_image_uri\": \"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.0-23:latest\",\n                    \"package_uris\": [\"gs://cyclonetix-packages/trainer-0.1.tar.gz\"],\n                    \"python_module\": \"trainer.task\",\n                    \"args\": [\n                        f\"--data-path={data_path}\",\n                        f\"--model-type={model_type}\",\n                        \"--epochs=20\"\n                    ],\n                },\n            }\n        ],\n    )\n\n    custom_job_run = custom_job.run()\n    return custom_job_run.name",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook.html#benefits-of-multi-cloud-agent-orchestration",
    "href": "cookbook.html#benefits-of-multi-cloud-agent-orchestration",
    "title": "Cookbook",
    "section": "Benefits of Multi-Cloud Agent Orchestration",
    "text": "Benefits of Multi-Cloud Agent Orchestration\n\nOptimal Cloud Selection: Route tasks to the most suitable cloud based on requirements\nCost Optimization: Utilize spot instances or serverless offerings from multiple providers\nAvoid Vendor Lock-in: Maintain flexibility to move workloads between clouds\nGeographical Distribution: Execute tasks in regions closest to data or users\nSpecialized Services: Use the best-in-class services from each cloud provider\nCompliance: Keep sensitive workloads on-premises while using cloud for non-sensitive tasks\nHybrid Strategy: Gradually migrate to cloud while maintaining on-premises systems",
    "crumbs": [
      "Cookbook"
    ]
  },
  {
    "objectID": "reference/cli.html",
    "href": "reference/cli.html",
    "title": "CLI Reference",
    "section": "",
    "text": "Cyclonetix provides a comprehensive command-line interface (CLI) for managing workflows, tasks, and system operations. This reference documents all available commands and options.\n\n\ncyclonetix [OPTIONS] [COMMAND]\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--config &lt;FILE&gt;\nPath to configuration file (default: config.yaml)\n\n\n--help, -h\nShow help information\n\n\n--version, -v\nShow version information\n\n\n--ui\nStart only the UI server\n\n\n--agent\nStart only an agent process\n\n\n--orchestrator\nStart only an orchestrator process\n\n\n\n\n\n\n\n\nStarting Cyclonetix without a command runs all components (orchestrator, agent, and UI) in development mode.\ncyclonetix --config my-config.yaml\n\n\n\nSchedule a task for execution.\ncyclonetix schedule-task &lt;TASK_ID&gt; [OPTIONS]\nOptions:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--context &lt;CONTEXT&gt;\nContext to use for execution\n\n\n--param &lt;KEY=VALUE&gt;\nTask parameter override (can be specified multiple times)\n\n\n--param-set &lt;SET&gt;\nParameter set to apply\n\n\n--env &lt;KEY=VALUE&gt;\nAdditional environment variable (can be specified multiple times)\n\n\n--run-id &lt;ID&gt;\nCustom run ID (auto-generated if not specified)\n\n\n--queue &lt;QUEUE&gt;\nOverride the task’s default queue\n\n\n--wait\nWait for task completion and show logs\n\n\n--timeout &lt;SECONDS&gt;\nTimeout for task execution in seconds\n\n\n--retries &lt;COUNT&gt;\nNumber of retries on failure\n\n\n--retry-delay &lt;SECONDS&gt;\nDelay between retries in seconds\n\n\n--ignore-dependencies\nExecute without resolving dependencies\n\n\n--git-ref &lt;REF&gt;\nGit reference for task definition\n\n\n\nExample:\ncyclonetix schedule-task process_data \\\n  --context production \\\n  --param BATCH_SIZE=200 \\\n  --param MODE=full \\\n  --env DATA_DATE=2023-11-01\n\n\n\nSchedule a DAG for execution.\ncyclonetix schedule-dag &lt;DAG_ID&gt; [OPTIONS]\nOptions:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--context &lt;CONTEXT&gt;\nContext to use for execution\n\n\n--param &lt;TASK_ID.KEY=VALUE&gt;\nTask parameter override (can be specified multiple times)\n\n\n--env &lt;KEY=VALUE&gt;\nAdditional environment variable (can be specified multiple times)\n\n\n--run-id &lt;ID&gt;\nCustom run ID (auto-generated if not specified)\n\n\n--wait\nWait for DAG completion\n\n\n--timeout &lt;SECONDS&gt;\nTimeout for DAG execution in seconds\n\n\n--git-ref &lt;REF&gt;\nGit reference for DAG definition\n\n\n\nExample:\ncyclonetix schedule-dag etl_pipeline \\\n  --context production \\\n  --param extract_data.LIMIT=1000 \\\n  --param load_data.MODE=incremental \\\n  --wait\n\n\n\nSchedule a DAG from a file (without requiring it to be in the tasks directory).\ncyclonetix schedule-dag-file &lt;FILE_PATH&gt; [OPTIONS]\nOptions are the same as schedule-dag.\nExample:\ncyclonetix schedule-dag-file ./my-custom-dag.yaml \\\n  --context production\n\n\n\n\n\n\nList available tasks.\ncyclonetix list-tasks [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--filter &lt;PATTERN&gt;\nFilter tasks by name or ID\n\n\n--queue &lt;QUEUE&gt;\nFilter tasks by queue\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n\nExample:\ncyclonetix list-tasks --filter \"data_*\" --format json\n\n\n\nList available DAGs.\ncyclonetix list-dags [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--filter &lt;PATTERN&gt;\nFilter DAGs by name or ID\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n\nExample:\ncyclonetix list-dags --format table\n\n\n\nShow details of a specific task.\ncyclonetix show-task &lt;TASK_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (yaml, json)\n\n\n--show-dependencies\nShow task dependencies\n\n\n--git-ref &lt;REF&gt;\nGit reference for task definition\n\n\n\nExample:\ncyclonetix show-task process_data --show-dependencies\n\n\n\nShow details of a specific DAG.\ncyclonetix show-dag &lt;DAG_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (yaml, json)\n\n\n--show-tasks\nShow tasks in the DAG\n\n\n--git-ref &lt;REF&gt;\nGit reference for DAG definition\n\n\n\nExample:\ncyclonetix show-dag etl_pipeline --show-tasks\n\n\n\n\n\n\nList current and recent workflow runs.\ncyclonetix list-runs [OPTIONS]\nOptions:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--status &lt;STATUS&gt;\nFilter by status (pending, running, completed, failed)\n\n\n--dag &lt;DAG_ID&gt;\nFilter by DAG ID\n\n\n--limit &lt;COUNT&gt;\nMaximum number of runs to show\n\n\n--since &lt;TIME&gt;\nShow runs since time (e.g., 1h, 2d, 2023-01-01)\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n\nExample:\ncyclonetix list-runs --status running --since 12h\n\n\n\nShow details of a specific workflow run.\ncyclonetix show-run &lt;RUN_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n--show-tasks\nShow tasks in the run\n\n\n--follow\nFollow the run and show updates\n\n\n\nExample:\ncyclonetix show-run etl_pipeline_20231105_123456 --show-tasks --follow\n\n\n\nCancel a running workflow.\ncyclonetix cancel-run &lt;RUN_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--force\nForce cancel even if in critical section\n\n\n\nExample:\ncyclonetix cancel-run etl_pipeline_20231105_123456\n\n\n\nRerun a failed or completed workflow.\ncyclonetix rerun &lt;RUN_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--failed-only\nRerun only failed tasks\n\n\n--from-task &lt;TASK_ID&gt;\nRerun from a specific task\n\n\n--reset-params\nReset parameters to original values\n\n\n\nExample:\ncyclonetix rerun etl_pipeline_20231105_123456 --failed-only\n\n\n\n\n\n\nList active agents.\ncyclonetix list-agents [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n--show-tasks\nShow tasks assigned to agents\n\n\n\nExample:\ncyclonetix list-agents --show-tasks\n\n\n\nShow detailed status of an agent.\ncyclonetix agent-status &lt;AGENT_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n\nExample:\ncyclonetix agent-status agent_20231105_123456\n\n\n\n\n\n\nList task queues and their status.\ncyclonetix list-queues [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n--show-tasks\nShow tasks in queues\n\n\n\nExample:\ncyclonetix list-queues --show-tasks\n\n\n\nPurge all tasks from a queue.\ncyclonetix purge-queue &lt;QUEUE&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--confirm\nConfirm the operation without prompting\n\n\n\nExample:\ncyclonetix purge-queue default --confirm\n\n\n\n\n\n\nCheck the health of the Cyclonetix deployment.\ncyclonetix check-health [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--backend\nCheck backend connectivity\n\n\n--agents\nCheck agent health\n\n\n--orchestrators\nCheck orchestrator health\n\n\n--all\nCheck all components\n\n\n\nExample:\ncyclonetix check-health --all\n\n\n\nVerify the configuration without starting the server.\ncyclonetix verify-config [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--config &lt;FILE&gt;\nPath to configuration file\n\n\n\nExample:\ncyclonetix verify-config --config production.yaml\n\n\n\nClear execution history.\ncyclonetix clear-history [OPTIONS]\nOptions:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--days &lt;DAYS&gt;\nClear history older than specified days\n\n\n--status &lt;STATUS&gt;\nClear runs with specific status (completed, failed)\n\n\n--confirm\nConfirm the operation without prompting\n\n\n\nExample:\ncyclonetix clear-history --days 30 --status completed --confirm\n\n\n\n\n\n\nRun a DAG for a date range (backfilling).\ncyclonetix backfill &lt;DAG_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--start-date &lt;DATE&gt;\nStart date (YYYY-MM-DD)\n\n\n--end-date &lt;DATE&gt;\nEnd date (YYYY-MM-DD)\n\n\n--context &lt;CONTEXT&gt;\nContext to use for execution\n\n\n--max-parallel &lt;COUNT&gt;\nMaximum parallel executions\n\n\n--wait\nWait for all executions to complete\n\n\n\nExample:\ncyclonetix backfill daily_etl \\\n  --start-date 2023-01-01 \\\n  --end-date 2023-01-31 \\\n  --context production \\\n  --max-parallel 5\n\n\n\nImport tasks or DAGs from external files.\ncyclonetix import &lt;TYPE&gt; &lt;FILE_PATH&gt; [OPTIONS]\nWhere &lt;TYPE&gt; is one of: task, dag, context, parameter-set.\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--overwrite\nOverwrite existing definitions\n\n\n--validate\nValidate without importing\n\n\n\nExample:\ncyclonetix import task ./external-tasks/*.yaml --validate\n\n\n\nExport tasks or DAGs to external files.\ncyclonetix export &lt;TYPE&gt; &lt;ID&gt; &lt;FILE_PATH&gt; [OPTIONS]\nWhere &lt;TYPE&gt; is one of: task, dag, context, parameter-set.\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (yaml, json)\n\n\n--include-dependencies\nInclude dependencies when exporting\n\n\n\nExample:\ncyclonetix export dag etl_pipeline ./exports/etl_pipeline.yaml --include-dependencies\n\n\n\n\n\nReview the API Reference for REST API documentation\nExplore the Configuration Reference for configuration options\nCheck the YAML Schema for task and DAG definitions",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#basic-usage",
    "href": "reference/cli.html#basic-usage",
    "title": "CLI Reference",
    "section": "",
    "text": "cyclonetix [OPTIONS] [COMMAND]",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#global-options",
    "href": "reference/cli.html#global-options",
    "title": "CLI Reference",
    "section": "",
    "text": "Option\nDescription\n\n\n\n\n--config &lt;FILE&gt;\nPath to configuration file (default: config.yaml)\n\n\n--help, -h\nShow help information\n\n\n--version, -v\nShow version information\n\n\n--ui\nStart only the UI server\n\n\n--agent\nStart only an agent process\n\n\n--orchestrator\nStart only an orchestrator process",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#common-commands",
    "href": "reference/cli.html#common-commands",
    "title": "CLI Reference",
    "section": "",
    "text": "Starting Cyclonetix without a command runs all components (orchestrator, agent, and UI) in development mode.\ncyclonetix --config my-config.yaml\n\n\n\nSchedule a task for execution.\ncyclonetix schedule-task &lt;TASK_ID&gt; [OPTIONS]\nOptions:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--context &lt;CONTEXT&gt;\nContext to use for execution\n\n\n--param &lt;KEY=VALUE&gt;\nTask parameter override (can be specified multiple times)\n\n\n--param-set &lt;SET&gt;\nParameter set to apply\n\n\n--env &lt;KEY=VALUE&gt;\nAdditional environment variable (can be specified multiple times)\n\n\n--run-id &lt;ID&gt;\nCustom run ID (auto-generated if not specified)\n\n\n--queue &lt;QUEUE&gt;\nOverride the task’s default queue\n\n\n--wait\nWait for task completion and show logs\n\n\n--timeout &lt;SECONDS&gt;\nTimeout for task execution in seconds\n\n\n--retries &lt;COUNT&gt;\nNumber of retries on failure\n\n\n--retry-delay &lt;SECONDS&gt;\nDelay between retries in seconds\n\n\n--ignore-dependencies\nExecute without resolving dependencies\n\n\n--git-ref &lt;REF&gt;\nGit reference for task definition\n\n\n\nExample:\ncyclonetix schedule-task process_data \\\n  --context production \\\n  --param BATCH_SIZE=200 \\\n  --param MODE=full \\\n  --env DATA_DATE=2023-11-01\n\n\n\nSchedule a DAG for execution.\ncyclonetix schedule-dag &lt;DAG_ID&gt; [OPTIONS]\nOptions:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--context &lt;CONTEXT&gt;\nContext to use for execution\n\n\n--param &lt;TASK_ID.KEY=VALUE&gt;\nTask parameter override (can be specified multiple times)\n\n\n--env &lt;KEY=VALUE&gt;\nAdditional environment variable (can be specified multiple times)\n\n\n--run-id &lt;ID&gt;\nCustom run ID (auto-generated if not specified)\n\n\n--wait\nWait for DAG completion\n\n\n--timeout &lt;SECONDS&gt;\nTimeout for DAG execution in seconds\n\n\n--git-ref &lt;REF&gt;\nGit reference for DAG definition\n\n\n\nExample:\ncyclonetix schedule-dag etl_pipeline \\\n  --context production \\\n  --param extract_data.LIMIT=1000 \\\n  --param load_data.MODE=incremental \\\n  --wait\n\n\n\nSchedule a DAG from a file (without requiring it to be in the tasks directory).\ncyclonetix schedule-dag-file &lt;FILE_PATH&gt; [OPTIONS]\nOptions are the same as schedule-dag.\nExample:\ncyclonetix schedule-dag-file ./my-custom-dag.yaml \\\n  --context production",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#task-and-dag-management",
    "href": "reference/cli.html#task-and-dag-management",
    "title": "CLI Reference",
    "section": "",
    "text": "List available tasks.\ncyclonetix list-tasks [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--filter &lt;PATTERN&gt;\nFilter tasks by name or ID\n\n\n--queue &lt;QUEUE&gt;\nFilter tasks by queue\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n\nExample:\ncyclonetix list-tasks --filter \"data_*\" --format json\n\n\n\nList available DAGs.\ncyclonetix list-dags [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--filter &lt;PATTERN&gt;\nFilter DAGs by name or ID\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n\nExample:\ncyclonetix list-dags --format table\n\n\n\nShow details of a specific task.\ncyclonetix show-task &lt;TASK_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (yaml, json)\n\n\n--show-dependencies\nShow task dependencies\n\n\n--git-ref &lt;REF&gt;\nGit reference for task definition\n\n\n\nExample:\ncyclonetix show-task process_data --show-dependencies\n\n\n\nShow details of a specific DAG.\ncyclonetix show-dag &lt;DAG_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (yaml, json)\n\n\n--show-tasks\nShow tasks in the DAG\n\n\n--git-ref &lt;REF&gt;\nGit reference for DAG definition\n\n\n\nExample:\ncyclonetix show-dag etl_pipeline --show-tasks",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#execution-management",
    "href": "reference/cli.html#execution-management",
    "title": "CLI Reference",
    "section": "",
    "text": "List current and recent workflow runs.\ncyclonetix list-runs [OPTIONS]\nOptions:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--status &lt;STATUS&gt;\nFilter by status (pending, running, completed, failed)\n\n\n--dag &lt;DAG_ID&gt;\nFilter by DAG ID\n\n\n--limit &lt;COUNT&gt;\nMaximum number of runs to show\n\n\n--since &lt;TIME&gt;\nShow runs since time (e.g., 1h, 2d, 2023-01-01)\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n\nExample:\ncyclonetix list-runs --status running --since 12h\n\n\n\nShow details of a specific workflow run.\ncyclonetix show-run &lt;RUN_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n--show-tasks\nShow tasks in the run\n\n\n--follow\nFollow the run and show updates\n\n\n\nExample:\ncyclonetix show-run etl_pipeline_20231105_123456 --show-tasks --follow\n\n\n\nCancel a running workflow.\ncyclonetix cancel-run &lt;RUN_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--force\nForce cancel even if in critical section\n\n\n\nExample:\ncyclonetix cancel-run etl_pipeline_20231105_123456\n\n\n\nRerun a failed or completed workflow.\ncyclonetix rerun &lt;RUN_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--failed-only\nRerun only failed tasks\n\n\n--from-task &lt;TASK_ID&gt;\nRerun from a specific task\n\n\n--reset-params\nReset parameters to original values\n\n\n\nExample:\ncyclonetix rerun etl_pipeline_20231105_123456 --failed-only",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#agent-management",
    "href": "reference/cli.html#agent-management",
    "title": "CLI Reference",
    "section": "",
    "text": "List active agents.\ncyclonetix list-agents [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n--show-tasks\nShow tasks assigned to agents\n\n\n\nExample:\ncyclonetix list-agents --show-tasks\n\n\n\nShow detailed status of an agent.\ncyclonetix agent-status &lt;AGENT_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n\nExample:\ncyclonetix agent-status agent_20231105_123456",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#queue-management",
    "href": "reference/cli.html#queue-management",
    "title": "CLI Reference",
    "section": "",
    "text": "List task queues and their status.\ncyclonetix list-queues [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (table, json, yaml)\n\n\n--show-tasks\nShow tasks in queues\n\n\n\nExample:\ncyclonetix list-queues --show-tasks\n\n\n\nPurge all tasks from a queue.\ncyclonetix purge-queue &lt;QUEUE&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--confirm\nConfirm the operation without prompting\n\n\n\nExample:\ncyclonetix purge-queue default --confirm",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#system-management",
    "href": "reference/cli.html#system-management",
    "title": "CLI Reference",
    "section": "",
    "text": "Check the health of the Cyclonetix deployment.\ncyclonetix check-health [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--backend\nCheck backend connectivity\n\n\n--agents\nCheck agent health\n\n\n--orchestrators\nCheck orchestrator health\n\n\n--all\nCheck all components\n\n\n\nExample:\ncyclonetix check-health --all\n\n\n\nVerify the configuration without starting the server.\ncyclonetix verify-config [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--config &lt;FILE&gt;\nPath to configuration file\n\n\n\nExample:\ncyclonetix verify-config --config production.yaml\n\n\n\nClear execution history.\ncyclonetix clear-history [OPTIONS]\nOptions:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--days &lt;DAYS&gt;\nClear history older than specified days\n\n\n--status &lt;STATUS&gt;\nClear runs with specific status (completed, failed)\n\n\n--confirm\nConfirm the operation without prompting\n\n\n\nExample:\ncyclonetix clear-history --days 30 --status completed --confirm",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#advanced-commands",
    "href": "reference/cli.html#advanced-commands",
    "title": "CLI Reference",
    "section": "",
    "text": "Run a DAG for a date range (backfilling).\ncyclonetix backfill &lt;DAG_ID&gt; [OPTIONS]\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--start-date &lt;DATE&gt;\nStart date (YYYY-MM-DD)\n\n\n--end-date &lt;DATE&gt;\nEnd date (YYYY-MM-DD)\n\n\n--context &lt;CONTEXT&gt;\nContext to use for execution\n\n\n--max-parallel &lt;COUNT&gt;\nMaximum parallel executions\n\n\n--wait\nWait for all executions to complete\n\n\n\nExample:\ncyclonetix backfill daily_etl \\\n  --start-date 2023-01-01 \\\n  --end-date 2023-01-31 \\\n  --context production \\\n  --max-parallel 5\n\n\n\nImport tasks or DAGs from external files.\ncyclonetix import &lt;TYPE&gt; &lt;FILE_PATH&gt; [OPTIONS]\nWhere &lt;TYPE&gt; is one of: task, dag, context, parameter-set.\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--overwrite\nOverwrite existing definitions\n\n\n--validate\nValidate without importing\n\n\n\nExample:\ncyclonetix import task ./external-tasks/*.yaml --validate\n\n\n\nExport tasks or DAGs to external files.\ncyclonetix export &lt;TYPE&gt; &lt;ID&gt; &lt;FILE_PATH&gt; [OPTIONS]\nWhere &lt;TYPE&gt; is one of: task, dag, context, parameter-set.\nOptions:\n\n\n\nOption\nDescription\n\n\n\n\n--format &lt;FORMAT&gt;\nOutput format (yaml, json)\n\n\n--include-dependencies\nInclude dependencies when exporting\n\n\n\nExample:\ncyclonetix export dag etl_pipeline ./exports/etl_pipeline.yaml --include-dependencies",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/cli.html#next-steps",
    "href": "reference/cli.html#next-steps",
    "title": "CLI Reference",
    "section": "",
    "text": "Review the API Reference for REST API documentation\nExplore the Configuration Reference for configuration options\nCheck the YAML Schema for task and DAG definitions",
    "crumbs": [
      "Reference",
      "CLI Reference"
    ]
  },
  {
    "objectID": "reference/api.html",
    "href": "reference/api.html",
    "title": "API Reference",
    "section": "",
    "text": "Cyclonetix provides a RESTful API for integrating with other systems and programmatically managing workflows. This reference documents all available API endpoints, parameters, and response formats.\n\n\nBy default, the API is available at:\nhttp://&lt;cyclonetix-host&gt;:3000/api\n\n\n\nWhen security is enabled, you can authenticate using:\n\nAPI Key: Pass the API key in the X-API-Key header:\nX-API-Key: your-api-key-here\nOAuth Token: After OAuth authentication, use the session cookie or pass the token in the Authorization header:\nAuthorization: Bearer your-token-here\n\n\n\n\n\n\n\n\nGET /api/tasks\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\nfilter\nFilter tasks by name or ID pattern\n\n\nqueue\nFilter tasks by queue\n\n\nlimit\nMaximum number of tasks to return\n\n\noffset\nPagination offset\n\n\n\nExample response:\n{\n  \"tasks\": [\n    {\n      \"id\": \"data_extraction\",\n      \"name\": \"Data Extraction\",\n      \"description\": \"Extract data from source system\",\n      \"command\": \"python extract.py --source ${SOURCE}\",\n      \"dependencies\": [],\n      \"parameters\": {\n        \"SOURCE\": \"prod_db\"\n      },\n      \"queue\": \"default\"\n    },\n    // More tasks...\n  ],\n  \"total\": 24,\n  \"limit\": 10,\n  \"offset\": 0\n}\n\n\n\nGET /api/tasks/{task_id}\nExample response:\n{\n  \"id\": \"data_extraction\",\n  \"name\": \"Data Extraction\",\n  \"description\": \"Extract data from source system\",\n  \"command\": \"python extract.py --source ${SOURCE}\",\n  \"dependencies\": [],\n  \"parameters\": {\n    \"SOURCE\": \"prod_db\"\n  },\n  \"queue\": \"default\",\n  \"created_at\": \"2023-01-01T00:00:00Z\",\n  \"updated_at\": \"2023-01-15T00:00:00Z\"\n}\n\n\n\nPUT /api/tasks/{task_id}\nRequest body:\n{\n  \"name\": \"Data Extraction\",\n  \"description\": \"Extract data from source system\",\n  \"command\": \"python extract.py --source ${SOURCE}\",\n  \"dependencies\": [],\n  \"parameters\": {\n    \"SOURCE\": \"prod_db\"\n  },\n  \"queue\": \"default\"\n}\nResponse: The created/updated task object.\n\n\n\nDELETE /api/tasks/{task_id}\nResponse:\n{\n  \"success\": true,\n  \"message\": \"Task deleted successfully\"\n}\n\n\n\n\n\n\nGET /api/dags\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\nfilter\nFilter DAGs by name or ID pattern\n\n\nlimit\nMaximum number of DAGs to return\n\n\noffset\nPagination offset\n\n\n\nExample response:\n{\n  \"dags\": [\n    {\n      \"id\": \"etl_pipeline\",\n      \"name\": \"ETL Pipeline\",\n      \"description\": \"Extract, transform, and load data\",\n      \"tasks\": [\"data_extraction\", \"data_transformation\", \"data_loading\"],\n      \"tags\": [\"etl\", \"daily\"]\n    },\n    // More DAGs...\n  ],\n  \"total\": 12,\n  \"limit\": 10,\n  \"offset\": 0\n}\n\n\n\nGET /api/dags/{dag_id}\nExample response:\n{\n  \"id\": \"etl_pipeline\",\n  \"name\": \"ETL Pipeline\",\n  \"description\": \"Extract, transform, and load data\",\n  \"tasks\": [\n    {\n      \"id\": \"data_extraction\",\n      \"parameters\": {\n        \"SOURCE\": \"prod_db\"\n      }\n    },\n    {\n      \"id\": \"data_transformation\",\n      \"parameters\": {\n        \"TRANSFORM_TYPE\": \"standard\"\n      }\n    },\n    {\n      \"id\": \"data_loading\",\n      \"parameters\": {\n        \"DESTINATION\": \"data_warehouse\"\n      }\n    }\n  ],\n  \"context\": \"production\",\n  \"tags\": [\"etl\", \"daily\"],\n  \"created_at\": \"2023-01-01T00:00:00Z\",\n  \"updated_at\": \"2023-01-15T00:00:00Z\"\n}\n\n\n\nPUT /api/dags/{dag_id}\nRequest body:\n{\n  \"name\": \"ETL Pipeline\",\n  \"description\": \"Extract, transform, and load data\",\n  \"tasks\": [\n    {\n      \"id\": \"data_extraction\",\n      \"parameters\": {\n        \"SOURCE\": \"prod_db\"\n      }\n    },\n    {\n      \"id\": \"data_transformation\",\n      \"parameters\": {\n        \"TRANSFORM_TYPE\": \"standard\"\n      }\n    },\n    {\n      \"id\": \"data_loading\",\n      \"parameters\": {\n        \"DESTINATION\": \"data_warehouse\"\n      }\n    }\n  ],\n  \"context\": \"production\",\n  \"tags\": [\"etl\", \"daily\"]\n}\nResponse: The created/updated DAG object.\n\n\n\nDELETE /api/dags/{dag_id}\nResponse:\n{\n  \"success\": true,\n  \"message\": \"DAG deleted successfully\"\n}\n\n\n\n\n\n\nPOST /api/schedule-task\nRequest body:\n{\n  \"task_id\": \"data_extraction\",\n  \"context\": \"production\",\n  \"env_vars\": {\n    \"DATA_DATE\": \"2023-11-01\",\n    \"LOG_LEVEL\": \"info\"\n  },\n  \"parameters\": {\n    \"SOURCE\": \"prod_db\",\n    \"LIMIT\": \"1000\"\n  },\n  \"run_id\": \"custom_run_id_20231101\",  // Optional\n  \"queue\": \"high_priority\",  // Optional\n  \"timeout_seconds\": 3600,  // Optional\n  \"retries\": 3,  // Optional\n  \"retry_delay_seconds\": 300  // Optional\n}\nResponse:\n{\n  \"success\": true,\n  \"run_id\": \"custom_run_id_20231101\",\n  \"dag_run_id\": \"dag_57f3e2a1\",\n  \"status\": \"pending\",\n  \"start_time\": \"2023-11-01T12:00:00Z\"\n}\n\n\n\nPOST /api/schedule-dag\nRequest body:\n{\n  \"dag_id\": \"etl_pipeline\",\n  \"context\": \"production\",\n  \"env_vars\": {\n    \"DATA_DATE\": \"2023-11-01\"\n  },\n  \"parameters\": {\n    \"data_extraction.SOURCE\": \"prod_db\",\n    \"data_loading.DESTINATION\": \"data_warehouse\"\n  },\n  \"run_id\": \"etl_20231101\",  // Optional\n  \"timeout_seconds\": 7200  // Optional\n}\nResponse:\n{\n  \"success\": true,\n  \"run_id\": \"etl_20231101\",\n  \"status\": \"pending\",\n  \"start_time\": \"2023-11-01T12:00:00Z\",\n  \"tasks\": [\n    {\n      \"id\": \"data_extraction\",\n      \"task_run_id\": \"data_extraction_a1b2c3d4\",\n      \"status\": \"pending\"\n    },\n    {\n      \"id\": \"data_transformation\",\n      \"task_run_id\": \"data_transformation_b2c3d4e5\",\n      \"status\": \"pending\"\n    },\n    {\n      \"id\": \"data_loading\",\n      \"task_run_id\": \"data_loading_c3d4e5f6\",\n      \"status\": \"pending\"\n    }\n  ]\n}\n\n\n\nPOST /api/cancel-run/{run_id}\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\nforce\nForce cancel even if in critical section (true/false)\n\n\n\nResponse:\n{\n  \"success\": true,\n  \"message\": \"Run cancelled successfully\",\n  \"run_id\": \"etl_20231101\"\n}\n\n\n\nPOST /api/rerun/{run_id}\nRequest body:\n{\n  \"failed_only\": true,  // Optional\n  \"from_task\": \"data_transformation\",  // Optional\n  \"reset_params\": false  // Optional\n}\nResponse:\n{\n  \"success\": true,\n  \"new_run_id\": \"etl_20231101_rerun\",\n  \"original_run_id\": \"etl_20231101\",\n  \"status\": \"pending\",\n  \"start_time\": \"2023-11-01T14:00:00Z\"\n}\n\n\n\n\n\n\nGET /api/runs\nQuery parameters:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nstatus\nFilter by status (pending, running, completed, failed)\n\n\ndag_id\nFilter by DAG ID\n\n\nsince\nShow runs since time (ISO 8601 format)\n\n\nuntil\nShow runs until time (ISO 8601 format)\n\n\nlimit\nMaximum number of runs to return\n\n\noffset\nPagination offset\n\n\n\nExample response:\n{\n  \"runs\": [\n    {\n      \"run_id\": \"etl_20231101\",\n      \"dag_id\": \"etl_pipeline\",\n      \"status\": \"completed\",\n      \"start_time\": \"2023-11-01T12:00:00Z\",\n      \"end_time\": \"2023-11-01T12:30:00Z\",\n      \"task_count\": 3,\n      \"completed_tasks\": 3,\n      \"failed_tasks\": 0\n    },\n    // More runs...\n  ],\n  \"total\": 45,\n  \"limit\": 10,\n  \"offset\": 0\n}\n\n\n\nGET /api/runs/{run_id}\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\ninclude_tasks\nInclude task details (true/false)\n\n\ninclude_logs\nInclude task logs (true/false)\n\n\n\nExample response:\n{\n  \"run_id\": \"etl_20231101\",\n  \"dag_id\": \"etl_pipeline\",\n  \"status\": \"completed\",\n  \"start_time\": \"2023-11-01T12:00:00Z\",\n  \"end_time\": \"2023-11-01T12:30:00Z\",\n  \"duration_seconds\": 1800,\n  \"task_count\": 3,\n  \"completed_tasks\": 3,\n  \"failed_tasks\": 0,\n  \"tasks\": [\n    {\n      \"task_id\": \"data_extraction\",\n      \"task_run_id\": \"data_extraction_a1b2c3d4\",\n      \"status\": \"completed\",\n      \"start_time\": \"2023-11-01T12:00:00Z\",\n      \"end_time\": \"2023-11-01T12:10:00Z\",\n      \"duration_seconds\": 600,\n      \"logs\": \"Starting extraction...\\nExtracted 10000 records\\nCompleted successfully\"\n    },\n    // More tasks...\n  ],\n  \"context\": {\n    \"DATA_DATE\": \"2023-11-01\",\n    \"ENV\": \"production\"\n  }\n}\n\n\n\nGET /api/runs/{run_id}/graph\nExample response:\n{\n  \"run_id\": \"etl_20231101\",\n  \"dag_id\": \"etl_pipeline\",\n  \"nodes\": [\n    {\n      \"id\": \"data_extraction_a1b2c3d4\",\n      \"task_id\": \"data_extraction\",\n      \"status\": \"completed\"\n    },\n    {\n      \"id\": \"data_transformation_b2c3d4e5\",\n      \"task_id\": \"data_transformation\",\n      \"status\": \"completed\"\n    },\n    {\n      \"id\": \"data_loading_c3d4e5f6\",\n      \"task_id\": \"data_loading\",\n      \"status\": \"completed\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"from\": \"data_extraction_a1b2c3d4\",\n      \"to\": \"data_transformation_b2c3d4e5\"\n    },\n    {\n      \"from\": \"data_transformation_b2c3d4e5\",\n      \"to\": \"data_loading_c3d4e5f6\"\n    }\n  ]\n}\n\n\n\nGET /api/tasks/{task_run_id}/logs\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\ntail\nNumber of lines to return from the end\n\n\nsince\nReturn logs since timestamp (ISO 8601 format)\n\n\n\nExample response:\n{\n  \"task_run_id\": \"data_extraction_a1b2c3d4\",\n  \"logs\": \"Starting extraction...\\nExtracted 10000 records\\nCompleted successfully\",\n  \"has_more\": false,\n  \"start_time\": \"2023-11-01T12:00:00Z\",\n  \"end_time\": \"2023-11-01T12:10:00Z\"\n}\n\n\n\n\n\n\nGET /api/agents\nExample response:\n{\n  \"agents\": [\n    {\n      \"agent_id\": \"agent_57f3e2a1\",\n      \"last_heartbeat\": \"2023-11-01T12:35:00Z\",\n      \"status\": \"active\",\n      \"assigned_tasks\": [\n        \"data_extraction_a1b2c3d4\"\n      ],\n      \"queues\": [\n        \"default\",\n        \"high_memory\"\n      ],\n      \"tags\": [\n        \"region:us-east\",\n        \"type:general\"\n      ]\n    },\n    // More agents...\n  ]\n}\n\n\n\nGET /api/queues\nExample response:\n{\n  \"queues\": [\n    {\n      \"name\": \"default\",\n      \"length\": 5,\n      \"consumer_count\": 3,\n      \"tasks\": [\n        {\n          \"task_run_id\": \"data_extraction_a1b2c3d4\",\n          \"dag_run_id\": \"etl_20231101\",\n          \"position\": 0\n        },\n        // More tasks...\n      ]\n    },\n    // More queues...\n  ]\n}\n\n\n\nPOST /api/queues/{queue_name}/purge\nResponse:\n{\n  \"success\": true,\n  \"message\": \"Queue purged successfully\",\n  \"tasks_removed\": 5\n}\n\n\n\n\n\n\nGET /api/contexts\nExample response:\n{\n  \"contexts\": [\n    {\n      \"id\": \"development\",\n      \"variables\": {\n        \"ENV\": \"development\",\n        \"LOG_LEVEL\": \"debug\"\n      }\n    },\n    {\n      \"id\": \"production\",\n      \"variables\": {\n        \"ENV\": \"production\",\n        \"LOG_LEVEL\": \"info\"\n      }\n    }\n  ]\n}\n\n\n\nGET /api/contexts/{context_id}\nExample response:\n{\n  \"id\": \"production\",\n  \"variables\": {\n    \"ENV\": \"production\",\n    \"LOG_LEVEL\": \"info\",\n    \"DATA_PATH\": \"/data/production\",\n    \"API_URL\": \"https://api.example.com\",\n    \"MAX_WORKERS\": \"16\"\n  },\n  \"extends\": null,\n  \"created_at\": \"2023-01-01T00:00:00Z\",\n  \"updated_at\": \"2023-01-15T00:00:00Z\"\n}\n\n\n\nPUT /api/contexts/{context_id}\nRequest body:\n{\n  \"variables\": {\n    \"ENV\": \"production\",\n    \"LOG_LEVEL\": \"info\",\n    \"DATA_PATH\": \"/data/production\",\n    \"API_URL\": \"https://api.example.com\",\n    \"MAX_WORKERS\": \"16\"\n  },\n  \"extends\": null\n}\nResponse: The created/updated context object.\n\n\n\nGET /api/parameter-sets\nExample response:\n{\n  \"parameter_sets\": [\n    {\n      \"id\": \"small_training\",\n      \"parameters\": {\n        \"EPOCHS\": \"10\",\n        \"BATCH_SIZE\": \"16\"\n      }\n    },\n    {\n      \"id\": \"large_training\",\n      \"parameters\": {\n        \"EPOCHS\": \"100\",\n        \"BATCH_SIZE\": \"128\"\n      }\n    }\n  ]\n}\n\n\n\nGET /api/parameter-sets/{parameter_set_id}\nExample response:\n{\n  \"id\": \"large_training\",\n  \"parameters\": {\n    \"EPOCHS\": \"100\",\n    \"BATCH_SIZE\": \"128\",\n    \"LEARNING_RATE\": \"0.001\"\n  },\n  \"created_at\": \"2023-01-01T00:00:00Z\",\n  \"updated_at\": \"2023-01-15T00:00:00Z\"\n}\n\n\n\nPUT /api/parameter-sets/{parameter_set_id}\nRequest body:\n{\n  \"parameters\": {\n    \"EPOCHS\": \"100\",\n    \"BATCH_SIZE\": \"128\",\n    \"LEARNING_RATE\": \"0.001\"\n  }\n}\nResponse: The created/updated parameter set object.\n\n\n\n\n\n\nGET /api/status\nExample response:\n{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.0\",\n  \"uptime_seconds\": 86400,\n  \"backend\": {\n    \"type\": \"redis\",\n    \"status\": \"connected\",\n    \"latency_ms\": 5\n  },\n  \"components\": {\n    \"orchestrator\": {\n      \"status\": \"running\",\n      \"count\": 1\n    },\n    \"agents\": {\n      \"status\": \"running\",\n      \"count\": 3,\n      \"active\": 3,\n      \"inactive\": 0\n    },\n    \"ui\": {\n      \"status\": \"running\"\n    }\n  },\n  \"queues\": {\n    \"total_pending_tasks\": 10,\n    \"queues\": [\n      {\n        \"name\": \"default\",\n        \"length\": 5\n      },\n      {\n        \"name\": \"high_memory\",\n        \"length\": 3\n      },\n      {\n        \"name\": \"gpu_tasks\",\n        \"length\": 2\n      }\n    ]\n  }\n}\n\n\n\nGET /api/health\nExample response:\n{\n  \"status\": \"healthy\",\n  \"components\": {\n    \"backend\": \"healthy\",\n    \"orchestrator\": \"healthy\",\n    \"agents\": \"healthy\"\n  }\n}\n\n\n\nGET /api/metrics\nExample response:\n{\n  \"tasks\": {\n    \"pending\": 10,\n    \"running\": 5,\n    \"completed\": 100,\n    \"failed\": 2,\n    \"total\": 117\n  },\n  \"runs\": {\n    \"pending\": 1,\n    \"running\": 2,\n    \"completed\": 20,\n    \"failed\": 1,\n    \"total\": 24\n  },\n  \"performance\": {\n    \"average_task_duration_seconds\": 45.2,\n    \"average_dag_duration_seconds\": 320.5,\n    \"tasks_per_minute\": 12.5\n  },\n  \"resources\": {\n    \"cpu_usage_percent\": 25.5,\n    \"memory_usage_mb\": 1024.5,\n    \"disk_usage_percent\": 45.2\n  }\n}\n\n\n\n\n\n\nPOST /api/webhooks\nRequest body:\n{\n  \"name\": \"Task Completion Notification\",\n  \"url\": \"https://example.com/webhook\",\n  \"events\": [\"task.completed\", \"task.failed\", \"dag.completed\"],\n  \"secret\": \"your-webhook-secret\",\n  \"headers\": {\n    \"Custom-Header\": \"custom-value\"\n  },\n  \"filters\": {\n    \"dag_id\": \"etl_pipeline\",\n    \"task_id\": \"data_extraction\"\n  }\n}\nResponse:\n{\n  \"id\": \"webhook_a1b2c3d4\",\n  \"name\": \"Task Completion Notification\",\n  \"url\": \"https://example.com/webhook\",\n  \"events\": [\"task.completed\", \"task.failed\", \"dag.completed\"],\n  \"filters\": {\n    \"dag_id\": \"etl_pipeline\",\n    \"task_id\": \"data_extraction\"\n  },\n  \"created_at\": \"2023-11-01T12:00:00Z\"\n}\n\n\n\nGET /api/webhooks\nExample response:\n{\n  \"webhooks\": [\n    {\n      \"id\": \"webhook_a1b2c3d4\",\n      \"name\": \"Task Completion Notification\",\n      \"url\": \"https://example.com/webhook\",\n      \"events\": [\"task.completed\", \"task.failed\", \"dag.completed\"],\n      \"filters\": {\n        \"dag_id\": \"etl_pipeline\",\n        \"task_id\": \"data_extraction\"\n      },\n      \"created_at\": \"2023-11-01T12:00:00Z\"\n    },\n    // More webhooks...\n  ]\n}\n\n\n\nDELETE /api/webhooks/{webhook_id}\nResponse:\n{\n  \"success\": true,\n  \"message\": \"Webhook deleted successfully\"\n}\n\n\n\n\nThe following event types can be used with webhooks:\n\n\n\nEvent Type\nDescription\n\n\n\n\ntask.pending\nTask is pending execution\n\n\ntask.queued\nTask has been queued\n\n\ntask.running\nTask has started running\n\n\ntask.completed\nTask has completed successfully\n\n\ntask.failed\nTask has failed\n\n\ndag.pending\nDAG is pending execution\n\n\ndag.running\nDAG has started running\n\n\ndag.completed\nDAG has completed successfully\n\n\ndag.failed\nDAG has failed\n\n\nagent.registered\nNew agent has registered\n\n\nagent.offline\nAgent has gone offline\n\n\n\n\n\n\nWhen an event occurs, Cyclonetix sends a POST request to the registered webhook URL with the following payload:\n{\n  \"event\": \"task.completed\",\n  \"timestamp\": \"2023-11-01T12:10:00Z\",\n  \"data\": {\n    \"run_id\": \"etl_20231101\",\n    \"dag_id\": \"etl_pipeline\",\n    \"task_id\": \"data_extraction\",\n    \"task_run_id\": \"data_extraction_a1b2c3d4\",\n    \"status\": \"completed\",\n    \"start_time\": \"2023-11-01T12:00:00Z\",\n    \"end_time\": \"2023-11-01T12:10:00Z\",\n    \"duration_seconds\": 600\n  }\n}\nFor security, Cyclonetix includes an X-Cyclonetix-Signature header with an HMAC-SHA256 signature of the payload using your webhook secret.\n\n\n\n\nAll API endpoints return standard HTTP status codes:\n\n200 OK: Request successful\n201 Created: Resource created successfully\n400 Bad Request: Invalid request parameters\n401 Unauthorized: Authentication required\n403 Forbidden: Permission denied\n404 Not Found: Resource not found\n409 Conflict: Resource already exists\n500 Internal Server Error: Server error\n\nError responses have a consistent format:\n{\n  \"error\": true,\n  \"code\": \"validation_error\",\n  \"message\": \"Invalid parameter value\",\n  \"details\": {\n    \"field\": \"timeout_seconds\",\n    \"error\": \"Must be a positive integer\"\n  }\n}\n\n\n\nList endpoints support pagination with the following parameters:\n\nlimit: Maximum number of items to return\noffset: Number of items to skip\n\nResponse includes metadata for pagination:\n{\n  \"items\": [...],\n  \"total\": 100,\n  \"limit\": 10,\n  \"offset\": 0,\n  \"next\": \"/api/tasks?limit=10&offset=10\",\n  \"prev\": null\n}\n\n\n\nMany endpoints support filtering and sorting:\n\nFiltering: Use query parameters like ?status=completed&dag_id=etl_pipeline\nSorting: Use the sort parameter, e.g., ?sort=start_time:desc\n\n\n\n\n\nReview the CLI Reference for command-line interface options\nCheck the Configuration Reference for configuration details\nExplore the YAML Schema for task and DAG definitions",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api.html#api-base-url",
    "href": "reference/api.html#api-base-url",
    "title": "API Reference",
    "section": "",
    "text": "By default, the API is available at:\nhttp://&lt;cyclonetix-host&gt;:3000/api",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api.html#authentication",
    "href": "reference/api.html#authentication",
    "title": "API Reference",
    "section": "",
    "text": "When security is enabled, you can authenticate using:\n\nAPI Key: Pass the API key in the X-API-Key header:\nX-API-Key: your-api-key-here\nOAuth Token: After OAuth authentication, use the session cookie or pass the token in the Authorization header:\nAuthorization: Bearer your-token-here",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api.html#api-endpoints",
    "href": "reference/api.html#api-endpoints",
    "title": "API Reference",
    "section": "",
    "text": "GET /api/tasks\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\nfilter\nFilter tasks by name or ID pattern\n\n\nqueue\nFilter tasks by queue\n\n\nlimit\nMaximum number of tasks to return\n\n\noffset\nPagination offset\n\n\n\nExample response:\n{\n  \"tasks\": [\n    {\n      \"id\": \"data_extraction\",\n      \"name\": \"Data Extraction\",\n      \"description\": \"Extract data from source system\",\n      \"command\": \"python extract.py --source ${SOURCE}\",\n      \"dependencies\": [],\n      \"parameters\": {\n        \"SOURCE\": \"prod_db\"\n      },\n      \"queue\": \"default\"\n    },\n    // More tasks...\n  ],\n  \"total\": 24,\n  \"limit\": 10,\n  \"offset\": 0\n}\n\n\n\nGET /api/tasks/{task_id}\nExample response:\n{\n  \"id\": \"data_extraction\",\n  \"name\": \"Data Extraction\",\n  \"description\": \"Extract data from source system\",\n  \"command\": \"python extract.py --source ${SOURCE}\",\n  \"dependencies\": [],\n  \"parameters\": {\n    \"SOURCE\": \"prod_db\"\n  },\n  \"queue\": \"default\",\n  \"created_at\": \"2023-01-01T00:00:00Z\",\n  \"updated_at\": \"2023-01-15T00:00:00Z\"\n}\n\n\n\nPUT /api/tasks/{task_id}\nRequest body:\n{\n  \"name\": \"Data Extraction\",\n  \"description\": \"Extract data from source system\",\n  \"command\": \"python extract.py --source ${SOURCE}\",\n  \"dependencies\": [],\n  \"parameters\": {\n    \"SOURCE\": \"prod_db\"\n  },\n  \"queue\": \"default\"\n}\nResponse: The created/updated task object.\n\n\n\nDELETE /api/tasks/{task_id}\nResponse:\n{\n  \"success\": true,\n  \"message\": \"Task deleted successfully\"\n}\n\n\n\n\n\n\nGET /api/dags\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\nfilter\nFilter DAGs by name or ID pattern\n\n\nlimit\nMaximum number of DAGs to return\n\n\noffset\nPagination offset\n\n\n\nExample response:\n{\n  \"dags\": [\n    {\n      \"id\": \"etl_pipeline\",\n      \"name\": \"ETL Pipeline\",\n      \"description\": \"Extract, transform, and load data\",\n      \"tasks\": [\"data_extraction\", \"data_transformation\", \"data_loading\"],\n      \"tags\": [\"etl\", \"daily\"]\n    },\n    // More DAGs...\n  ],\n  \"total\": 12,\n  \"limit\": 10,\n  \"offset\": 0\n}\n\n\n\nGET /api/dags/{dag_id}\nExample response:\n{\n  \"id\": \"etl_pipeline\",\n  \"name\": \"ETL Pipeline\",\n  \"description\": \"Extract, transform, and load data\",\n  \"tasks\": [\n    {\n      \"id\": \"data_extraction\",\n      \"parameters\": {\n        \"SOURCE\": \"prod_db\"\n      }\n    },\n    {\n      \"id\": \"data_transformation\",\n      \"parameters\": {\n        \"TRANSFORM_TYPE\": \"standard\"\n      }\n    },\n    {\n      \"id\": \"data_loading\",\n      \"parameters\": {\n        \"DESTINATION\": \"data_warehouse\"\n      }\n    }\n  ],\n  \"context\": \"production\",\n  \"tags\": [\"etl\", \"daily\"],\n  \"created_at\": \"2023-01-01T00:00:00Z\",\n  \"updated_at\": \"2023-01-15T00:00:00Z\"\n}\n\n\n\nPUT /api/dags/{dag_id}\nRequest body:\n{\n  \"name\": \"ETL Pipeline\",\n  \"description\": \"Extract, transform, and load data\",\n  \"tasks\": [\n    {\n      \"id\": \"data_extraction\",\n      \"parameters\": {\n        \"SOURCE\": \"prod_db\"\n      }\n    },\n    {\n      \"id\": \"data_transformation\",\n      \"parameters\": {\n        \"TRANSFORM_TYPE\": \"standard\"\n      }\n    },\n    {\n      \"id\": \"data_loading\",\n      \"parameters\": {\n        \"DESTINATION\": \"data_warehouse\"\n      }\n    }\n  ],\n  \"context\": \"production\",\n  \"tags\": [\"etl\", \"daily\"]\n}\nResponse: The created/updated DAG object.\n\n\n\nDELETE /api/dags/{dag_id}\nResponse:\n{\n  \"success\": true,\n  \"message\": \"DAG deleted successfully\"\n}\n\n\n\n\n\n\nPOST /api/schedule-task\nRequest body:\n{\n  \"task_id\": \"data_extraction\",\n  \"context\": \"production\",\n  \"env_vars\": {\n    \"DATA_DATE\": \"2023-11-01\",\n    \"LOG_LEVEL\": \"info\"\n  },\n  \"parameters\": {\n    \"SOURCE\": \"prod_db\",\n    \"LIMIT\": \"1000\"\n  },\n  \"run_id\": \"custom_run_id_20231101\",  // Optional\n  \"queue\": \"high_priority\",  // Optional\n  \"timeout_seconds\": 3600,  // Optional\n  \"retries\": 3,  // Optional\n  \"retry_delay_seconds\": 300  // Optional\n}\nResponse:\n{\n  \"success\": true,\n  \"run_id\": \"custom_run_id_20231101\",\n  \"dag_run_id\": \"dag_57f3e2a1\",\n  \"status\": \"pending\",\n  \"start_time\": \"2023-11-01T12:00:00Z\"\n}\n\n\n\nPOST /api/schedule-dag\nRequest body:\n{\n  \"dag_id\": \"etl_pipeline\",\n  \"context\": \"production\",\n  \"env_vars\": {\n    \"DATA_DATE\": \"2023-11-01\"\n  },\n  \"parameters\": {\n    \"data_extraction.SOURCE\": \"prod_db\",\n    \"data_loading.DESTINATION\": \"data_warehouse\"\n  },\n  \"run_id\": \"etl_20231101\",  // Optional\n  \"timeout_seconds\": 7200  // Optional\n}\nResponse:\n{\n  \"success\": true,\n  \"run_id\": \"etl_20231101\",\n  \"status\": \"pending\",\n  \"start_time\": \"2023-11-01T12:00:00Z\",\n  \"tasks\": [\n    {\n      \"id\": \"data_extraction\",\n      \"task_run_id\": \"data_extraction_a1b2c3d4\",\n      \"status\": \"pending\"\n    },\n    {\n      \"id\": \"data_transformation\",\n      \"task_run_id\": \"data_transformation_b2c3d4e5\",\n      \"status\": \"pending\"\n    },\n    {\n      \"id\": \"data_loading\",\n      \"task_run_id\": \"data_loading_c3d4e5f6\",\n      \"status\": \"pending\"\n    }\n  ]\n}\n\n\n\nPOST /api/cancel-run/{run_id}\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\nforce\nForce cancel even if in critical section (true/false)\n\n\n\nResponse:\n{\n  \"success\": true,\n  \"message\": \"Run cancelled successfully\",\n  \"run_id\": \"etl_20231101\"\n}\n\n\n\nPOST /api/rerun/{run_id}\nRequest body:\n{\n  \"failed_only\": true,  // Optional\n  \"from_task\": \"data_transformation\",  // Optional\n  \"reset_params\": false  // Optional\n}\nResponse:\n{\n  \"success\": true,\n  \"new_run_id\": \"etl_20231101_rerun\",\n  \"original_run_id\": \"etl_20231101\",\n  \"status\": \"pending\",\n  \"start_time\": \"2023-11-01T14:00:00Z\"\n}\n\n\n\n\n\n\nGET /api/runs\nQuery parameters:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nstatus\nFilter by status (pending, running, completed, failed)\n\n\ndag_id\nFilter by DAG ID\n\n\nsince\nShow runs since time (ISO 8601 format)\n\n\nuntil\nShow runs until time (ISO 8601 format)\n\n\nlimit\nMaximum number of runs to return\n\n\noffset\nPagination offset\n\n\n\nExample response:\n{\n  \"runs\": [\n    {\n      \"run_id\": \"etl_20231101\",\n      \"dag_id\": \"etl_pipeline\",\n      \"status\": \"completed\",\n      \"start_time\": \"2023-11-01T12:00:00Z\",\n      \"end_time\": \"2023-11-01T12:30:00Z\",\n      \"task_count\": 3,\n      \"completed_tasks\": 3,\n      \"failed_tasks\": 0\n    },\n    // More runs...\n  ],\n  \"total\": 45,\n  \"limit\": 10,\n  \"offset\": 0\n}\n\n\n\nGET /api/runs/{run_id}\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\ninclude_tasks\nInclude task details (true/false)\n\n\ninclude_logs\nInclude task logs (true/false)\n\n\n\nExample response:\n{\n  \"run_id\": \"etl_20231101\",\n  \"dag_id\": \"etl_pipeline\",\n  \"status\": \"completed\",\n  \"start_time\": \"2023-11-01T12:00:00Z\",\n  \"end_time\": \"2023-11-01T12:30:00Z\",\n  \"duration_seconds\": 1800,\n  \"task_count\": 3,\n  \"completed_tasks\": 3,\n  \"failed_tasks\": 0,\n  \"tasks\": [\n    {\n      \"task_id\": \"data_extraction\",\n      \"task_run_id\": \"data_extraction_a1b2c3d4\",\n      \"status\": \"completed\",\n      \"start_time\": \"2023-11-01T12:00:00Z\",\n      \"end_time\": \"2023-11-01T12:10:00Z\",\n      \"duration_seconds\": 600,\n      \"logs\": \"Starting extraction...\\nExtracted 10000 records\\nCompleted successfully\"\n    },\n    // More tasks...\n  ],\n  \"context\": {\n    \"DATA_DATE\": \"2023-11-01\",\n    \"ENV\": \"production\"\n  }\n}\n\n\n\nGET /api/runs/{run_id}/graph\nExample response:\n{\n  \"run_id\": \"etl_20231101\",\n  \"dag_id\": \"etl_pipeline\",\n  \"nodes\": [\n    {\n      \"id\": \"data_extraction_a1b2c3d4\",\n      \"task_id\": \"data_extraction\",\n      \"status\": \"completed\"\n    },\n    {\n      \"id\": \"data_transformation_b2c3d4e5\",\n      \"task_id\": \"data_transformation\",\n      \"status\": \"completed\"\n    },\n    {\n      \"id\": \"data_loading_c3d4e5f6\",\n      \"task_id\": \"data_loading\",\n      \"status\": \"completed\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"from\": \"data_extraction_a1b2c3d4\",\n      \"to\": \"data_transformation_b2c3d4e5\"\n    },\n    {\n      \"from\": \"data_transformation_b2c3d4e5\",\n      \"to\": \"data_loading_c3d4e5f6\"\n    }\n  ]\n}\n\n\n\nGET /api/tasks/{task_run_id}/logs\nQuery parameters:\n\n\n\nParameter\nDescription\n\n\n\n\ntail\nNumber of lines to return from the end\n\n\nsince\nReturn logs since timestamp (ISO 8601 format)\n\n\n\nExample response:\n{\n  \"task_run_id\": \"data_extraction_a1b2c3d4\",\n  \"logs\": \"Starting extraction...\\nExtracted 10000 records\\nCompleted successfully\",\n  \"has_more\": false,\n  \"start_time\": \"2023-11-01T12:00:00Z\",\n  \"end_time\": \"2023-11-01T12:10:00Z\"\n}\n\n\n\n\n\n\nGET /api/agents\nExample response:\n{\n  \"agents\": [\n    {\n      \"agent_id\": \"agent_57f3e2a1\",\n      \"last_heartbeat\": \"2023-11-01T12:35:00Z\",\n      \"status\": \"active\",\n      \"assigned_tasks\": [\n        \"data_extraction_a1b2c3d4\"\n      ],\n      \"queues\": [\n        \"default\",\n        \"high_memory\"\n      ],\n      \"tags\": [\n        \"region:us-east\",\n        \"type:general\"\n      ]\n    },\n    // More agents...\n  ]\n}\n\n\n\nGET /api/queues\nExample response:\n{\n  \"queues\": [\n    {\n      \"name\": \"default\",\n      \"length\": 5,\n      \"consumer_count\": 3,\n      \"tasks\": [\n        {\n          \"task_run_id\": \"data_extraction_a1b2c3d4\",\n          \"dag_run_id\": \"etl_20231101\",\n          \"position\": 0\n        },\n        // More tasks...\n      ]\n    },\n    // More queues...\n  ]\n}\n\n\n\nPOST /api/queues/{queue_name}/purge\nResponse:\n{\n  \"success\": true,\n  \"message\": \"Queue purged successfully\",\n  \"tasks_removed\": 5\n}\n\n\n\n\n\n\nGET /api/contexts\nExample response:\n{\n  \"contexts\": [\n    {\n      \"id\": \"development\",\n      \"variables\": {\n        \"ENV\": \"development\",\n        \"LOG_LEVEL\": \"debug\"\n      }\n    },\n    {\n      \"id\": \"production\",\n      \"variables\": {\n        \"ENV\": \"production\",\n        \"LOG_LEVEL\": \"info\"\n      }\n    }\n  ]\n}\n\n\n\nGET /api/contexts/{context_id}\nExample response:\n{\n  \"id\": \"production\",\n  \"variables\": {\n    \"ENV\": \"production\",\n    \"LOG_LEVEL\": \"info\",\n    \"DATA_PATH\": \"/data/production\",\n    \"API_URL\": \"https://api.example.com\",\n    \"MAX_WORKERS\": \"16\"\n  },\n  \"extends\": null,\n  \"created_at\": \"2023-01-01T00:00:00Z\",\n  \"updated_at\": \"2023-01-15T00:00:00Z\"\n}\n\n\n\nPUT /api/contexts/{context_id}\nRequest body:\n{\n  \"variables\": {\n    \"ENV\": \"production\",\n    \"LOG_LEVEL\": \"info\",\n    \"DATA_PATH\": \"/data/production\",\n    \"API_URL\": \"https://api.example.com\",\n    \"MAX_WORKERS\": \"16\"\n  },\n  \"extends\": null\n}\nResponse: The created/updated context object.\n\n\n\nGET /api/parameter-sets\nExample response:\n{\n  \"parameter_sets\": [\n    {\n      \"id\": \"small_training\",\n      \"parameters\": {\n        \"EPOCHS\": \"10\",\n        \"BATCH_SIZE\": \"16\"\n      }\n    },\n    {\n      \"id\": \"large_training\",\n      \"parameters\": {\n        \"EPOCHS\": \"100\",\n        \"BATCH_SIZE\": \"128\"\n      }\n    }\n  ]\n}\n\n\n\nGET /api/parameter-sets/{parameter_set_id}\nExample response:\n{\n  \"id\": \"large_training\",\n  \"parameters\": {\n    \"EPOCHS\": \"100\",\n    \"BATCH_SIZE\": \"128\",\n    \"LEARNING_RATE\": \"0.001\"\n  },\n  \"created_at\": \"2023-01-01T00:00:00Z\",\n  \"updated_at\": \"2023-01-15T00:00:00Z\"\n}\n\n\n\nPUT /api/parameter-sets/{parameter_set_id}\nRequest body:\n{\n  \"parameters\": {\n    \"EPOCHS\": \"100\",\n    \"BATCH_SIZE\": \"128\",\n    \"LEARNING_RATE\": \"0.001\"\n  }\n}\nResponse: The created/updated parameter set object.\n\n\n\n\n\n\nGET /api/status\nExample response:\n{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.0\",\n  \"uptime_seconds\": 86400,\n  \"backend\": {\n    \"type\": \"redis\",\n    \"status\": \"connected\",\n    \"latency_ms\": 5\n  },\n  \"components\": {\n    \"orchestrator\": {\n      \"status\": \"running\",\n      \"count\": 1\n    },\n    \"agents\": {\n      \"status\": \"running\",\n      \"count\": 3,\n      \"active\": 3,\n      \"inactive\": 0\n    },\n    \"ui\": {\n      \"status\": \"running\"\n    }\n  },\n  \"queues\": {\n    \"total_pending_tasks\": 10,\n    \"queues\": [\n      {\n        \"name\": \"default\",\n        \"length\": 5\n      },\n      {\n        \"name\": \"high_memory\",\n        \"length\": 3\n      },\n      {\n        \"name\": \"gpu_tasks\",\n        \"length\": 2\n      }\n    ]\n  }\n}\n\n\n\nGET /api/health\nExample response:\n{\n  \"status\": \"healthy\",\n  \"components\": {\n    \"backend\": \"healthy\",\n    \"orchestrator\": \"healthy\",\n    \"agents\": \"healthy\"\n  }\n}\n\n\n\nGET /api/metrics\nExample response:\n{\n  \"tasks\": {\n    \"pending\": 10,\n    \"running\": 5,\n    \"completed\": 100,\n    \"failed\": 2,\n    \"total\": 117\n  },\n  \"runs\": {\n    \"pending\": 1,\n    \"running\": 2,\n    \"completed\": 20,\n    \"failed\": 1,\n    \"total\": 24\n  },\n  \"performance\": {\n    \"average_task_duration_seconds\": 45.2,\n    \"average_dag_duration_seconds\": 320.5,\n    \"tasks_per_minute\": 12.5\n  },\n  \"resources\": {\n    \"cpu_usage_percent\": 25.5,\n    \"memory_usage_mb\": 1024.5,\n    \"disk_usage_percent\": 45.2\n  }\n}\n\n\n\n\n\n\nPOST /api/webhooks\nRequest body:\n{\n  \"name\": \"Task Completion Notification\",\n  \"url\": \"https://example.com/webhook\",\n  \"events\": [\"task.completed\", \"task.failed\", \"dag.completed\"],\n  \"secret\": \"your-webhook-secret\",\n  \"headers\": {\n    \"Custom-Header\": \"custom-value\"\n  },\n  \"filters\": {\n    \"dag_id\": \"etl_pipeline\",\n    \"task_id\": \"data_extraction\"\n  }\n}\nResponse:\n{\n  \"id\": \"webhook_a1b2c3d4\",\n  \"name\": \"Task Completion Notification\",\n  \"url\": \"https://example.com/webhook\",\n  \"events\": [\"task.completed\", \"task.failed\", \"dag.completed\"],\n  \"filters\": {\n    \"dag_id\": \"etl_pipeline\",\n    \"task_id\": \"data_extraction\"\n  },\n  \"created_at\": \"2023-11-01T12:00:00Z\"\n}\n\n\n\nGET /api/webhooks\nExample response:\n{\n  \"webhooks\": [\n    {\n      \"id\": \"webhook_a1b2c3d4\",\n      \"name\": \"Task Completion Notification\",\n      \"url\": \"https://example.com/webhook\",\n      \"events\": [\"task.completed\", \"task.failed\", \"dag.completed\"],\n      \"filters\": {\n        \"dag_id\": \"etl_pipeline\",\n        \"task_id\": \"data_extraction\"\n      },\n      \"created_at\": \"2023-11-01T12:00:00Z\"\n    },\n    // More webhooks...\n  ]\n}\n\n\n\nDELETE /api/webhooks/{webhook_id}\nResponse:\n{\n  \"success\": true,\n  \"message\": \"Webhook deleted successfully\"\n}\n\n\n\n\nThe following event types can be used with webhooks:\n\n\n\nEvent Type\nDescription\n\n\n\n\ntask.pending\nTask is pending execution\n\n\ntask.queued\nTask has been queued\n\n\ntask.running\nTask has started running\n\n\ntask.completed\nTask has completed successfully\n\n\ntask.failed\nTask has failed\n\n\ndag.pending\nDAG is pending execution\n\n\ndag.running\nDAG has started running\n\n\ndag.completed\nDAG has completed successfully\n\n\ndag.failed\nDAG has failed\n\n\nagent.registered\nNew agent has registered\n\n\nagent.offline\nAgent has gone offline\n\n\n\n\n\n\nWhen an event occurs, Cyclonetix sends a POST request to the registered webhook URL with the following payload:\n{\n  \"event\": \"task.completed\",\n  \"timestamp\": \"2023-11-01T12:10:00Z\",\n  \"data\": {\n    \"run_id\": \"etl_20231101\",\n    \"dag_id\": \"etl_pipeline\",\n    \"task_id\": \"data_extraction\",\n    \"task_run_id\": \"data_extraction_a1b2c3d4\",\n    \"status\": \"completed\",\n    \"start_time\": \"2023-11-01T12:00:00Z\",\n    \"end_time\": \"2023-11-01T12:10:00Z\",\n    \"duration_seconds\": 600\n  }\n}\nFor security, Cyclonetix includes an X-Cyclonetix-Signature header with an HMAC-SHA256 signature of the payload using your webhook secret.",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api.html#error-handling",
    "href": "reference/api.html#error-handling",
    "title": "API Reference",
    "section": "",
    "text": "All API endpoints return standard HTTP status codes:\n\n200 OK: Request successful\n201 Created: Resource created successfully\n400 Bad Request: Invalid request parameters\n401 Unauthorized: Authentication required\n403 Forbidden: Permission denied\n404 Not Found: Resource not found\n409 Conflict: Resource already exists\n500 Internal Server Error: Server error\n\nError responses have a consistent format:\n{\n  \"error\": true,\n  \"code\": \"validation_error\",\n  \"message\": \"Invalid parameter value\",\n  \"details\": {\n    \"field\": \"timeout_seconds\",\n    \"error\": \"Must be a positive integer\"\n  }\n}",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api.html#pagination",
    "href": "reference/api.html#pagination",
    "title": "API Reference",
    "section": "",
    "text": "List endpoints support pagination with the following parameters:\n\nlimit: Maximum number of items to return\noffset: Number of items to skip\n\nResponse includes metadata for pagination:\n{\n  \"items\": [...],\n  \"total\": 100,\n  \"limit\": 10,\n  \"offset\": 0,\n  \"next\": \"/api/tasks?limit=10&offset=10\",\n  \"prev\": null\n}",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api.html#filtering-and-sorting",
    "href": "reference/api.html#filtering-and-sorting",
    "title": "API Reference",
    "section": "",
    "text": "Many endpoints support filtering and sorting:\n\nFiltering: Use query parameters like ?status=completed&dag_id=etl_pipeline\nSorting: Use the sort parameter, e.g., ?sort=start_time:desc",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api.html#next-steps",
    "href": "reference/api.html#next-steps",
    "title": "API Reference",
    "section": "",
    "text": "Review the CLI Reference for command-line interface options\nCheck the Configuration Reference for configuration details\nExplore the YAML Schema for task and DAG definitions",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "advanced/contexts-parameters.html",
    "href": "advanced/contexts-parameters.html",
    "title": "Contexts & Parameters",
    "section": "",
    "text": "Contexts and parameters are powerful features in Cyclonetix that allow you to make your workflows flexible, reusable, and environment-aware. This guide explains how to use these features effectively.\n\n\nA context is a collection of variables that can be passed to tasks during execution. Contexts allow you to:\n\nShare environment variables across tasks\nDefine environment-specific configurations\nOverride values at different levels\nPropagate data between workflow steps\n\n\n\nContexts are defined in YAML files in the data/contexts directory:\n# data/contexts/development.yaml\nid: \"development\"\nvariables:\n  ENV: \"development\"\n  LOG_LEVEL: \"debug\"\n  DATA_PATH: \"/tmp/data\"\n  API_URL: \"https://dev-api.example.com\"\n  MAX_WORKERS: \"4\"\n# data/contexts/production.yaml\nid: \"production\"\nvariables:\n  ENV: \"production\"\n  LOG_LEVEL: \"info\"\n  DATA_PATH: \"/data/production\"\n  API_URL: \"https://api.example.com\"\n  MAX_WORKERS: \"16\"\n\n\n\nContext variables are automatically available as environment variables in tasks:\n# Task definition\nid: \"process_data\"\nname: \"Process Data\"\ncommand: \"python process.py --log-level ${LOG_LEVEL} --path ${DATA_PATH} --workers ${MAX_WORKERS}\"\n\n\n\nYou can specify which context to use when scheduling a workflow:\n# CLI\n./cyclonetix schedule-task process_data --context production\n\n# API\ncurl -X POST \"http://localhost:3000/api/schedule-task\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"task_id\": \"process_data\", \"context\": \"production\"}'\n\n\n\nSet a default context in your configuration:\n# config.yaml\ndefault_context: \"development\"\n\n\n\nContexts can inherit from each other to create hierarchies:\n# data/contexts/staging.yaml\nid: \"staging\"\nextends: \"production\"  # Inherit from production\nvariables:\n  ENV: \"staging\"\n  API_URL: \"https://staging-api.example.com\"\n  # Other variables inherited from production\n\n\n\nContexts support macros for dynamic values:\nvariables:\n  RUN_DATE: \"${DATE}\"         # Current date (YYYY-MM-DD)\n  RUN_TIME: \"${TIME}\"         # Current time (HH:MM:SS)\n  RUN_DATETIME: \"${DATETIME}\" # Current datetime (YYYY-MM-DDTHH:MM:SS)\n  RUN_ID: \"${RUN_ID}\"         # ID of the current workflow run\n  RANDOM_ID: \"${RANDOM:16}\"   # 16-character random string\n\n\n\nVariables can be overridden at different levels:\n\nBase context definition (lowest priority)\nInherited context values\nDAG-level context overrides\nTask-level environment variables\nScheduling-time context overrides (highest priority)\n\n\n\n\n\nParameters are task-specific values that can be configured at scheduling time. Unlike contexts, which are global, parameters are scoped to specific tasks.\n\n\nDefine parameters in task definitions:\n# Task definition\nid: \"train_model\"\nname: \"Train Model\"\ncommand: \"python train.py --model-type ${MODEL_TYPE} --epochs ${EPOCHS} --batch-size ${BATCH_SIZE}\"\nparameters:\n  MODEL_TYPE: \"random_forest\"  # Default value\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"32\"\n\n\n\nParameters can be simple key-value pairs or more complex definitions:\nparameters:\n  # Simple parameter with default value\n  SIMPLE_PARAM: \"default_value\"\n\n  # Complex parameter with validation\n  COMPLEX_PARAM:\n    default: \"value1\"\n    description: \"Parameter description\"\n    required: true\n    options: [\"value1\", \"value2\", \"value3\"]\n    validation: \"^[a-z0-9_]+$\"\n\n\n\nParameter sets are reusable collections of parameters that can be applied to tasks:\n# data/parameter_sets/small_training.yaml\nid: \"small_training\"\nparameters:\n  EPOCHS: \"10\"\n  BATCH_SIZE: \"16\"\n  LEARNING_RATE: \"0.01\"\n# data/parameter_sets/large_training.yaml\nid: \"large_training\"\nparameters:\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"128\"\n  LEARNING_RATE: \"0.001\"\nApply parameter sets to tasks:\n# Task definition\nid: \"train_model\"\nname: \"Train Model\"\ncommand: \"python train.py --epochs ${EPOCHS} --batch-size ${BATCH_SIZE} --lr ${LEARNING_RATE}\"\nparameter_sets:\n  - \"small_training\"  # Default parameter set\n\n\n\nOverride parameters when scheduling a task:\n# CLI\n./cyclonetix schedule-task train_model --param EPOCHS=50 --param BATCH_SIZE=64\n\n# API\ncurl -X POST \"http://localhost:3000/api/schedule-task\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"task_id\": \"train_model\",\n    \"parameters\": {\n      \"EPOCHS\": \"50\",\n      \"BATCH_SIZE\": \"64\"\n    }\n  }'\n\n\n\nUse parameters to create conditional dependencies:\n# Task that depends on a specific parameter set\ndependencies:\n  - \"train_model:large_training\"  # Only depends on train_model when using large_training parameter set\n\n\n\n\nContexts and parameters work together to provide comprehensive configuration:\n# Scheduling with both context and parameters\n./cyclonetix schedule-task train_model \\\n  --context production \\\n  --param-set large_training \\\n  --param LEARNING_RATE=0.0005\nIn this example: 1. The production context provides global environment settings 2. The large_training parameter set provides task-specific defaults 3. The individual parameter override for LEARNING_RATE has highest priority\n\n\n\nYou can interpolate context variables in parameters and vice versa:\n# Context\nvariables:\n  BASE_PATH: \"/data\"\n  ENV: \"production\"\n\n# Task parameters\nparameters:\n  DATA_PATH: \"${BASE_PATH}/${ENV}/input\"  # Resolves to \"/data/production/input\"\n\n\n\n\n\n\nCreate environment-specific contexts (development, staging, production)\nUse namespaced variable names to avoid conflicts (e.g., APP_LOG_LEVEL vs DB_LOG_LEVEL)\nKeep sensitive information in separate contexts that can be secured separately\nDocument context variables with clear descriptions\n\n\n\n\n\nProvide sensible defaults for all parameters\nAdd validation rules for parameters with specific formats\nGroup related parameters into parameter sets\nVersion parameter sets alongside code changes\n\n\n\n\n\n\n\n# Context definition (data/contexts/production.yaml)\nid: \"production\"\nvariables:\n  ENV: \"production\"\n  BASE_PATH: \"/data\"\n  LOG_LEVEL: \"info\"\n  API_URL: \"https://api.example.com\"\n\n# Parameter set (data/parameter_sets/daily_processing.yaml)\nid: \"daily_processing\"\nparameters:\n  DATE: \"${DATE}\"\n  LIMIT: \"10000\"\n  MODE: \"incremental\"\n\n# Task definition\nid: \"process_daily_data\"\nname: \"Process Daily Data\"\ncommand: |\n  python process.py \\\n    --date ${DATE} \\\n    --input-path ${BASE_PATH}/raw/${DATE} \\\n    --output-path ${BASE_PATH}/processed/${DATE} \\\n    --mode ${MODE} \\\n    --limit ${LIMIT} \\\n    --log-level ${LOG_LEVEL}\nparameter_sets:\n  - \"daily_processing\"\n\n\n\n# Context for database connections\nid: \"db_connections\"\nvariables:\n  DB_HOST: \"db.example.com\"\n  DB_PORT: \"5432\"\n  DB_USER: \"app_user\"\n  DB_PASSWORD: \"${DB_PASSWORD}\"  # Will be filled from environment or secret store\n  DB_CONNECT_TIMEOUT: \"30\"\n\n# Task using database connection\nid: \"export_data\"\nname: \"Export Data to Database\"\ncommand: |\n  python export.py \\\n    --host ${DB_HOST} \\\n    --port ${DB_PORT} \\\n    --user ${DB_USER} \\\n    --password ${DB_PASSWORD} \\\n    --timeout ${DB_CONNECT_TIMEOUT} \\\n    --data-file ${DATA_FILE}\nparameters:\n  DATA_FILE: \"${BASE_PATH}/exports/data.csv\"\n\n\n\n\n\nLearn about Evaluation Points for dynamic workflows\nExplore Git Integration for version-controlled workflows\nCheck Troubleshooting & FAQ for common issues",
    "crumbs": [
      "Advanced Features",
      "Contexts & Parameters"
    ]
  },
  {
    "objectID": "advanced/contexts-parameters.html#contexts",
    "href": "advanced/contexts-parameters.html#contexts",
    "title": "Contexts & Parameters",
    "section": "",
    "text": "A context is a collection of variables that can be passed to tasks during execution. Contexts allow you to:\n\nShare environment variables across tasks\nDefine environment-specific configurations\nOverride values at different levels\nPropagate data between workflow steps\n\n\n\nContexts are defined in YAML files in the data/contexts directory:\n# data/contexts/development.yaml\nid: \"development\"\nvariables:\n  ENV: \"development\"\n  LOG_LEVEL: \"debug\"\n  DATA_PATH: \"/tmp/data\"\n  API_URL: \"https://dev-api.example.com\"\n  MAX_WORKERS: \"4\"\n# data/contexts/production.yaml\nid: \"production\"\nvariables:\n  ENV: \"production\"\n  LOG_LEVEL: \"info\"\n  DATA_PATH: \"/data/production\"\n  API_URL: \"https://api.example.com\"\n  MAX_WORKERS: \"16\"\n\n\n\nContext variables are automatically available as environment variables in tasks:\n# Task definition\nid: \"process_data\"\nname: \"Process Data\"\ncommand: \"python process.py --log-level ${LOG_LEVEL} --path ${DATA_PATH} --workers ${MAX_WORKERS}\"\n\n\n\nYou can specify which context to use when scheduling a workflow:\n# CLI\n./cyclonetix schedule-task process_data --context production\n\n# API\ncurl -X POST \"http://localhost:3000/api/schedule-task\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"task_id\": \"process_data\", \"context\": \"production\"}'\n\n\n\nSet a default context in your configuration:\n# config.yaml\ndefault_context: \"development\"\n\n\n\nContexts can inherit from each other to create hierarchies:\n# data/contexts/staging.yaml\nid: \"staging\"\nextends: \"production\"  # Inherit from production\nvariables:\n  ENV: \"staging\"\n  API_URL: \"https://staging-api.example.com\"\n  # Other variables inherited from production\n\n\n\nContexts support macros for dynamic values:\nvariables:\n  RUN_DATE: \"${DATE}\"         # Current date (YYYY-MM-DD)\n  RUN_TIME: \"${TIME}\"         # Current time (HH:MM:SS)\n  RUN_DATETIME: \"${DATETIME}\" # Current datetime (YYYY-MM-DDTHH:MM:SS)\n  RUN_ID: \"${RUN_ID}\"         # ID of the current workflow run\n  RANDOM_ID: \"${RANDOM:16}\"   # 16-character random string\n\n\n\nVariables can be overridden at different levels:\n\nBase context definition (lowest priority)\nInherited context values\nDAG-level context overrides\nTask-level environment variables\nScheduling-time context overrides (highest priority)",
    "crumbs": [
      "Advanced Features",
      "Contexts & Parameters"
    ]
  },
  {
    "objectID": "advanced/contexts-parameters.html#parameters",
    "href": "advanced/contexts-parameters.html#parameters",
    "title": "Contexts & Parameters",
    "section": "",
    "text": "Parameters are task-specific values that can be configured at scheduling time. Unlike contexts, which are global, parameters are scoped to specific tasks.\n\n\nDefine parameters in task definitions:\n# Task definition\nid: \"train_model\"\nname: \"Train Model\"\ncommand: \"python train.py --model-type ${MODEL_TYPE} --epochs ${EPOCHS} --batch-size ${BATCH_SIZE}\"\nparameters:\n  MODEL_TYPE: \"random_forest\"  # Default value\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"32\"\n\n\n\nParameters can be simple key-value pairs or more complex definitions:\nparameters:\n  # Simple parameter with default value\n  SIMPLE_PARAM: \"default_value\"\n\n  # Complex parameter with validation\n  COMPLEX_PARAM:\n    default: \"value1\"\n    description: \"Parameter description\"\n    required: true\n    options: [\"value1\", \"value2\", \"value3\"]\n    validation: \"^[a-z0-9_]+$\"\n\n\n\nParameter sets are reusable collections of parameters that can be applied to tasks:\n# data/parameter_sets/small_training.yaml\nid: \"small_training\"\nparameters:\n  EPOCHS: \"10\"\n  BATCH_SIZE: \"16\"\n  LEARNING_RATE: \"0.01\"\n# data/parameter_sets/large_training.yaml\nid: \"large_training\"\nparameters:\n  EPOCHS: \"100\"\n  BATCH_SIZE: \"128\"\n  LEARNING_RATE: \"0.001\"\nApply parameter sets to tasks:\n# Task definition\nid: \"train_model\"\nname: \"Train Model\"\ncommand: \"python train.py --epochs ${EPOCHS} --batch-size ${BATCH_SIZE} --lr ${LEARNING_RATE}\"\nparameter_sets:\n  - \"small_training\"  # Default parameter set\n\n\n\nOverride parameters when scheduling a task:\n# CLI\n./cyclonetix schedule-task train_model --param EPOCHS=50 --param BATCH_SIZE=64\n\n# API\ncurl -X POST \"http://localhost:3000/api/schedule-task\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"task_id\": \"train_model\",\n    \"parameters\": {\n      \"EPOCHS\": \"50\",\n      \"BATCH_SIZE\": \"64\"\n    }\n  }'\n\n\n\nUse parameters to create conditional dependencies:\n# Task that depends on a specific parameter set\ndependencies:\n  - \"train_model:large_training\"  # Only depends on train_model when using large_training parameter set",
    "crumbs": [
      "Advanced Features",
      "Contexts & Parameters"
    ]
  },
  {
    "objectID": "advanced/contexts-parameters.html#combining-contexts-and-parameters",
    "href": "advanced/contexts-parameters.html#combining-contexts-and-parameters",
    "title": "Contexts & Parameters",
    "section": "",
    "text": "Contexts and parameters work together to provide comprehensive configuration:\n# Scheduling with both context and parameters\n./cyclonetix schedule-task train_model \\\n  --context production \\\n  --param-set large_training \\\n  --param LEARNING_RATE=0.0005\nIn this example: 1. The production context provides global environment settings 2. The large_training parameter set provides task-specific defaults 3. The individual parameter override for LEARNING_RATE has highest priority",
    "crumbs": [
      "Advanced Features",
      "Contexts & Parameters"
    ]
  },
  {
    "objectID": "advanced/contexts-parameters.html#context-and-parameter-interpolation",
    "href": "advanced/contexts-parameters.html#context-and-parameter-interpolation",
    "title": "Contexts & Parameters",
    "section": "",
    "text": "You can interpolate context variables in parameters and vice versa:\n# Context\nvariables:\n  BASE_PATH: \"/data\"\n  ENV: \"production\"\n\n# Task parameters\nparameters:\n  DATA_PATH: \"${BASE_PATH}/${ENV}/input\"  # Resolves to \"/data/production/input\"",
    "crumbs": [
      "Advanced Features",
      "Contexts & Parameters"
    ]
  },
  {
    "objectID": "advanced/contexts-parameters.html#best-practices",
    "href": "advanced/contexts-parameters.html#best-practices",
    "title": "Contexts & Parameters",
    "section": "",
    "text": "Create environment-specific contexts (development, staging, production)\nUse namespaced variable names to avoid conflicts (e.g., APP_LOG_LEVEL vs DB_LOG_LEVEL)\nKeep sensitive information in separate contexts that can be secured separately\nDocument context variables with clear descriptions\n\n\n\n\n\nProvide sensible defaults for all parameters\nAdd validation rules for parameters with specific formats\nGroup related parameters into parameter sets\nVersion parameter sets alongside code changes",
    "crumbs": [
      "Advanced Features",
      "Contexts & Parameters"
    ]
  },
  {
    "objectID": "advanced/contexts-parameters.html#examples",
    "href": "advanced/contexts-parameters.html#examples",
    "title": "Contexts & Parameters",
    "section": "",
    "text": "# Context definition (data/contexts/production.yaml)\nid: \"production\"\nvariables:\n  ENV: \"production\"\n  BASE_PATH: \"/data\"\n  LOG_LEVEL: \"info\"\n  API_URL: \"https://api.example.com\"\n\n# Parameter set (data/parameter_sets/daily_processing.yaml)\nid: \"daily_processing\"\nparameters:\n  DATE: \"${DATE}\"\n  LIMIT: \"10000\"\n  MODE: \"incremental\"\n\n# Task definition\nid: \"process_daily_data\"\nname: \"Process Daily Data\"\ncommand: |\n  python process.py \\\n    --date ${DATE} \\\n    --input-path ${BASE_PATH}/raw/${DATE} \\\n    --output-path ${BASE_PATH}/processed/${DATE} \\\n    --mode ${MODE} \\\n    --limit ${LIMIT} \\\n    --log-level ${LOG_LEVEL}\nparameter_sets:\n  - \"daily_processing\"\n\n\n\n# Context for database connections\nid: \"db_connections\"\nvariables:\n  DB_HOST: \"db.example.com\"\n  DB_PORT: \"5432\"\n  DB_USER: \"app_user\"\n  DB_PASSWORD: \"${DB_PASSWORD}\"  # Will be filled from environment or secret store\n  DB_CONNECT_TIMEOUT: \"30\"\n\n# Task using database connection\nid: \"export_data\"\nname: \"Export Data to Database\"\ncommand: |\n  python export.py \\\n    --host ${DB_HOST} \\\n    --port ${DB_PORT} \\\n    --user ${DB_USER} \\\n    --password ${DB_PASSWORD} \\\n    --timeout ${DB_CONNECT_TIMEOUT} \\\n    --data-file ${DATA_FILE}\nparameters:\n  DATA_FILE: \"${BASE_PATH}/exports/data.csv\"",
    "crumbs": [
      "Advanced Features",
      "Contexts & Parameters"
    ]
  },
  {
    "objectID": "advanced/contexts-parameters.html#next-steps",
    "href": "advanced/contexts-parameters.html#next-steps",
    "title": "Contexts & Parameters",
    "section": "",
    "text": "Learn about Evaluation Points for dynamic workflows\nExplore Git Integration for version-controlled workflows\nCheck Troubleshooting & FAQ for common issues",
    "crumbs": [
      "Advanced Features",
      "Contexts & Parameters"
    ]
  },
  {
    "objectID": "advanced/agentic-flows.html",
    "href": "advanced/agentic-flows.html",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "Cyclonetix is designed to be AI-first from the ground up, integrating intelligent agents directly into the workflow orchestration engine. This document outlines the architecture and features for MCP (Model Context Protocol) integration and agentic workflow capabilities.\n\n\n\n\nTasks in Cyclonetix can be either traditional (shell commands) or agentic (MCP calls). Agentic tasks leverage AI agents to make intelligent decisions, analyze data, and dynamically modify workflow execution.\n\n\n\nEvaluation points serve as decision nodes where AI agents can:\n\nAnalyze workflow state and data\nMake intelligent routing decisions\nDetermine appropriate next steps based on context\nModify execution graphs dynamically\n\n\n\n\nCyclonetix combines high-speed stream processing (millions of events/second) with intelligent reasoning where it matters:\n\nFast path: Traditional stream processing for routine operations\nSmart path: Agentic analysis for edge cases, exceptions, and complex decisions\nHuman approval gates: Critical decisions requiring human oversight\n\n\n\n\n\n\n\n\nMCP server runs as sidecar to orchestrator\nAgent pool for concurrent task processing\nTask routing between traditional and agentic execution paths\nContext preservation across agent interactions\n\n\n\n\n\n\nid: \"process_data\"\nname: \"Process Daily Data\"\ncommand: \"python process_data.py --input ${INPUT_FILE}\"\ntype: \"traditional\"\n\n\n\nid: \"analyze_patterns\"\nname: \"Analyze Data Patterns\"\ncommand: \"mcp://pattern-analyzer\"\ntype: \"agentic\"\ncontext:\n  - data_history\n  - system_conditions\n  - business_rules\nevaluation_point: true\n\n\n\n\nAgents can dynamically modify workflow execution by:\n\nAdding new tasks based on analysis results\nModifying parameters for downstream tasks\nBranching workflows based on intelligent decisions\nUpdating context for subsequent operations\n\n\n\n\n\n\n\nworkflow: \"data_processing\"\ndescription: \"AI-powered data processing with dynamic response\"\ntasks:\n  - id: \"ingest_data\"\n    type: \"traditional\"\n    command: \"kafka-consumer --topic data-stream\"\n    \n  - id: \"analyze_patterns\"\n    type: \"agentic\" \n    command: \"mcp://pattern-analyzer\"\n    dependencies: [\"ingest_data\"]\n    evaluation_point: true\n    context:\n      - data_history\n      - system_metrics\n      - business_rules\n    \n  - id: \"generate_insights\"\n    type: \"conditional\"  # Created dynamically by agent\n    command: \"python generate_insights.py --severity ${SEVERITY}\"\n\n\n\nworkflow: \"compliance_monitoring\"\ndescription: \"Automated compliance monitoring and reporting\"\ntasks:\n  - id: \"collect_data\"\n    type: \"traditional\"\n    command: \"extract_data.py --date ${DATE}\"\n    \n  - id: \"compliance_check\"\n    type: \"agentic\"\n    command: \"mcp://compliance-analyzer\"\n    dependencies: [\"collect_data\"]\n    evaluation_point: true\n    context:\n      - regulation_text\n      - historical_cases\n      - company_policies\n    \n  - id: \"risk_assessment\"\n    type: \"agentic\"\n    command: \"mcp://risk-evaluator\"\n    dependencies: [\"compliance_check\"]\n    context:\n      - compliance_results\n      - entity_profile\n      - transaction_history\n\n\n\nAgents can intelligently route exceptions based on:\n\nSeverity analysis: Determine if manual review is required\nHistorical patterns: Learn from previous resolutions\nContext awareness: Consider system conditions, entity risk profiles\nEscalation logic: Automatically involve appropriate teams\n\n\n\n\n\n\n\n#[derive(Clone, Debug)]\npub enum CyclonetixEvent {\n    // Traditional events\n    TaskCompleted { task_id: String, result: TaskResult },\n    \n    // Agentic events\n    AgentAnalysisRequested { task_id: String, context: AgentContext },\n    AgentDecisionMade { task_id: String, decision: AgentDecision },\n    WorkflowModified { run_id: String, modifications: Vec&lt;GraphModification&gt; },\n}\n\n\n\npub struct MCPIntegration {\n    server_pool: MCPServerPool,\n    context_manager: ContextManager,\n    decision_engine: DecisionEngine,\n}\n\nimpl MCPIntegration {\n    pub async fn execute_agentic_task(\n        &self, \n        task: &AgenticTask,\n        context: &WorkflowContext\n    ) -&gt; Result&lt;AgentDecision, MCPError&gt; {\n        // Execute MCP call with context\n        // Parse agent response\n        // Return decision with graph modifications\n    }\n}\n\n\n\n{\n  \"decision\": \"route_for_review\",\n  \"confidence\": 0.85,\n  \"reasoning\": \"Unusual pattern detected in data processing pipeline\",\n  \"next_tasks\": [\"escalate_to_team\", \"generate_detailed_report\"],\n  \"parameters\": {\n    \"escalate_to_team\": {\n      \"priority\": \"high\",\n      \"reviewer\": \"senior_analyst\"\n    }\n  },\n  \"context_updates\": {\n    \"risk_level\": \"elevated\",\n    \"requires_review\": true\n  },\n  \"metadata\": {\n    \"analysis_time_ms\": 250,\n    \"data_sources\": [\"historical_data\", \"system_metrics\", \"entity_profile\"]\n  }\n}\n\n\n\n\n\n\n\nFull audit trail for all agent decisions\nReasoning capture for compliance review\nHuman override capabilities\nPerformance metrics and accuracy tracking\n\n\n\n\n\nContext isolation per workflow run\nData classification enforcement (Public, Internal, Restricted)\nPII handling with automatic redaction\nSecure agent communication via encrypted channels\n\n\n\n\n\nApproval gates for high-risk decisions\nConfidence thresholds for automatic execution\nEscalation paths for uncertain outcomes\nOverride mechanisms for exceptional cases\n\n\n\n\n\n\n\n\nTraditional tasks: Sub-5ms scheduling\nAgentic tasks: Sub-500ms analysis and decision\nGraph modification: Sub-100ms workflow updates\nHuman approval: Configurable timeout with fallback\n\n\n\n\n\nConcurrent agents: Horizontal scaling of MCP servers\nContext caching: Efficient reuse of analysis results\nBatch processing: Group similar decisions for efficiency\nResource isolation: Prevent agent workloads from impacting core orchestration\n\n\n\n\n\n\n\nworkflow: \"system_validation\"\ntasks:\n  - id: \"fetch_data\"\n    command: \"external-api --endpoint data --filter ${FILTER}\"\n    \n  - id: \"validate_data\"\n    type: \"agentic\"\n    command: \"mcp://data-validator\"\n    context:\n      - data_schemas\n      - validation_rules\n      - historical_patterns\n    evaluation_point: true\n    \n  - id: \"exception_analysis\"\n    type: \"conditional\"  # Created by agent if exceptions found\n    command: \"mcp://exception-analyzer\"\n\n\n\nworkflow: \"real_time_monitoring\"\ntrigger: \"kafka://system-events\"\ntasks:\n  - id: \"metric_calculation\"\n    type: \"traditional\"\n    command: \"calculate_metrics.py --input ${INPUT}\"\n    \n  - id: \"threshold_analysis\"\n    type: \"agentic\"\n    command: \"mcp://threshold-analyzer\"\n    context:\n      - system_limits\n      - historical_variance\n      - current_load\n    evaluation_point: true\n\n\n\n\n\n\n\nAgent collaboration on complex decisions\nConsensus mechanisms for critical choices\nSpecialized agents for different domains (compliance, operations, analysis)\n\n\n\n\n\nDecision feedback loops for continuous improvement\nPattern recognition from historical outcomes\nAdaptive thresholds based on performance metrics\n\n\n\n\n\nCross-workflow context sharing\nTemporal context for time-series analysis\nExternal data integration (feeds, news, regulatory updates)\n\n\n\n\n\n\n\n\nAgent decision latency (target: &lt;500ms)\nWorkflow modification time (target: &lt;100ms)\nContext retrieval efficiency\nResource utilization per agent\n\n\n\n\n\nFalse positive reduction in monitoring\nException handling automation rate\nProcess review efficiency\nHuman intervention reduction\n\n\n\n\n\nAgent decision accuracy\nReasoning quality scores\nAudit trail completeness\nCompliance adherence\n\n\n\n\n\n\nIntelligence where it matters: Use AI for complex decisions, traditional processing for routine operations\nAudit everything: Full traceability for regulatory compliance\nHuman oversight: Always maintain human control and override capabilities\nPerformance first: AI enhancement should not compromise core orchestration speed\nContext awareness: Agents should understand business context, not just technical data\nFail safely: Graceful degradation when AI services are unavailable\n\nThis AI-first design positions Cyclonetix as the next generation of workflow orchestration, combining the speed of traditional processing with the intelligence of modern AI agents.\n\n\n\n\nReview the Evaluation Points documentation for implementing dynamic workflows\nExplore Contexts & Parameters for managing agent context\nCheck the Developer Guide for extending AI capabilities\nSee the Cookbook for practical AI workflow examples"
  },
  {
    "objectID": "advanced/agentic-flows.html#core-concepts",
    "href": "advanced/agentic-flows.html#core-concepts",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "Tasks in Cyclonetix can be either traditional (shell commands) or agentic (MCP calls). Agentic tasks leverage AI agents to make intelligent decisions, analyze data, and dynamically modify workflow execution.\n\n\n\nEvaluation points serve as decision nodes where AI agents can:\n\nAnalyze workflow state and data\nMake intelligent routing decisions\nDetermine appropriate next steps based on context\nModify execution graphs dynamically\n\n\n\n\nCyclonetix combines high-speed stream processing (millions of events/second) with intelligent reasoning where it matters:\n\nFast path: Traditional stream processing for routine operations\nSmart path: Agentic analysis for edge cases, exceptions, and complex decisions\nHuman approval gates: Critical decisions requiring human oversight"
  },
  {
    "objectID": "advanced/agentic-flows.html#mcp-integration-architecture",
    "href": "advanced/agentic-flows.html#mcp-integration-architecture",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "MCP server runs as sidecar to orchestrator\nAgent pool for concurrent task processing\nTask routing between traditional and agentic execution paths\nContext preservation across agent interactions\n\n\n\n\n\n\nid: \"process_data\"\nname: \"Process Daily Data\"\ncommand: \"python process_data.py --input ${INPUT_FILE}\"\ntype: \"traditional\"\n\n\n\nid: \"analyze_patterns\"\nname: \"Analyze Data Patterns\"\ncommand: \"mcp://pattern-analyzer\"\ntype: \"agentic\"\ncontext:\n  - data_history\n  - system_conditions\n  - business_rules\nevaluation_point: true\n\n\n\n\nAgents can dynamically modify workflow execution by:\n\nAdding new tasks based on analysis results\nModifying parameters for downstream tasks\nBranching workflows based on intelligent decisions\nUpdating context for subsequent operations"
  },
  {
    "objectID": "advanced/agentic-flows.html#use-case-examples",
    "href": "advanced/agentic-flows.html#use-case-examples",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "workflow: \"data_processing\"\ndescription: \"AI-powered data processing with dynamic response\"\ntasks:\n  - id: \"ingest_data\"\n    type: \"traditional\"\n    command: \"kafka-consumer --topic data-stream\"\n    \n  - id: \"analyze_patterns\"\n    type: \"agentic\" \n    command: \"mcp://pattern-analyzer\"\n    dependencies: [\"ingest_data\"]\n    evaluation_point: true\n    context:\n      - data_history\n      - system_metrics\n      - business_rules\n    \n  - id: \"generate_insights\"\n    type: \"conditional\"  # Created dynamically by agent\n    command: \"python generate_insights.py --severity ${SEVERITY}\"\n\n\n\nworkflow: \"compliance_monitoring\"\ndescription: \"Automated compliance monitoring and reporting\"\ntasks:\n  - id: \"collect_data\"\n    type: \"traditional\"\n    command: \"extract_data.py --date ${DATE}\"\n    \n  - id: \"compliance_check\"\n    type: \"agentic\"\n    command: \"mcp://compliance-analyzer\"\n    dependencies: [\"collect_data\"]\n    evaluation_point: true\n    context:\n      - regulation_text\n      - historical_cases\n      - company_policies\n    \n  - id: \"risk_assessment\"\n    type: \"agentic\"\n    command: \"mcp://risk-evaluator\"\n    dependencies: [\"compliance_check\"]\n    context:\n      - compliance_results\n      - entity_profile\n      - transaction_history\n\n\n\nAgents can intelligently route exceptions based on:\n\nSeverity analysis: Determine if manual review is required\nHistorical patterns: Learn from previous resolutions\nContext awareness: Consider system conditions, entity risk profiles\nEscalation logic: Automatically involve appropriate teams"
  },
  {
    "objectID": "advanced/agentic-flows.html#technical-implementation",
    "href": "advanced/agentic-flows.html#technical-implementation",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "#[derive(Clone, Debug)]\npub enum CyclonetixEvent {\n    // Traditional events\n    TaskCompleted { task_id: String, result: TaskResult },\n    \n    // Agentic events\n    AgentAnalysisRequested { task_id: String, context: AgentContext },\n    AgentDecisionMade { task_id: String, decision: AgentDecision },\n    WorkflowModified { run_id: String, modifications: Vec&lt;GraphModification&gt; },\n}\n\n\n\npub struct MCPIntegration {\n    server_pool: MCPServerPool,\n    context_manager: ContextManager,\n    decision_engine: DecisionEngine,\n}\n\nimpl MCPIntegration {\n    pub async fn execute_agentic_task(\n        &self, \n        task: &AgenticTask,\n        context: &WorkflowContext\n    ) -&gt; Result&lt;AgentDecision, MCPError&gt; {\n        // Execute MCP call with context\n        // Parse agent response\n        // Return decision with graph modifications\n    }\n}\n\n\n\n{\n  \"decision\": \"route_for_review\",\n  \"confidence\": 0.85,\n  \"reasoning\": \"Unusual pattern detected in data processing pipeline\",\n  \"next_tasks\": [\"escalate_to_team\", \"generate_detailed_report\"],\n  \"parameters\": {\n    \"escalate_to_team\": {\n      \"priority\": \"high\",\n      \"reviewer\": \"senior_analyst\"\n    }\n  },\n  \"context_updates\": {\n    \"risk_level\": \"elevated\",\n    \"requires_review\": true\n  },\n  \"metadata\": {\n    \"analysis_time_ms\": 250,\n    \"data_sources\": [\"historical_data\", \"system_metrics\", \"entity_profile\"]\n  }\n}"
  },
  {
    "objectID": "advanced/agentic-flows.html#governance-and-compliance",
    "href": "advanced/agentic-flows.html#governance-and-compliance",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "Full audit trail for all agent decisions\nReasoning capture for compliance review\nHuman override capabilities\nPerformance metrics and accuracy tracking\n\n\n\n\n\nContext isolation per workflow run\nData classification enforcement (Public, Internal, Restricted)\nPII handling with automatic redaction\nSecure agent communication via encrypted channels\n\n\n\n\n\nApproval gates for high-risk decisions\nConfidence thresholds for automatic execution\nEscalation paths for uncertain outcomes\nOverride mechanisms for exceptional cases"
  },
  {
    "objectID": "advanced/agentic-flows.html#performance-characteristics",
    "href": "advanced/agentic-flows.html#performance-characteristics",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "Traditional tasks: Sub-5ms scheduling\nAgentic tasks: Sub-500ms analysis and decision\nGraph modification: Sub-100ms workflow updates\nHuman approval: Configurable timeout with fallback\n\n\n\n\n\nConcurrent agents: Horizontal scaling of MCP servers\nContext caching: Efficient reuse of analysis results\nBatch processing: Group similar decisions for efficiency\nResource isolation: Prevent agent workloads from impacting core orchestration"
  },
  {
    "objectID": "advanced/agentic-flows.html#integration-examples",
    "href": "advanced/agentic-flows.html#integration-examples",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "workflow: \"system_validation\"\ntasks:\n  - id: \"fetch_data\"\n    command: \"external-api --endpoint data --filter ${FILTER}\"\n    \n  - id: \"validate_data\"\n    type: \"agentic\"\n    command: \"mcp://data-validator\"\n    context:\n      - data_schemas\n      - validation_rules\n      - historical_patterns\n    evaluation_point: true\n    \n  - id: \"exception_analysis\"\n    type: \"conditional\"  # Created by agent if exceptions found\n    command: \"mcp://exception-analyzer\"\n\n\n\nworkflow: \"real_time_monitoring\"\ntrigger: \"kafka://system-events\"\ntasks:\n  - id: \"metric_calculation\"\n    type: \"traditional\"\n    command: \"calculate_metrics.py --input ${INPUT}\"\n    \n  - id: \"threshold_analysis\"\n    type: \"agentic\"\n    command: \"mcp://threshold-analyzer\"\n    context:\n      - system_limits\n      - historical_variance\n      - current_load\n    evaluation_point: true"
  },
  {
    "objectID": "advanced/agentic-flows.html#future-enhancements",
    "href": "advanced/agentic-flows.html#future-enhancements",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "Agent collaboration on complex decisions\nConsensus mechanisms for critical choices\nSpecialized agents for different domains (compliance, operations, analysis)\n\n\n\n\n\nDecision feedback loops for continuous improvement\nPattern recognition from historical outcomes\nAdaptive thresholds based on performance metrics\n\n\n\n\n\nCross-workflow context sharing\nTemporal context for time-series analysis\nExternal data integration (feeds, news, regulatory updates)"
  },
  {
    "objectID": "advanced/agentic-flows.html#success-metrics",
    "href": "advanced/agentic-flows.html#success-metrics",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "Agent decision latency (target: &lt;500ms)\nWorkflow modification time (target: &lt;100ms)\nContext retrieval efficiency\nResource utilization per agent\n\n\n\n\n\nFalse positive reduction in monitoring\nException handling automation rate\nProcess review efficiency\nHuman intervention reduction\n\n\n\n\n\nAgent decision accuracy\nReasoning quality scores\nAudit trail completeness\nCompliance adherence"
  },
  {
    "objectID": "advanced/agentic-flows.html#design-principles",
    "href": "advanced/agentic-flows.html#design-principles",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "Intelligence where it matters: Use AI for complex decisions, traditional processing for routine operations\nAudit everything: Full traceability for regulatory compliance\nHuman oversight: Always maintain human control and override capabilities\nPerformance first: AI enhancement should not compromise core orchestration speed\nContext awareness: Agents should understand business context, not just technical data\nFail safely: Graceful degradation when AI services are unavailable\n\nThis AI-first design positions Cyclonetix as the next generation of workflow orchestration, combining the speed of traditional processing with the intelligence of modern AI agents."
  },
  {
    "objectID": "advanced/agentic-flows.html#next-steps",
    "href": "advanced/agentic-flows.html#next-steps",
    "title": "AI-First Design: MCP and Agentic Workflows",
    "section": "",
    "text": "Review the Evaluation Points documentation for implementing dynamic workflows\nExplore Contexts & Parameters for managing agent context\nCheck the Developer Guide for extending AI capabilities\nSee the Cookbook for practical AI workflow examples"
  },
  {
    "objectID": "deployment/scaling.html",
    "href": "deployment/scaling.html",
    "title": "Scaling",
    "section": "",
    "text": "This guide covers strategies for scaling Cyclonetix from small deployments to large enterprise environments.\n\n\nCyclonetix has several components that can be scaled independently:\n\nOrchestrator: Manages workflow execution and task scheduling\nAgents: Execute tasks and report results\nState backend: Stores workflow and task state (Redis, PostgreSQL)\nUI server: Serves the web interface\n\n\n\n\n\n\nFor small deployments or development environments:\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\nflowchart TD\nOrchestrator[\"Orchestrator\"]\nAgent[\"Agent\"]\nUI[\"UI Server\"]\nInMem[\"In-Memory State\"]\n\n        Orchestrator &lt;--&gt; InMem\n        Agent &lt;--&gt; InMem\n        UI &lt;--&gt; InMem\n\n\n\n\n\n\n\n\nSingle instance running all components\nIn-memory or single Redis instance as backend\nSuitable for up to a few hundred tasks per day\n\n\n\n\nFor medium-sized deployments:\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\nflowchart TD\nOrchestrator[\"Orchestrator\"]\nUI[\"UI Server\"]\nRedis[\"Redis Cluster\"]\nAgentCPU[\"Agent (CPU)\"]\nAgentMem[\"Agent (Memory)\"]\nAgentGPU[\"Agent (GPU)\"]\n\n    Orchestrator &lt;--&gt; Redis\n    UI &lt;--&gt; Redis\n    \n    Redis --&gt; AgentCPU\n    Redis --&gt; AgentMem\n    Redis --&gt; AgentGPU\n    \n    AgentCPU --&gt; Orchestrator\n    AgentMem --&gt; Orchestrator\n    AgentGPU --&gt; Orchestrator\n\n\n\n\n\n\n\nSeparate orchestrator and agent instances\nRedis cluster for state management\nSuitable for thousands of tasks per day\n\n\n\n\nFor large enterprise deployments:\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\nflowchart TD\nOrch1[\"Orchestrator 1\"]\nOrch2[\"Orchestrator 2\"]\nOrch3[\"Orchestrator 3\"]\n\n    Postgres[\"PostgreSQL\"]\n    Redis[\"Redis\"]\n    \n    Agent1[\"Agent 1\"]\n    Agent2[\"Agent 2\"]\n    Agent3[\"Agent 3\"]\n    AgentN[\"Agent N...\"]\n    \n    Orch1 &lt;--&gt; Postgres\n    Orch2 &lt;--&gt; Postgres\n    Orch3 &lt;--&gt; Postgres\n    \n    Orch1 &lt;--&gt; Redis\n    Orch2 &lt;--&gt; Redis\n    Orch3 &lt;--&gt; Redis\n    \n    Redis --&gt; Agent1\n    Redis --&gt; Agent2\n    Redis --&gt; Agent3\n    Redis --&gt; AgentN\n    \n    Agent1 --&gt; Orch1\n    Agent2 --&gt; Orch2\n    Agent3 --&gt; Orch3\n    AgentN --&gt; Orch1\n\n\n\n\n\n\n\nMultiple orchestrators with work distribution\nMany specialized agents\nPostgreSQL for state management\nRedis for queuing\nLoad-balanced UI servers\nSuitable for millions of tasks per day\n\n\n\n\n\n\n\nThe orchestrator can be scaled horizontally:\n\nMultiple Orchestrators: Start multiple orchestrator instances\nWork Distribution: DAGs are automatically assigned to orchestrators based on a hashing algorithm\nAutomatic Failover: If an orchestrator fails, its DAGs are reassigned\n\nConfiguration:\norchestrator:\n  id: \"auto\"              # Auto-generate ID or specify\n  cluster_mode: true      # Enable orchestrator clustering\n  distribution_algorithm: \"consistent_hash\"  # Work distribution method\n\n\n\nAgents can be scaled horizontally and specialized:\n\nTask Types: Dedicate agents to specific task types\nResource Requirements: Create agent pools for different resource needs\nLocality: Deploy agents close to data or resources they need\n\nConfiguration:\nagent:\n  queues: [\"cpu_tasks\", \"default\"]  # Queues this agent subscribes to\n  tags: [\"region:us-east\", \"cpu:high\", \"memory:standard\"]  # Agent capabilities\n  concurrency: 8                    # Number of concurrent tasks\n\n\n\nUse specialized queues for workload distribution:\n# In task definition\nqueue: \"gpu_tasks\"  # Assign task to GPU queue\n\n# In agent configuration\nagent:\n  queues: [\"gpu_tasks\"]  # This agent only processes GPU tasks\n\n\n\n\n\nOn Kubernetes, use Horizontal Pod Autoscaler:\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: cyclonetix-agent-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: cyclonetix-agent\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n  - type: External\n    external:\n      metric:\n        name: redis_queue_length\n        selector:\n          matchLabels:\n            queue: default\n      target:\n        type: AverageValue\n        averageValue: 5\n\n\n\nFor cloud deployments, use cloud provider auto-scaling:\n\nAWS: Auto Scaling Groups\nGCP: Managed Instance Groups\nAzure: Virtual Machine Scale Sets\n\n\n\n\n\n\n\nFor Redis-based state management:\n\nRedis Cluster: Set up a Redis cluster for horizontal scaling\nRedis Sentinel: Use Redis Sentinel for high availability\nRedis Enterprise: Consider Redis Enterprise for large deployments\n\nConfiguration:\nbackend: \"redis\"\nbackend_url: \"redis://redis-cluster:6379\"\nredis:\n  cluster_mode: true\n  read_from_replicas: true\n  connection_pool_size: 20\n\n\n\nFor PostgreSQL-based state management:\n\nConnection Pooling: Use PgBouncer for connection pooling\nRead Replicas: Distribute read operations to replicas\nPartitioning: Partition data for large-scale deployments\n\nConfiguration:\nbackend: \"postgresql\"\nbackend_url: \"postgres://user:password@pg-host/cyclonetix\"\npostgresql:\n  max_connections: 20\n  statement_timeout_seconds: 30\n  use_prepared_statements: true\n\n\n\n\nFor the UI server:\n\nLoad Balancing: Deploy multiple UI servers behind a load balancer\nCaching: Implement caching for frequently accessed data\nWebSocket Optimization: Tune WebSocket connections for large numbers of clients\n\n\n\n\n\n\n\nGroup small tasks into batches:\nbatch:\n  enabled: true\n  max_tasks: 10\n  max_delay_seconds: 5\n\n\n\nUse binary serialization for better performance:\nserialization_format: \"binary\"  # Instead of default JSON\n\n\n\nTune resource allocation based on workload:\nagent:\n  concurrency: 8              # Number of concurrent tasks\n  resource_allocation:\n    memory_per_task_mb: 256   # Memory allocation per task\n    cpu_weight_per_task: 1    # CPU weight per task\n\n\n\n\nAs you scale, monitoring becomes crucial:\n\nPrometheus Integration: Expose metrics for Prometheus\nGrafana Dashboards: Create Grafana dashboards for monitoring\nAlerts: Set up alerts for queue depth, agent health, etc.\n\nmonitoring:\n  prometheus:\n    enabled: true\n    endpoint: \"/metrics\"\n  metrics:\n    include_task_metrics: true\n    include_queue_metrics: true\n    include_agent_metrics: true\n\n\n\n\nCoordination Overhead: More orchestrators increase coordination overhead\nDatabase Performance: State backend can become a bottleneck\nNetwork Latency: Distributed systems introduce latency\nConsistency vs. Availability: Trade-offs in distributed systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeployment Size\nTasks/Day\nOrchestrators\nAgents\nBackend\nVM/Pod Size\n\n\n\n\nSmall\n&lt;1,000\n1\n1-3\nRedis Single\n2 CPU, 4GB RAM\n\n\nMedium\n&lt;10,000\n1-3\n5-10\nRedis Cluster\n4 CPU, 8GB RAM\n\n\nLarge\n&lt;100,000\n3-5\n10-30\nPostgreSQL\n8 CPU, 16GB RAM\n\n\nEnterprise\n&gt;100,000\n5+\n30+\nPostgreSQL HA\n16+ CPU, 32+ GB RAM\n\n\n\n\n\n\n\n\n# config.yaml\nbackend: \"redis\"\nbackend_url: \"redis://redis-service.cyclonetix.svc.cluster.local:6379\"\n# kubernetes manifests\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-orchestrator\nspec:\n  replicas: 3\n  # ...\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-agent-cpu\nspec:\n  replicas: 5\n  # ...\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-agent-memory\nspec:\n  replicas: 3\n  # ...\n\n\n\n# docker-compose.yml\nversion: '3'\nservices:\n  redis:\n    image: redis:alpine\n    # ...\n\n  orchestrator:\n    image: cyclonetix:latest\n    command: --orchestrator\n    # ...\n\n  ui:\n    image: cyclonetix:latest\n    command: --ui\n    ports:\n      - \"3000:3000\"\n    # ...\n\n  agent-1:\n    image: cyclonetix:latest\n    command: --agent\n    environment:\n      - CYCLO_AGENT_QUEUES=default,cpu_tasks\n    # ...\n\n  agent-2:\n    image: cyclonetix:latest\n    command: --agent\n    environment:\n      - CYCLO_AGENT_QUEUES=memory_tasks\n    # ...\n\n\n\n\n\nReview Configuration Options for fine-tuning\nSet up Security for your scaled deployment\nCheck the Developer Guide for extending Cyclonetix",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#scaling-components",
    "href": "deployment/scaling.html#scaling-components",
    "title": "Scaling",
    "section": "",
    "text": "Cyclonetix has several components that can be scaled independently:\n\nOrchestrator: Manages workflow execution and task scheduling\nAgents: Execute tasks and report results\nState backend: Stores workflow and task state (Redis, PostgreSQL)\nUI server: Serves the web interface",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#scaling-patterns",
    "href": "deployment/scaling.html#scaling-patterns",
    "title": "Scaling",
    "section": "",
    "text": "For small deployments or development environments:\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\nflowchart TD\nOrchestrator[\"Orchestrator\"]\nAgent[\"Agent\"]\nUI[\"UI Server\"]\nInMem[\"In-Memory State\"]\n\n        Orchestrator &lt;--&gt; InMem\n        Agent &lt;--&gt; InMem\n        UI &lt;--&gt; InMem\n\n\n\n\n\n\n\n\nSingle instance running all components\nIn-memory or single Redis instance as backend\nSuitable for up to a few hundred tasks per day\n\n\n\n\nFor medium-sized deployments:\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\nflowchart TD\nOrchestrator[\"Orchestrator\"]\nUI[\"UI Server\"]\nRedis[\"Redis Cluster\"]\nAgentCPU[\"Agent (CPU)\"]\nAgentMem[\"Agent (Memory)\"]\nAgentGPU[\"Agent (GPU)\"]\n\n    Orchestrator &lt;--&gt; Redis\n    UI &lt;--&gt; Redis\n    \n    Redis --&gt; AgentCPU\n    Redis --&gt; AgentMem\n    Redis --&gt; AgentGPU\n    \n    AgentCPU --&gt; Orchestrator\n    AgentMem --&gt; Orchestrator\n    AgentGPU --&gt; Orchestrator\n\n\n\n\n\n\n\nSeparate orchestrator and agent instances\nRedis cluster for state management\nSuitable for thousands of tasks per day\n\n\n\n\nFor large enterprise deployments:\n\n\n\n\n\n%%{init: {'theme': 'forest', 'themeCSS': '.node rect { rx: 10; ry: 10; } '}}%%\nflowchart TD\nOrch1[\"Orchestrator 1\"]\nOrch2[\"Orchestrator 2\"]\nOrch3[\"Orchestrator 3\"]\n\n    Postgres[\"PostgreSQL\"]\n    Redis[\"Redis\"]\n    \n    Agent1[\"Agent 1\"]\n    Agent2[\"Agent 2\"]\n    Agent3[\"Agent 3\"]\n    AgentN[\"Agent N...\"]\n    \n    Orch1 &lt;--&gt; Postgres\n    Orch2 &lt;--&gt; Postgres\n    Orch3 &lt;--&gt; Postgres\n    \n    Orch1 &lt;--&gt; Redis\n    Orch2 &lt;--&gt; Redis\n    Orch3 &lt;--&gt; Redis\n    \n    Redis --&gt; Agent1\n    Redis --&gt; Agent2\n    Redis --&gt; Agent3\n    Redis --&gt; AgentN\n    \n    Agent1 --&gt; Orch1\n    Agent2 --&gt; Orch2\n    Agent3 --&gt; Orch3\n    AgentN --&gt; Orch1\n\n\n\n\n\n\n\nMultiple orchestrators with work distribution\nMany specialized agents\nPostgreSQL for state management\nRedis for queuing\nLoad-balanced UI servers\nSuitable for millions of tasks per day",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#scaling-strategies",
    "href": "deployment/scaling.html#scaling-strategies",
    "title": "Scaling",
    "section": "",
    "text": "The orchestrator can be scaled horizontally:\n\nMultiple Orchestrators: Start multiple orchestrator instances\nWork Distribution: DAGs are automatically assigned to orchestrators based on a hashing algorithm\nAutomatic Failover: If an orchestrator fails, its DAGs are reassigned\n\nConfiguration:\norchestrator:\n  id: \"auto\"              # Auto-generate ID or specify\n  cluster_mode: true      # Enable orchestrator clustering\n  distribution_algorithm: \"consistent_hash\"  # Work distribution method\n\n\n\nAgents can be scaled horizontally and specialized:\n\nTask Types: Dedicate agents to specific task types\nResource Requirements: Create agent pools for different resource needs\nLocality: Deploy agents close to data or resources they need\n\nConfiguration:\nagent:\n  queues: [\"cpu_tasks\", \"default\"]  # Queues this agent subscribes to\n  tags: [\"region:us-east\", \"cpu:high\", \"memory:standard\"]  # Agent capabilities\n  concurrency: 8                    # Number of concurrent tasks\n\n\n\nUse specialized queues for workload distribution:\n# In task definition\nqueue: \"gpu_tasks\"  # Assign task to GPU queue\n\n# In agent configuration\nagent:\n  queues: [\"gpu_tasks\"]  # This agent only processes GPU tasks\n\n\n\n\n\nOn Kubernetes, use Horizontal Pod Autoscaler:\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: cyclonetix-agent-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: cyclonetix-agent\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n  - type: External\n    external:\n      metric:\n        name: redis_queue_length\n        selector:\n          matchLabels:\n            queue: default\n      target:\n        type: AverageValue\n        averageValue: 5\n\n\n\nFor cloud deployments, use cloud provider auto-scaling:\n\nAWS: Auto Scaling Groups\nGCP: Managed Instance Groups\nAzure: Virtual Machine Scale Sets\n\n\n\n\n\n\n\nFor Redis-based state management:\n\nRedis Cluster: Set up a Redis cluster for horizontal scaling\nRedis Sentinel: Use Redis Sentinel for high availability\nRedis Enterprise: Consider Redis Enterprise for large deployments\n\nConfiguration:\nbackend: \"redis\"\nbackend_url: \"redis://redis-cluster:6379\"\nredis:\n  cluster_mode: true\n  read_from_replicas: true\n  connection_pool_size: 20\n\n\n\nFor PostgreSQL-based state management:\n\nConnection Pooling: Use PgBouncer for connection pooling\nRead Replicas: Distribute read operations to replicas\nPartitioning: Partition data for large-scale deployments\n\nConfiguration:\nbackend: \"postgresql\"\nbackend_url: \"postgres://user:password@pg-host/cyclonetix\"\npostgresql:\n  max_connections: 20\n  statement_timeout_seconds: 30\n  use_prepared_statements: true\n\n\n\n\nFor the UI server:\n\nLoad Balancing: Deploy multiple UI servers behind a load balancer\nCaching: Implement caching for frequently accessed data\nWebSocket Optimization: Tune WebSocket connections for large numbers of clients",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#performance-tuning",
    "href": "deployment/scaling.html#performance-tuning",
    "title": "Scaling",
    "section": "",
    "text": "Group small tasks into batches:\nbatch:\n  enabled: true\n  max_tasks: 10\n  max_delay_seconds: 5\n\n\n\nUse binary serialization for better performance:\nserialization_format: \"binary\"  # Instead of default JSON\n\n\n\nTune resource allocation based on workload:\nagent:\n  concurrency: 8              # Number of concurrent tasks\n  resource_allocation:\n    memory_per_task_mb: 256   # Memory allocation per task\n    cpu_weight_per_task: 1    # CPU weight per task",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#monitoring-for-scale",
    "href": "deployment/scaling.html#monitoring-for-scale",
    "title": "Scaling",
    "section": "",
    "text": "As you scale, monitoring becomes crucial:\n\nPrometheus Integration: Expose metrics for Prometheus\nGrafana Dashboards: Create Grafana dashboards for monitoring\nAlerts: Set up alerts for queue depth, agent health, etc.\n\nmonitoring:\n  prometheus:\n    enabled: true\n    endpoint: \"/metrics\"\n  metrics:\n    include_task_metrics: true\n    include_queue_metrics: true\n    include_agent_metrics: true",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#scaling-limitations-and-considerations",
    "href": "deployment/scaling.html#scaling-limitations-and-considerations",
    "title": "Scaling",
    "section": "",
    "text": "Coordination Overhead: More orchestrators increase coordination overhead\nDatabase Performance: State backend can become a bottleneck\nNetwork Latency: Distributed systems introduce latency\nConsistency vs. Availability: Trade-offs in distributed systems",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#benchmarks-and-sizing-guidelines",
    "href": "deployment/scaling.html#benchmarks-and-sizing-guidelines",
    "title": "Scaling",
    "section": "",
    "text": "Deployment Size\nTasks/Day\nOrchestrators\nAgents\nBackend\nVM/Pod Size\n\n\n\n\nSmall\n&lt;1,000\n1\n1-3\nRedis Single\n2 CPU, 4GB RAM\n\n\nMedium\n&lt;10,000\n1-3\n5-10\nRedis Cluster\n4 CPU, 8GB RAM\n\n\nLarge\n&lt;100,000\n3-5\n10-30\nPostgreSQL\n8 CPU, 16GB RAM\n\n\nEnterprise\n&gt;100,000\n5+\n30+\nPostgreSQL HA\n16+ CPU, 32+ GB RAM",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#example-scaling-configurations",
    "href": "deployment/scaling.html#example-scaling-configurations",
    "title": "Scaling",
    "section": "",
    "text": "# config.yaml\nbackend: \"redis\"\nbackend_url: \"redis://redis-service.cyclonetix.svc.cluster.local:6379\"\n# kubernetes manifests\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-orchestrator\nspec:\n  replicas: 3\n  # ...\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-agent-cpu\nspec:\n  replicas: 5\n  # ...\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-agent-memory\nspec:\n  replicas: 3\n  # ...\n\n\n\n# docker-compose.yml\nversion: '3'\nservices:\n  redis:\n    image: redis:alpine\n    # ...\n\n  orchestrator:\n    image: cyclonetix:latest\n    command: --orchestrator\n    # ...\n\n  ui:\n    image: cyclonetix:latest\n    command: --ui\n    ports:\n      - \"3000:3000\"\n    # ...\n\n  agent-1:\n    image: cyclonetix:latest\n    command: --agent\n    environment:\n      - CYCLO_AGENT_QUEUES=default,cpu_tasks\n    # ...\n\n  agent-2:\n    image: cyclonetix:latest\n    command: --agent\n    environment:\n      - CYCLO_AGENT_QUEUES=memory_tasks\n    # ...",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/scaling.html#next-steps",
    "href": "deployment/scaling.html#next-steps",
    "title": "Scaling",
    "section": "",
    "text": "Review Configuration Options for fine-tuning\nSet up Security for your scaled deployment\nCheck the Developer Guide for extending Cyclonetix",
    "crumbs": [
      "Deployment",
      "Scaling"
    ]
  },
  {
    "objectID": "deployment/installation.html",
    "href": "deployment/installation.html",
    "title": "Installation",
    "section": "",
    "text": "This guide covers the different ways to install and deploy Cyclonetix, from local development to production environments.\n\n\nBefore installing Cyclonetix, ensure you have the following prerequisites:\n\nRust (1.84.0 or later)\nRedis (for production deployments)\nPostgreSQL (optional, for large-scale deployments)\n\n\n\n\n\n\n\nClone the repository:\n\ngit clone https://github.com/neural-chilli/Cyclonetix.git\ncd Cyclonetix\n\nBuild the project:\n\ncargo build --release\n\nCreate a basic configuration:\n\nmkdir -p data/tasks data/dags data/contexts\ncp config.yaml.example config.yaml\n\nRun with in-memory backend:\n\n./target/release/cyclonetix\n\n\n\nFor UI development, you can enable template hot-reloading:\nDEV_MODE=true cargo run\nThis mode disables Tera template caching and reloads templates on each request, allowing you to:\n\nEdit templates directly and see changes on refresh\nAvoid server restarts when modifying UI code\nSpeed up the UI development workflow\n\n\n\n\n\n\n\n\nCreate a configuration directory:\n\nmkdir -p cyclonetix-config/data/{tasks,dags,contexts}\n\nCreate a configuration file:\n\ncp config.yaml.example cyclonetix-config/config.yaml\n\nEdit the configuration to point to Redis:\n\nbackend: \"redis\"\nbackend_url: \"redis://redis:6379\"\n\nRun with Docker Compose:\n\n# docker-compose.yml\nversion: '3'\nservices:\n  redis:\n    image: redis:alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n\n  cyclonetix:\n    image: neuralchilli/cyclonetix:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - ./cyclonetix-config:/config\n    command: --config /config/config.yaml\n    depends_on:\n      - redis\n\nvolumes:\n  redis-data:\nStart the services:\ndocker-compose up -d\n\n\n\nIf you need to customize the Docker image:\n\nCreate a Dockerfile:\n\nFROM rust:1.84 as builder\nWORKDIR /app\nCOPY . .\nRUN cargo build --release\n\nFROM debian:bullseye-slim\nRUN apt-get update && apt-get install -y --no-install-recommends ca-certificates && rm -rf /var/lib/apt/lists/*\nCOPY --from=builder /app/target/release/cyclonetix /usr/local/bin/\nENTRYPOINT [\"cyclonetix\"]\n\nBuild and run:\n\ndocker build -t custom-cyclonetix .\ndocker run -p 3000:3000 -v ./config.yaml:/config.yaml custom-cyclonetix --config /config.yaml\n\n\n\n\nFor production environments, we recommend the following setup:\n\n\n\nInstall prerequisites:\n\n# Install Redis\nsudo apt-get update\nsudo apt-get install redis-server\n\n# Start and enable Redis\nsudo systemctl start redis-server\nsudo systemctl enable redis-server\n\nInstall Cyclonetix binary:\n\n# Option 1: Build from source\ncargo build --release\nsudo cp target/release/cyclonetix /usr/local/bin/\n\n# Option 2: Download pre-built binary\nsudo wget -O /usr/local/bin/cyclonetix https://github.com/neural-chilli/Cyclonetix/releases/latest/download/cyclonetix\nsudo chmod +x /usr/local/bin/cyclonetix\n\nCreate configuration:\n\nsudo mkdir -p /etc/cyclonetix/data/{tasks,dags,contexts}\nsudo cp config.yaml.example /etc/cyclonetix/config.yaml\n\nCreate a systemd service:\n\n# /etc/systemd/system/cyclonetix.service\n[Unit]\nDescription=Cyclonetix Workflow Orchestrator\nAfter=network.target redis-server.service\nWants=redis-server.service\n\n[Service]\nExecStart=/usr/local/bin/cyclonetix --config /etc/cyclonetix/config.yaml\nRestart=on-failure\nUser=cyclonetix\nGroup=cyclonetix\nWorkingDirectory=/etc/cyclonetix\n\n[Install]\nWantedBy=multi-user.target\n\nStart and enable the service:\n\nsudo systemctl daemon-reload\nsudo systemctl start cyclonetix\nsudo systemctl enable cyclonetix\n\n\n\nFor scalable production deployments, use Kubernetes:\n\nCreate a ConfigMap for configuration:\n\n# cyclonetix-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cyclonetix-config\ndata:\n  config.yaml: |\n    backend: \"redis\"\n    backend_url: \"redis://redis-service:6379\"\n    queues:\n      - \"default\"\n      - \"high_memory\"\n      - \"gpu_tasks\"\n    security:\n      enabled: true\n      # ... rest of configuration\n\nDeploy Redis:\n\n# redis-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        ports:\n        - containerPort: 6379\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-service\nspec:\n  selector:\n    app: redis\n  ports:\n  - port: 6379\n    targetPort: 6379\n\nDeploy Cyclonetix orchestrator:\n\n# cyclonetix-orchestrator.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-orchestrator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cyclonetix-orchestrator\n  template:\n    metadata:\n      labels:\n        app: cyclonetix-orchestrator\n    spec:\n      containers:\n      - name: cyclonetix\n        image: neuralchilli/cyclonetix:latest\n        args: [\"--config\", \"/config/config.yaml\", \"--orchestrator\"]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: cyclonetix-config\n\nDeploy Cyclonetix UI:\n\n# cyclonetix-ui.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cyclonetix-ui\n  template:\n    metadata:\n      labels:\n        app: cyclonetix-ui\n    spec:\n      containers:\n      - name: cyclonetix-ui\n        image: neuralchilli/cyclonetix:latest\n        args: [\"--config\", \"/config/config.yaml\", \"--ui\"]\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: config-volume\n          mountPath: /config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: cyclonetix-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cyclonetix-ui-service\nspec:\n  selector:\n    app: cyclonetix-ui\n  ports:\n  - port: 80\n    targetPort: 3000\n  type: ClusterIP\n\nDeploy Cyclonetix agents:\n\n# cyclonetix-agent.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-agent\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cyclonetix-agent\n  template:\n    metadata:\n      labels:\n        app: cyclonetix-agent\n    spec:\n      containers:\n      - name: cyclonetix-agent\n        image: neuralchilli/cyclonetix:latest\n        args: [\"--config\", \"/config/config.yaml\", \"--agent\"]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: cyclonetix-config\n\nDeploy an Ingress for the UI:\n\n# cyclonetix-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: cyclonetix-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: cyclonetix.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: cyclonetix-ui-service\n            port:\n              number: 80\nApply all the configurations:\nkubectl apply -f cyclonetix-configmap.yaml\nkubectl apply -f redis-deployment.yaml\nkubectl apply -f cyclonetix-orchestrator.yaml\nkubectl apply -f cyclonetix-ui.yaml\nkubectl apply -f cyclonetix-agent.yaml\nkubectl apply -f cyclonetix-ingress.yaml\n\n\n\n\nTo upgrade an existing installation:\n\nStop the Cyclonetix service:\n\nsudo systemctl stop cyclonetix\n\nInstall the new version:\n\n# Option 1: Build from source\ngit pull\ncargo build --release\nsudo cp target/release/cyclonetix /usr/local/bin/\n\n# Option 2: Download pre-built binary\nsudo wget -O /usr/local/bin/cyclonetix https://github.com/neural-chilli/Cyclonetix/releases/latest/download/cyclonetix\nsudo chmod +x /usr/local/bin/cyclonetix\n\nRestart the service:\n\nsudo systemctl start cyclonetix\n\n\n\n\nConfigure Security for your deployment\nLearn about Scaling Cyclonetix\nExplore the Configuration Reference",
    "crumbs": [
      "Deployment",
      "Installation"
    ]
  },
  {
    "objectID": "deployment/installation.html#prerequisites",
    "href": "deployment/installation.html#prerequisites",
    "title": "Installation",
    "section": "",
    "text": "Before installing Cyclonetix, ensure you have the following prerequisites:\n\nRust (1.84.0 or later)\nRedis (for production deployments)\nPostgreSQL (optional, for large-scale deployments)",
    "crumbs": [
      "Deployment",
      "Installation"
    ]
  },
  {
    "objectID": "deployment/installation.html#local-development-setup",
    "href": "deployment/installation.html#local-development-setup",
    "title": "Installation",
    "section": "",
    "text": "Clone the repository:\n\ngit clone https://github.com/neural-chilli/Cyclonetix.git\ncd Cyclonetix\n\nBuild the project:\n\ncargo build --release\n\nCreate a basic configuration:\n\nmkdir -p data/tasks data/dags data/contexts\ncp config.yaml.example config.yaml\n\nRun with in-memory backend:\n\n./target/release/cyclonetix\n\n\n\nFor UI development, you can enable template hot-reloading:\nDEV_MODE=true cargo run\nThis mode disables Tera template caching and reloads templates on each request, allowing you to:\n\nEdit templates directly and see changes on refresh\nAvoid server restarts when modifying UI code\nSpeed up the UI development workflow",
    "crumbs": [
      "Deployment",
      "Installation"
    ]
  },
  {
    "objectID": "deployment/installation.html#docker-based-setup",
    "href": "deployment/installation.html#docker-based-setup",
    "title": "Installation",
    "section": "",
    "text": "Create a configuration directory:\n\nmkdir -p cyclonetix-config/data/{tasks,dags,contexts}\n\nCreate a configuration file:\n\ncp config.yaml.example cyclonetix-config/config.yaml\n\nEdit the configuration to point to Redis:\n\nbackend: \"redis\"\nbackend_url: \"redis://redis:6379\"\n\nRun with Docker Compose:\n\n# docker-compose.yml\nversion: '3'\nservices:\n  redis:\n    image: redis:alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n\n  cyclonetix:\n    image: neuralchilli/cyclonetix:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - ./cyclonetix-config:/config\n    command: --config /config/config.yaml\n    depends_on:\n      - redis\n\nvolumes:\n  redis-data:\nStart the services:\ndocker-compose up -d\n\n\n\nIf you need to customize the Docker image:\n\nCreate a Dockerfile:\n\nFROM rust:1.84 as builder\nWORKDIR /app\nCOPY . .\nRUN cargo build --release\n\nFROM debian:bullseye-slim\nRUN apt-get update && apt-get install -y --no-install-recommends ca-certificates && rm -rf /var/lib/apt/lists/*\nCOPY --from=builder /app/target/release/cyclonetix /usr/local/bin/\nENTRYPOINT [\"cyclonetix\"]\n\nBuild and run:\n\ndocker build -t custom-cyclonetix .\ndocker run -p 3000:3000 -v ./config.yaml:/config.yaml custom-cyclonetix --config /config.yaml",
    "crumbs": [
      "Deployment",
      "Installation"
    ]
  },
  {
    "objectID": "deployment/installation.html#production-deployment",
    "href": "deployment/installation.html#production-deployment",
    "title": "Installation",
    "section": "",
    "text": "For production environments, we recommend the following setup:\n\n\n\nInstall prerequisites:\n\n# Install Redis\nsudo apt-get update\nsudo apt-get install redis-server\n\n# Start and enable Redis\nsudo systemctl start redis-server\nsudo systemctl enable redis-server\n\nInstall Cyclonetix binary:\n\n# Option 1: Build from source\ncargo build --release\nsudo cp target/release/cyclonetix /usr/local/bin/\n\n# Option 2: Download pre-built binary\nsudo wget -O /usr/local/bin/cyclonetix https://github.com/neural-chilli/Cyclonetix/releases/latest/download/cyclonetix\nsudo chmod +x /usr/local/bin/cyclonetix\n\nCreate configuration:\n\nsudo mkdir -p /etc/cyclonetix/data/{tasks,dags,contexts}\nsudo cp config.yaml.example /etc/cyclonetix/config.yaml\n\nCreate a systemd service:\n\n# /etc/systemd/system/cyclonetix.service\n[Unit]\nDescription=Cyclonetix Workflow Orchestrator\nAfter=network.target redis-server.service\nWants=redis-server.service\n\n[Service]\nExecStart=/usr/local/bin/cyclonetix --config /etc/cyclonetix/config.yaml\nRestart=on-failure\nUser=cyclonetix\nGroup=cyclonetix\nWorkingDirectory=/etc/cyclonetix\n\n[Install]\nWantedBy=multi-user.target\n\nStart and enable the service:\n\nsudo systemctl daemon-reload\nsudo systemctl start cyclonetix\nsudo systemctl enable cyclonetix\n\n\n\nFor scalable production deployments, use Kubernetes:\n\nCreate a ConfigMap for configuration:\n\n# cyclonetix-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cyclonetix-config\ndata:\n  config.yaml: |\n    backend: \"redis\"\n    backend_url: \"redis://redis-service:6379\"\n    queues:\n      - \"default\"\n      - \"high_memory\"\n      - \"gpu_tasks\"\n    security:\n      enabled: true\n      # ... rest of configuration\n\nDeploy Redis:\n\n# redis-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        ports:\n        - containerPort: 6379\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-service\nspec:\n  selector:\n    app: redis\n  ports:\n  - port: 6379\n    targetPort: 6379\n\nDeploy Cyclonetix orchestrator:\n\n# cyclonetix-orchestrator.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-orchestrator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cyclonetix-orchestrator\n  template:\n    metadata:\n      labels:\n        app: cyclonetix-orchestrator\n    spec:\n      containers:\n      - name: cyclonetix\n        image: neuralchilli/cyclonetix:latest\n        args: [\"--config\", \"/config/config.yaml\", \"--orchestrator\"]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: cyclonetix-config\n\nDeploy Cyclonetix UI:\n\n# cyclonetix-ui.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cyclonetix-ui\n  template:\n    metadata:\n      labels:\n        app: cyclonetix-ui\n    spec:\n      containers:\n      - name: cyclonetix-ui\n        image: neuralchilli/cyclonetix:latest\n        args: [\"--config\", \"/config/config.yaml\", \"--ui\"]\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: config-volume\n          mountPath: /config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: cyclonetix-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cyclonetix-ui-service\nspec:\n  selector:\n    app: cyclonetix-ui\n  ports:\n  - port: 80\n    targetPort: 3000\n  type: ClusterIP\n\nDeploy Cyclonetix agents:\n\n# cyclonetix-agent.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cyclonetix-agent\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cyclonetix-agent\n  template:\n    metadata:\n      labels:\n        app: cyclonetix-agent\n    spec:\n      containers:\n      - name: cyclonetix-agent\n        image: neuralchilli/cyclonetix:latest\n        args: [\"--config\", \"/config/config.yaml\", \"--agent\"]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: cyclonetix-config\n\nDeploy an Ingress for the UI:\n\n# cyclonetix-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: cyclonetix-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: cyclonetix.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: cyclonetix-ui-service\n            port:\n              number: 80\nApply all the configurations:\nkubectl apply -f cyclonetix-configmap.yaml\nkubectl apply -f redis-deployment.yaml\nkubectl apply -f cyclonetix-orchestrator.yaml\nkubectl apply -f cyclonetix-ui.yaml\nkubectl apply -f cyclonetix-agent.yaml\nkubectl apply -f cyclonetix-ingress.yaml",
    "crumbs": [
      "Deployment",
      "Installation"
    ]
  },
  {
    "objectID": "deployment/installation.html#upgrading-cyclonetix",
    "href": "deployment/installation.html#upgrading-cyclonetix",
    "title": "Installation",
    "section": "",
    "text": "To upgrade an existing installation:\n\nStop the Cyclonetix service:\n\nsudo systemctl stop cyclonetix\n\nInstall the new version:\n\n# Option 1: Build from source\ngit pull\ncargo build --release\nsudo cp target/release/cyclonetix /usr/local/bin/\n\n# Option 2: Download pre-built binary\nsudo wget -O /usr/local/bin/cyclonetix https://github.com/neural-chilli/Cyclonetix/releases/latest/download/cyclonetix\nsudo chmod +x /usr/local/bin/cyclonetix\n\nRestart the service:\n\nsudo systemctl start cyclonetix",
    "crumbs": [
      "Deployment",
      "Installation"
    ]
  },
  {
    "objectID": "deployment/installation.html#next-steps",
    "href": "deployment/installation.html#next-steps",
    "title": "Installation",
    "section": "",
    "text": "Configure Security for your deployment\nLearn about Scaling Cyclonetix\nExplore the Configuration Reference",
    "crumbs": [
      "Deployment",
      "Installation"
    ]
  }
]